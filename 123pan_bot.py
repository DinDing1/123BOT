import warnings
warnings.filterwarnings("ignore", message="python-telegram-bot is using upstream urllib3.*")
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*", category=UserWarning)
import os
import re
import json
import time
import logging
import requests
import sqlite3
import threading
from contextlib import closing
from datetime import datetime, timedelta, timezone
from telegram import Update, BotCommand, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Updater, 
    MessageHandler, 
    Filters, 
    CallbackContext, 
    CommandHandler,
    CallbackQueryHandler
)
from functools import wraps
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from urllib.parse import urlparse, parse_qs

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

######################ç‰ˆæœ¬ä¿¡æ¯###########
def get_version():
    """ä» VERSION æ–‡ä»¶ä¸­è¯»å–ç‰ˆæœ¬å·"""
    version_file = "/app/VERSION"
    if os.path.exists(version_file):
        with open(version_file, "r", encoding="utf-8") as f:
            return f.read().strip()
    return "æœªçŸ¥ç‰ˆæœ¬"

VERSION = get_version()
#######################################

# é…ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å¿½ç•¥ç¬¬ä¸‰æ–¹åº“çš„è­¦å‘Š
logging.getLogger("telegram").setLevel(logging.WARNING)
logging.getLogger("apscheduler").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

# ====================== é…ç½®åŒºåŸŸ ======================
# æ•°æ®åº“æ–‡ä»¶è·¯å¾„
DB_PATH = os.getenv("DB_PATH", "bot123.db")

# 123äº‘ç›˜APIé…ç½®
PAN_HOST = "https://www.123pan.com"
API_PATHS = {
    "TOKEN": "/api/v1/access_token",
    "USER_INFO": "/api/v1/user/info",
    "LIST_FILES_V2": "/api/v2/file/list",
    "UPLOAD_REQUEST": "/b/api/file/upload_request",
    "CLEAR_TRASH": "/api/file/trash_delete_all",
    "GET_SHARE": "/b/api/share/get",  # æ·»åŠ åˆ†äº«é“¾æ¥æ¥å£
}

# å¼€æ”¾å¹³å°åœ°å€
OPEN_API_HOST = "https://open-api.123pan.com"

# ç§’ä¼ é“¾æ¥å‰ç¼€
LEGACY_FOLDER_LINK_PREFIX_V1 = "123FSLinkV1$"
LEGACY_FOLDER_LINK_PREFIX_V2 = "123FSLinkV2$"
COMMON_PATH_LINK_PREFIX_V1 = "123FLCPV1$"
COMMON_PATH_LINK_PREFIX_V2 = "123FLCPV2$"
COMMON_PATH_DELIMITER = "%"

# Base62å­—ç¬¦é›†
BASE62_CHARS = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"

# ç¯å¢ƒå˜é‡é…ç½®
DEFAULT_SAVE_DIR = os.getenv("DEFAULT_SAVE_DIR", "").strip()
EXPORT_BASE_DIRS = [d.strip() for d in os.getenv("EXPORT_BASE_DIR", "").split(';') if d.strip()]
SEARCH_MAX_DEPTH = int(os.getenv("SEARCH_MAX_DEPTH", ""))

# APIé€Ÿç‡æ§åˆ¶é…ç½®
API_RATE_LIMIT = float(os.getenv("API_RATE_LIMIT", "2.0"))
TRANSFER_RATE_LIMIT = float(os.getenv("TRANSFER_RATE_LIMIT", "3"))

# å…è®¸çš„æ–‡ä»¶ç±»å‹é…ç½®
ALLOWED_VIDEO_EXTENSIONS = [ext.strip().lower() for ext in os.getenv("ALLOWED_VIDEO_EXT", ".mp4,.mkv,.avi,.mov,.flv,.wmv,.webm,.ts,.m2ts,.iso,.mp3,.flac,.wav").split(',') if ext.strip()]
ALLOWED_SUB_EXTENSIONS = [ext.strip().lower() for ext in os.getenv("ALLOWED_SUB_EXT", ".srt,.ass,.ssa,.sub,.idx,.vtt,.sup").split(',') if ext.strip()]

# =====================================================

def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    try:
        with closing(sqlite3.connect(DB_PATH)) as conn:
            c = conn.cursor()
            c.execute('''CREATE TABLE IF NOT EXISTS token_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                access_token TEXT NOT NULL,
                client_id TEXT NOT NULL,
                client_secret TEXT NOT NULL,
                expired_at TIMESTAMP NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )''')
            
            c.execute('''CREATE TABLE IF NOT EXISTS directory_cache (
                file_id INTEGER PRIMARY KEY,
                filename TEXT NOT NULL,
                parent_id INTEGER NOT NULL,
                full_path TEXT NOT NULL,
                base_dir_id INTEGER NOT NULL,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )''')
            
            c.execute('''CREATE INDEX IF NOT EXISTS idx_filename ON directory_cache (filename)''')
            c.execute('''CREATE INDEX IF NOT EXISTS idx_full_path ON directory_cache (full_path)''')
            c.execute('''CREATE INDEX IF NOT EXISTS idx_base_dir ON directory_cache (base_dir_id)''')
            
            conn.commit()
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")

init_db()

class TokenManager:
    """ç®¡ç†API tokençš„è·å–å’Œç¼“å­˜"""
    def __init__(self, client_id, client_secret):
        self.client_id = client_id
        self.client_secret = client_secret
        self.session = self._create_session()
        self.access_token = None
        self.token_expiry = None
        self.start_time = datetime.now()
        
        if not self.load_token_from_cache():
            self.get_new_token()
    
    def _create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session"""
        session = requests.Session()
        retry_strategy = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        session.verify = False
        return session
    
    def load_token_from_cache(self):
        """ä»æ•°æ®åº“åŠ è½½ç¼“å­˜çš„Token"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                c.execute("SELECT access_token, client_id, client_secret, expired_at FROM token_cache ORDER BY id DESC LIMIT 1")
                row = c.fetchone()
                
                if row:
                    token, cached_id, cached_secret, expired_at_str = row
                    expired_at = datetime.fromisoformat(expired_at_str).astimezone(timezone.utc)
                    now = datetime.now(timezone.utc)
                    
                    if (expired_at > now + timedelta(minutes=5) and
                        self.client_id == cached_id and 
                        self.client_secret == cached_secret):
                        
                        self.access_token = token
                        self.token_expiry = expired_at
                        logger.info("ä½¿ç”¨ç¼“å­˜Token")
                        return True
        except Exception:
            pass
        return False
    
    def save_token_to_cache(self, access_token, expired_at):
        """ä¿å­˜Tokenåˆ°æ•°æ®åº“"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                c.execute("DELETE FROM token_cache")
                c.execute('''INSERT INTO token_cache 
                           (access_token, client_id, client_secret, expired_at)
                           VALUES (?,?,?,?)''',
                           (access_token, self.client_id, self.client_secret, expired_at.isoformat()))
                conn.commit()
                return True
        except Exception:
            return False
    
    def get_new_token(self):
        """è·å–æ–°token"""
        try:
            logger.info("æ­£åœ¨è·å–æ–°Token...")
            url = f"{OPEN_API_HOST}{API_PATHS['TOKEN']}"
            payload = {
                "clientID": self.client_id,
                "clientSecret": self.client_secret
            }
            
            headers = {
                "Content-Type": "application/json",
                "Platform": "open_platform"
            }
            
            response = self.session.post(url, json=payload, headers=headers, timeout=20)
            
            if response.status_code != 200:
                logger.error(f"è®¤è¯å¤±è´¥: {response.status_code}")
                return False
            
            data = response.json()
            if data.get("code") != 0:
                logger.error(f"APIé”™è¯¯: {data.get('code')} - {data.get('message')}")
                return False
            
            self.access_token = data["data"]["accessToken"]
            expired_at_str = data["data"]["expiredAt"]
            
            if expired_at_str.endswith('Z'):
                self.token_expiry = datetime.fromisoformat(expired_at_str[:-1]).replace(tzinfo=timezone.utc)
            elif '+' in expired_at_str or '-' in expired_at_str:
                dt = datetime.fromisoformat(expired_at_str)
                self.token_expiry = dt.astimezone(timezone.utc)
            else:
                self.token_expiry = datetime.fromisoformat(expired_at_str).replace(tzinfo=timezone.utc)
            
            if self.save_token_to_cache(self.access_token, self.token_expiry):
                logger.info(f"æ›´æ–°TokenæˆåŠŸï¼Œæœ‰æ•ˆæœŸè‡³: {self.token_expiry} (UTC)")
                return True
            return False
        except Exception as e:
            logger.error(f"è·å–Tokenå¤±è´¥: {str(e)}")
            return False
    
    def ensure_token_valid(self):
        """ç¡®ä¿tokenæœ‰æ•ˆ"""
        current_time = datetime.now(timezone.utc)
        if not self.access_token or not self.token_expiry or current_time >= self.token_expiry - timedelta(minutes=5):
            logger.info("Tokenæ— æ•ˆæˆ–å³å°†è¿‡æœŸï¼Œåˆ·æ–°ä¸­...")
            return self.get_new_token()
        return True
    
    def get_auth_header(self):
        """è·å–è®¤è¯å¤´"""
        if not self.ensure_token_valid():
            raise Exception("æ— æ³•è·å–æœ‰æ•ˆçš„Token")
        return {
            "Authorization": f"Bearer {self.access_token}",
            "Platform": "open_platform",
            "Content-Type": "application/json"
        }
        
def is_allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºå…è®¸çš„ç±»å‹"""
    ext = os.path.splitext(filename)[1].lower()
    return ext in ALLOWED_VIDEO_EXTENSIONS or ext in ALLOWED_SUB_EXTENSIONS

class Pan123Client:
    def __init__(self, client_id, client_secret):
        self.token_manager = TokenManager(client_id, client_secret)
        self.session = self._create_session()
        self.last_api_call = 0
        self.api_rate_limit = API_RATE_LIMIT
        self.share_root_folder = ""  # æ·»åŠ è¿™ä¸ªå±æ€§
        
        # åˆå§‹åŒ–ç›®å½•ID
        self.default_save_dir_id = 0
        self.export_base_dir_ids = []
        self.export_base_dir_map = {0: "æ ¹ç›®å½•"}
        
        # APIé€Ÿç‡æ§åˆ¶
        self.rate_limit_lock = threading.Lock()
        
        if DEFAULT_SAVE_DIR:
            self.default_save_dir_id = self.get_or_create_directory(DEFAULT_SAVE_DIR)
            logger.info(f"é»˜è®¤ä¿å­˜ç›®å½•å·²è®¾ç½®: '{DEFAULT_SAVE_DIR}' (ID: {self.default_save_dir_id})")
        
        for base_dir in EXPORT_BASE_DIRS:
            base_dir_id = self.get_or_create_directory(base_dir)
            self.export_base_dir_ids.append(base_dir_id)
            self.export_base_dir_map[base_dir_id] = base_dir
            logger.info(f"å¯¼å‡ºåŸºç›®å½•å·²è®¾ç½®: '{base_dir}' (ID: {base_dir_id})")
        
        self.search_max_depth = SEARCH_MAX_DEPTH
        logger.info(f"æœç´¢æœ€å¤§æ·±åº¦å·²è®¾ç½®: {self.search_max_depth} å±‚")
        
        # åˆå§‹åŒ–ç›®å½•ç¼“å­˜
        self.directory_cache = {}
        self.load_directory_cache()
        logger.info(f"å·²åŠ è½½ {len(self.directory_cache)} ä¸ªç›®å½•ç¼“å­˜")
    
    def _create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session"""
        session = requests.Session()
        retry_strategy = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        session.verify = False
        return session
    
    def get_or_create_directory(self, path):
        """è·å–æˆ–åˆ›å»ºç›®å½•è·¯å¾„"""
        parent_id = 0
        parts = path.strip('/').split('/')
        
        for part in parts:
            if not part:
                continue
                
            folder_info = self.search_folder(part, parent_id)
            if folder_info:
                parent_id = folder_info["fileId"]
                logger.debug(f"æ‰¾åˆ°ç›®å½•: '{part}' (ID: {parent_id})")
            else:
                logger.info(f"åˆ›å»ºç›®å½•: '{part}' (çˆ¶ID: {parent_id})")
                folder = self.create_folder(parent_id, part)
                if folder:
                    parent_id = folder["FileId"]
                    logger.info(f"å·²åˆ›å»ºç›®å½•: '{part}' (ID: {parent_id})")
        
        return parent_id
    
    def search_folder(self, folder_name, parent_id=0):
        """åœ¨æŒ‡å®šçˆ¶ç›®å½•ä¸‹æœç´¢æ–‡ä»¶å¤¹"""
        try:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": parent_id,
                "trashed": 0,
                "limit": 100,
                "lastFileId": 0
            }
            headers = self.token_manager.get_auth_header()
            
            response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
            if not response or response.status_code != 200:
                return None
                
            data = response.json()
            if data.get("code") != 0:
                return None
                
            for item in data["data"].get("fileList", []):
                if item["type"] == 1 and item["filename"] == folder_name:
                    return {
                        "fileId": item["fileId"],
                        "filename": item["filename"]
                    }
        except Exception as e:
            logger.error(f"æœç´¢ç›®å½•å‡ºé”™: {str(e)}")
        return None

    def _call_api(self, method, url, **kwargs):
        """æ§åˆ¶APIè°ƒç”¨é¢‘ç‡ï¼Œæ·»åŠ æœ€å¤§é‡è¯•æ¬¡æ•°é™åˆ¶"""
        retry_count = 0
        max_retries = 5
        
        while retry_count < max_retries:
            try:
                with self.rate_limit_lock:
                    elapsed = time.time() - self.last_api_call
                    required_delay = 1.0 / self.api_rate_limit
                    if elapsed < required_delay:
                        time.sleep(required_delay - elapsed)
                    response = self.session.request(method, url, **kwargs)
                    self.last_api_call = time.time()
                
                if response.status_code == 429:
                    retry_after = response.headers.get('Retry-After')
                    wait_time = float(retry_after) if retry_after else 5.0  # å‡å°‘ç­‰å¾…æ—¶é—´
                    logger.warning(f"APIé™æµï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    retry_count += 1  # å¢åŠ é‡è¯•è®¡æ•°
                    continue
                
                try:
                    data = response.json()
                    if data.get("code") == 429 or "æ“ä½œé¢‘ç¹" in data.get("message", ""):
                        logger.warning("APIé™æµï¼ˆå†…å®¹æ£€æµ‹ï¼‰ï¼Œç­‰å¾…5ç§’åé‡è¯•...")
                        time.sleep(5.0)
                        retry_count += 1  # å¢åŠ é‡è¯•è®¡æ•°
                        continue
                except:
                    pass
                
                return response
                
            except (requests.exceptions.SSLError, 
                    requests.exceptions.ConnectionError,
                    requests.exceptions.ChunkedEncodingError,
                    requests.exceptions.HTTPError) as e:
                retry_count += 1
                logger.error(f"ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}ï¼Œé‡è¯• {retry_count}/{max_retries}")
                time.sleep(2 ** retry_count)
            except ConnectionResetError as e:
                retry_count += 1
                logger.error(f"è¿æ¥è¢«é‡ç½®: {str(e)}ï¼Œé‡è¯• {retry_count}/{max_retries}")
                time.sleep(5)
            except Exception as e:
                logger.error(f"APIè°ƒç”¨å‡ºé”™: {str(e)}")
                retry_count += 1
                time.sleep(2 ** retry_count)
        
        logger.error(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}")
        return None
    
    def _get_auth_headers(self):
        """è·å–è®¤è¯å¤´"""
        auth_header = self.token_manager.get_auth_header()
        return {
            **auth_header,
            "platform": "web",
            "App-Version": "3",
            "Origin": PAN_HOST,
            "Referer": f"{PAN_HOST}/",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
        }
    
    def get_user_info(self):
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        try:
            if not self.token_manager.ensure_token_valid():
                return None
                
            url = f"{OPEN_API_HOST}{API_PATHS['USER_INFO']}"
            headers = self.token_manager.get_auth_header()
            response = self._call_api("GET", url, headers=headers, timeout=30)
            if not response or response.status_code != 200:
                return None
                
            data = response.json()
            if data.get("code") != 0:
                return None
                
            return data.get("data")
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ä¿¡æ¯å‡ºé”™: {str(e)}")
            return None
    
    def create_folder(self, parent_id, folder_name, retry_count=3):
        """åˆ›å»ºæ–‡ä»¶å¤¹"""
        logger.info(f"åˆ›å»ºæ–‡ä»¶å¤¹: '{folder_name}' (çˆ¶ID: {parent_id})")
        for attempt in range(retry_count):
            try:
                url = f"{PAN_HOST}{API_PATHS['UPLOAD_REQUEST']}"
                payload = {
                    "driveId": 0,
                    "etag": "",
                    "fileName": folder_name,
                    "parentFileId": int(parent_id),
                    "size": 0,
                    "type": 1,
                    "NotReuse": True,
                    "RequestSource": None,
                    "duplicate": 1,
                    "event": "newCreateFolder",
                    "operateType": 1
                }
                headers = self._get_auth_headers()
                response = self.session.post(url, json=payload, headers=headers, timeout=20, verify=False)
                data = response.json()
                
                if data.get("code") == 0 and data["data"].get("Info", {}).get("FileId"):
                    folder_id = data["data"]["Info"]["FileId"]
                    logger.info(f"æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ: '{folder_name}' (ID: {folder_id})")
                    return data["data"]["Info"]
                else:
                    error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {error_msg}")
            except Exception as e:
                logger.error(f"åˆ›å»ºæ–‡ä»¶å¤¹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            time.sleep(1)
        return None
    
    def rapid_upload(self, etag, size, file_name, parent_id, max_retries=8):
        """ç§’ä¼ æ–‡ä»¶"""
        logger.info(f"å°è¯•ç§’ä¼ æ–‡ä»¶: '{file_name}' (å¤§å°: {size} bytes, çˆ¶ID: {parent_id})")
        original_etag = etag
        
        if len(etag) != 32 or not all(c in '0123456789abcdef' for c in etag.lower()):
            logger.info(f"è½¬æ¢Etagæ ¼å¼: {etag}")
            etag = FastLinkProcessor.optimized_etag_to_hex(etag, True)
        
        base_delay = 2.0
        max_delay = 180.0
        
        for attempt in range(max_retries):
            try:
                delay = min(max_delay, base_delay * (2 ** attempt))
                if attempt > 0:
                    logger.warning(f"ç§’ä¼ å¤±è´¥ï¼Œç­‰å¾… {delay:.1f} ç§’åé‡è¯• (å°è¯• {attempt+1}/{max_retries})...")
                    time.sleep(delay)
                
                url = f"{PAN_HOST}{API_PATHS['UPLOAD_REQUEST']}"
                payload = {
                    "driveId": 0,
                    "etag": etag,
                    "fileName": file_name,
                    "parentFileId": int(parent_id),
                    "size": int(size),
                    "type": 0,
                    "NotReuse": False,
                    "RequestSource": None,
                    "duplicate": 1,
                    "event": "rapidUpload",
                    "operateType": 1
                }
                headers = self._get_auth_headers()
                response = self._call_api("POST", url, json=payload, headers=headers, timeout=30)
                
                if not response:
                    continue
                
                try:
                    data = response.json()
                except json.JSONDecodeError:
                    continue
                
                if data.get("code") == 0 and data["data"].get("Info", {}).get("FileId"):
                    file_id = data["data"]["Info"]["FileId"]
                    logger.info(f"æ–‡ä»¶ç§’ä¼ æˆåŠŸ: '{file_name}' (ID: {file_id})")
                    return data["data"]["Info"]
                else:
                    error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"æ–‡ä»¶ç§’ä¼ å¤±è´¥: {error_msg}")
                    if "etag" in error_msg.lower() and etag != original_etag:
                        logger.info(f"å°è¯•ä½¿ç”¨åŸå§‹Etag: {original_etag}")
                        etag = original_etag
                        continue
                    if "æ“ä½œé¢‘ç¹" in error_msg or "é™æµ" in error_msg or "é¢‘ç¹" in error_msg:
                        with self.rate_limit_lock:
                            self.api_rate_limit = max(0.8, self.api_rate_limit * 0.9)
                        logger.warning(f"è§¦å‘é™æµï¼Œé™ä½å…¨å±€é€Ÿç‡è‡³ {self.api_rate_limit:.2f} è¯·æ±‚/ç§’")
                        continue
            except Exception as e:
                logger.error(f"ç§’ä¼ è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        logger.error(f"ç§’ä¼ å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}")
        return None
    
    def load_directory_cache(self):
        """ä»æ•°æ®åº“åŠ è½½ç›®å½•ç¼“å­˜"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                conn.row_factory = sqlite3.Row
                c = conn.cursor()
                
                if not self.export_base_dir_ids:
                    c.execute("SELECT * FROM directory_cache")
                else:
                    placeholders = ','.join(['?'] * len(self.export_base_dir_ids))
                    c.execute(f"SELECT * FROM directory_cache WHERE base_dir_id IN ({placeholders})", 
                              self.export_base_dir_ids)
                
                rows = c.fetchall()
                for row in rows:
                    file_id = row["file_id"]
                    self.directory_cache[file_id] = dict(row)
                logger.info(f"å·²åŠ è½½ {len(rows)} ä¸ªç›®å½•ç¼“å­˜")
        except Exception as e:
            logger.error(f"åŠ è½½ç›®å½•ç¼“å­˜å¤±è´¥: {str(e)}")
    
    def update_directory_cache(self, file_id, filename, parent_id, full_path, base_dir_id):
        """æ›´æ–°ç›®å½•ç¼“å­˜"""
        try:
            if file_id in self.directory_cache:
                existing = self.directory_cache[file_id]
                if (existing["filename"] == filename and 
                    existing["parent_id"] == parent_id and 
                    existing["full_path"] == full_path and
                    existing["base_dir_id"] == base_dir_id):
                    return False
            
            cache_entry = {
                "file_id": file_id,
                "filename": filename,
                "parent_id": parent_id,
                "full_path": full_path,
                "base_dir_id": base_dir_id
            }
            self.directory_cache[file_id] = cache_entry
            
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                c.execute('''INSERT OR REPLACE INTO directory_cache 
                            (file_id, filename, parent_id, full_path, base_dir_id) 
                            VALUES (?,?,?,?,?)''',
                          (file_id, filename, parent_id, full_path, base_dir_id))
                conn.commit()
            logger.info(f"æ›´æ–°ç›®å½•ç¼“å­˜: {filename} (ID: {file_id}, è·¯å¾„: {full_path})")
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°ç›®å½•ç¼“å­˜å¤±è´¥: {str(e)}")
            return False
    
    def full_sync_directory_cache(self):
        """å…¨é‡åŒæ­¥ç›®å½•ç¼“å­˜"""
        logger.info("å¼€å§‹å…¨é‡åŒæ­¥ç›®å½•ç¼“å­˜...")
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                c.execute("DELETE FROM directory_cache")
                c.execute("DELETE FROM sqlite_sequence WHERE name='directory_cache'")
                conn.commit()
                logger.info("å·²æ¸…ç©ºæ—§ç¼“å­˜æ•°æ®è¡¨")

            self.directory_cache = {}
            logger.info("å·²æ¸…ç©ºå†…å­˜ç¼“å­˜")
            update_count = 0
            
            for base_dir_id in self.export_base_dir_ids:
                base_dir_path = self.export_base_dir_map.get(base_dir_id, f"åŸºç›®å½•({base_dir_id})")
                update_count += self.sync_directory(base_dir_id, base_dir_path, base_dir_id)
            
            logger.info(f"å…¨é‡åŒæ­¥å®Œæˆï¼Œæ›´æ–° {update_count} ä¸ªç›®å½•")
            return update_count
        except Exception as e:
            logger.error(f"å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}")
            return 0
    
    def sync_directory(self, directory_id, current_path, base_dir_id, current_depth=0):
        """åŒæ­¥æŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•"""
        logger.info(f"å¼€å§‹åŒæ­¥ç›®å½•: '{current_path}' (ID: {directory_id}, æ·±åº¦: {current_depth})")
        update_count = 0
        last_file_id = 0
        
        while True:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": directory_id,
                "trashed": 0,
                "limit": 100,
                "lastFileId": last_file_id
            }
            headers = self.token_manager.get_auth_header()
            
            try:
                response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
                if not response or response.status_code != 200:
                    break
                
                data = response.json()
                if data.get("code") != 0:
                    break
                
                for item in data["data"].get("fileList", []):
                    if item.get("trashed", 1) != 0:
                        continue
                    
                    item_path = f"{current_path}/{item['filename']}" if current_path else item['filename']
                    
                    if item["type"] == 1:
                        updated = self.update_directory_cache(
                            item["fileId"],
                            item["filename"],
                            directory_id,
                            item_path,
                            base_dir_id
                        )
                        if updated:
                            update_count += 1
                        
                        if current_depth < self.search_max_depth:
                            update_count += self.sync_directory(
                                item["fileId"],
                                item_path,
                                base_dir_id,
                                current_depth + 1
                            )
                
                last_file_id = data["data"].get("lastFileId", -1)
                if last_file_id == -1:
                    break
            except Exception as e:
                logger.error(f"åŒæ­¥ç›®å½•å‡ºé”™: {str(e)}")
                break
        
        logger.info(f"åŒæ­¥å®Œæˆ: '{current_path}' (ID: {directory_id}), æ›´æ–° {update_count} ä¸ªç›®å½•")
        return update_count
    
    def get_directory_files(self, directory_id=0, base_path="", current_path=""):
        """è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"""
        logger.info(f"è·å–ç›®å½•å†…å®¹ (ID: {directory_id}, åŸºç¡€è·¯å¾„: '{base_path}', å½“å‰è·¯å¾„: '{current_path}')")
        all_files = []
        
        if not self.token_manager.ensure_token_valid():
            return []
        
        last_file_id = 0
        while True:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": directory_id,
                "trashed": 0,
                "limit": 100,
                "lastFileId": last_file_id
            }
            headers = self.token_manager.get_auth_header()
            
            try:
                response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
                if not response or response.status_code != 200:
                    return all_files
                
                data = response.json()
                if data.get("code") != 0:
                    return all_files
                
                for item in data["data"].get("fileList", []):
                    if item.get("trashed", 1) != 0:
                        continue
                    
                    if current_path:
                        file_path = f"{current_path}/{item['filename']}"
                    else:
                        file_path = item['filename']
                    
                    if item["type"] == 0:
                        if not is_allowed_file(item['filename']):
                            continue
                        all_files.append({
                            "path": file_path,
                            "etag": item["etag"],
                            "size": item["size"]
                        })
                    elif item["type"] == 1:
                        if current_path:
                            sub_path = f"{current_path}/{item['filename']}"
                        else:
                            sub_path = item['filename']
                        time.sleep(0.5)
                        sub_files = self.get_directory_files(item["fileId"], base_path, sub_path)
                        all_files.extend(sub_files)
                
                last_file_id = data["data"].get("lastFileId", -1)
                if last_file_id == -1:
                    break
            except Exception as e:
                logger.error(f"è·å–ç›®å½•åˆ—è¡¨å‡ºé”™: {str(e)}")
                return all_files
        
        logger.info(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶ (ID: {directory_id})")
        return all_files

    def clear_trash(self):
        """æ¸…ç©ºå›æ”¶ç«™"""
        logger.info("æ¸…ç©ºå›æ”¶ç«™ä¸­...")
        try:
            url = f"{PAN_HOST}{API_PATHS['CLEAR_TRASH']}"
            headers = self._get_auth_headers()
            payload = {"event": "recycleClear"}
            response = self._call_api("POST", url, json=payload, headers=headers, timeout=30)
            if not response or response.status_code != 200:
                return False
            data = response.json()
            if data.get("code") == 7301 or data.get("code") == 0:
                logger.info("å›æ”¶ç«™å·²æ¸…ç©º")
                return True
            return False
        except Exception as e:
            logger.error(f"æ¸…ç©ºå›æ”¶ç«™å‡ºé”™: {str(e)}")
            return False
   
    def extract_share_info(self, share_url):
        """ä»åˆ†äº«é“¾æ¥æå–åˆ†äº«Keyå’Œå¯†ç ï¼ˆä½¿ç”¨æ”¹è¿›çš„æ­£åˆ™ï¼‰"""
        # ä½¿ç”¨æä¾›çš„æ­£åˆ™è¡¨è¾¾å¼
        pattern = r'(https?://(?:[a-zA-Z0-9-]+\.)*123[a-zA-Z0-9-]*\.[a-z]{2,6}+/s/)([a-zA-Z0-9\-_]+)(?:[\s\S]*?(?:æå–ç |å¯†ç |code)[\s:ï¼š=]*(\w{4}))?'
        match = re.search(pattern, share_url)
        if not match:
            raise ValueError("æ— æ•ˆçš„åˆ†äº«é“¾æ¥æ ¼å¼")
        
        share_key = match.group(2)
        password = match.group(3) or ""
        
        return share_key, password

    def save_share_files(self, share_url, save_dir_id):
        """ä¿å­˜åˆ†äº«é“¾æ¥ä¸­çš„æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•ï¼Œä¿ç•™åŸå§‹ç›®å½•ç»“æ„"""
        try:
            # æå–åˆ†äº«ä¿¡æ¯
            share_key, password = self.extract_share_info(share_url)
            logger.info(f"å¼€å§‹è½¬å­˜åˆ†äº«: key={share_key}, password={password}")
            
            # é€’å½’è·å–æ‰€æœ‰æ–‡ä»¶
            files = self._get_share_files_recursive(share_key, password, "0", "")
            if not files:
                logger.warning("åˆ†äº«ä¸­æ²¡æœ‰æ–‡ä»¶")
                return 0, 0, [], 0
                
            # ç”¨äºå­˜å‚¨ç›®å½•æ˜ å°„ï¼šè·¯å¾„ -> äº‘ç›˜ç›®å½•ID
            dir_map = {"": save_dir_id}  # æ ¹ç›®å½•æ˜ å°„
            success_count = 0
            failure_count = 0
            results = []
            total_size = 0  # ç»Ÿè®¡æ€»å¤§å°
            
            # é¦–å…ˆåˆ›å»ºæ‰€æœ‰éœ€è¦çš„ç›®å½•
            all_dirs = {os.path.dirname(f["path"]) for f in files}
            for dir_path in sorted(all_dirs):
                if not dir_path or dir_path in dir_map:
                    continue
                    
                # åˆ›å»ºç›®å½•è·¯å¾„
                parent_id = save_dir_id
                parts = dir_path.split('/')
                current_path = ""
                
                for part in parts:
                    if not part:
                        continue
                        
                    current_path = f"{current_path}/{part}" if current_path else part
                    if current_path in dir_map:
                        parent_id = dir_map[current_path]
                        continue
                        
                    # åˆ›å»ºç›®å½•
                    folder = self.create_folder(parent_id, part)
                    if folder:
                        dir_map[current_path] = folder["FileId"]
                        logger.info(f"åˆ›å»ºç›®å½•: {current_path} (ID: {folder['FileId']})")
                        parent_id = folder["FileId"]
                    else:
                        logger.warning(f"åˆ›å»ºç›®å½•å¤±è´¥: {current_path}")
                        break
            
            # è½¬å­˜æ–‡ä»¶
            for file_info in files:
                file_path = file_info["path"]
                file_name = os.path.basename(file_path)
                dir_path = os.path.dirname(file_path)
                parent_id = dir_map.get(dir_path, save_dir_id)
                
                # åªè½¬å­˜å…è®¸çš„æ–‡ä»¶ç±»å‹
                if not is_allowed_file(file_name):
                    continue
                
                total_size += file_info["size"]
                
                try:
                    result = self.rapid_upload(
                        file_info["etag"], 
                        file_info["size"], 
                        file_name, 
                        parent_id
                    )
                    
                    if result:
                        success_count += 1
                        results.append({
                            "success": True,
                            "file_name": file_path,
                            "size": file_info["size"]
                        })
                        logger.info(f"æ–‡ä»¶è½¬å­˜æˆåŠŸ: {file_path}")
                    else:
                        failure_count += 1
                        results.append({
                            "success": False,
                            "file_name": file_path,
                            "size": file_info["size"],
                            "error": "ç§’ä¼ å¤±è´¥"
                        })
                        logger.warning(f"æ–‡ä»¶ç§’ä¼ å¤±è´¥: {file_path}")
                except Exception as e:
                    failure_count += 1
                    results.append({
                        "success": False,
                        "file_name": file_path,
                        "size": file_info["size"],
                        "error": str(e)
                    })
                    logger.error(f"æ–‡ä»¶è½¬å­˜å‡ºé”™: {file_path} - {str(e)}")
            
            return success_count, failure_count, results, total_size
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†äº«æ–‡ä»¶å¤±è´¥: {str(e)}")
            return 0, 0, [], 0
    
    def _get_share_files_recursive(self, share_key, password, fid, current_path):
        """é€’å½’è·å–åˆ†äº«ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        files = []
        items = self._get_share_files(share_key, password, fid)
        
        for item in items:
            if item["Type"] == 0:  # æ–‡ä»¶
                file_path = f"{current_path}/{item['FileName']}" if current_path else item['FileName']
                files.append({
                    "path": file_path,
                    "name": item["FileName"],
                    "size": item["Size"],
                    "etag": item["Etag"]
                })
            elif item["Type"] == 1:  # ç›®å½•
                sub_path = f"{current_path}/{item['FileName']}" if current_path else item['FileName']
                sub_files = self._get_share_files_recursive(
                    share_key, 
                    password, 
                    item["FileId"], 
                    sub_path
                )
                files.extend(sub_files)
        
        return files
    
    def _get_share_files(self, share_key, password, fid="0"):
        """è·å–åˆ†äº«ä¸­çš„æ–‡ä»¶å’Œç›®å½•åˆ—è¡¨ï¼ˆéé€’å½’ï¼‰"""
        items = []
        next_marker = "0"
        page = 1
        
        while next_marker != "-1":
            params = {
                "shareKey": share_key,
                "SharePwd": password,
                "parentFileId": fid,
                "limit": 100,
                "next": next_marker,
                "orderBy": "file_name",
                "orderDirection": "asc",
                "Page": page
            }
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
                "Referer": f"{PAN_HOST}/s/{share_key}",
                "Origin": PAN_HOST
            }
            
            try:
                response = self._call_api("GET", f"{PAN_HOST}{API_PATHS['GET_SHARE']}", 
                                         params=params, headers=headers, timeout=30)
                if not response or response.status_code != 200:
                    break
                
                data = response.json()
                if data.get("code") != 0:
                    logger.warning(f"è·å–åˆ†äº«æ–‡ä»¶å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    break
                
                # æ·»åŠ å½“å‰é¡µçš„é¡¹ç›®
                for item in data["data"]["InfoList"]:
                    item["Type"] = item.get("Type", 0)  # ç¡®ä¿æœ‰Typeå­—æ®µ
                    items.append(item)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                next_marker = data["data"].get("Next", "-1")
                page += 1
                
            except Exception as e:
                logger.error(f"è·å–åˆ†äº«æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                break
        
        logger.info(f"è·å–åˆ° {len(items)} ä¸ªåˆ†äº«é¡¹ç›® (fid={fid})")
        return items

class FastLinkProcessor:
    @staticmethod
    def parse_share_link(share_link):
        """è§£æç§’ä¼ é“¾æ¥"""
        logger.info("è§£æç§’ä¼ é“¾æ¥...")
        common_base_path = ""
        is_common_path_format = False
        is_v2_etag_format = False
        
        if share_link.startswith(COMMON_PATH_LINK_PREFIX_V2):
            is_common_path_format = True
            is_v2_etag_format = True
            share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V2):]
        elif share_link.startswith(COMMON_PATH_LINK_PREFIX_V1):
            is_common_path_format = True
            share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V1):]
        elif share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V2):
            is_v2_etag_format = True
            share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V2):]
        elif share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V1):
            share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V1):]
        
        if is_common_path_format:
            delimiter_pos = share_link.find(COMMON_PATH_DELIMITER)
            if delimiter_pos > -1:
                common_base_path = share_link[:delimiter_pos]
                share_link = share_link[delimiter_pos + 1:]
        
        files = []
        for s_link in share_link.split('$'):
            if not s_link:
                continue
            parts = s_link.split('#')
            if len(parts) < 3:
                continue
            
            etag = parts[0]
            size = parts[1]
            file_path = '#'.join(parts[2:])
            
            if is_common_path_format and common_base_path:
                file_path = common_base_path + file_path
            
            files.append({
                "etag": etag,
                "size": int(size),
                "file_name": file_path,
                "is_v2_etag": is_v2_etag_format
            })
        
        logger.info(f"è§£æåˆ° {len(files)} ä¸ªæ–‡ä»¶")
        return files
    
    @staticmethod
    def optimized_etag_to_hex(optimized_etag, is_v2_etag):
        """å°†ä¼˜åŒ–åçš„ETagè½¬æ¢ä¸ºåå…­è¿›åˆ¶æ ¼å¼"""
        if not is_v2_etag:
            return optimized_etag
        
        try:
            if len(optimized_etag) == 32 and all(c in '0123456789abcdefABCDEF' for c in optimized_etag):
                return optimized_etag.lower()
            
            num = 0
            for char in optimized_etag:
                if char not in BASE62_CHARS:
                    return optimized_etag
                num = num * 62 + BASE62_CHARS.index(char)
            
            hex_str = hex(num)[2:].lower()
            if len(hex_str) > 32:
                hex_str = hex_str[-32:]
            elif len(hex_str) < 32:
                hex_str = hex_str.zfill(32)
            
            if len(hex_str) != 32 or not all(c in '0123456789abcdef' for c in hex_str):
                return optimized_etag
            return hex_str
        except Exception as e:
            logger.error(f"ETagè½¬æ¢å¤±è´¥: {str(e)}")
            return optimized_etag

class TelegramBotHandler:
    def __init__(self, token, pan_client, allowed_user_ids):
        self.token = token
        self.pan_client = pan_client
        self.allowed_user_ids = allowed_user_ids
        self.updater = Updater(token, use_context=True)
        self.dispatcher = self.updater.dispatcher
        self.start_time = pan_client.token_manager.start_time
        
        # æ³¨å†Œå¤„ç†ç¨‹åº
        self.dispatcher.add_handler(CommandHandler("start", self.start_command))
        self.dispatcher.add_handler(CommandHandler("export", self.export_command))
        self.dispatcher.add_handler(CommandHandler("sync_full", self.sync_full_command))
        self.dispatcher.add_handler(CommandHandler("clear_trash", self.clear_trash_command))
        self.dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, self.handle_text))
        self.dispatcher.add_handler(MessageHandler(Filters.document, self.handle_document))
        self.dispatcher.add_handler(CallbackQueryHandler(self.button_callback))
        
        # è®¾ç½®èœå•å‘½ä»¤
        self.set_menu_commands()
    
    def set_menu_commands(self):
        """è®¾ç½®Telegram Botèœå•å‘½ä»¤"""
        commands = [
            BotCommand("start", "ç”¨æˆ·ä¿¡æ¯"),
            BotCommand("export", "å¯¼å‡ºç§’ä¼ æ–‡ä»¶"),
            BotCommand("sync_full", "å…¨é‡åŒæ­¥"),
            BotCommand("clear_trash", "æ¸…ç©ºå›æ”¶ç«™"),
        ]
        
        try:
            self.updater.bot.set_my_commands(commands)
            logger.info("å·²è®¾ç½®Telegram Botèœå•å‘½ä»¤")
        except Exception as e:
            logger.error(f"è®¾ç½®èœå•å‘½ä»¤å¤±è´¥: {str(e)}")
    
    def start(self):
        """å¯åŠ¨æœºå™¨äºº"""
        try:
            self.updater.start_polling()
            logger.info("ğŸ¤– æœºå™¨äººå·²å¯åŠ¨ï¼Œç­‰å¾…æ¶ˆæ¯...")
            logger.info(f"ç®¡ç†å‘˜ç”¨æˆ·ID: {self.allowed_user_ids}")
            self.updater.idle()
        except Exception as e:
            logger.error(f"å¯åŠ¨æœºå™¨äººå¤±è´¥: {str(e)}")
    
    # ç®¡ç†å‘˜æƒé™æ£€æŸ¥è£…é¥°å™¨
    def admin_required(func):
        @wraps(func)
        def wrapper(self, update: Update, context: CallbackContext, *args, **kwargs):
            user_id = update.message.from_user.id
            if user_id not in self.allowed_user_ids:
                return
            return func(self, update, context, *args, **kwargs)
        return wrapper
    
    def auto_delete_message(self, context, chat_id, message_id, delay=3):
        """è‡ªåŠ¨åˆ é™¤æ¶ˆæ¯"""
        def delete():
            try:
                context.bot.delete_message(chat_id=chat_id, message_id=message_id)
            except Exception:
                pass
        threading.Timer(delay, delete).start()
    
    def send_auto_delete_message(self, update, context, text, delay=3, chat_id=None):
        """å‘é€è‡ªåŠ¨åˆ é™¤çš„æ¶ˆæ¯"""
        if chat_id is None:
            if update and update.message:
                chat_id = update.message.chat_id
            elif update and update.callback_query and update.callback_query.message:
                chat_id = update.callback_query.message.chat_id
            elif context and hasattr(context, '_chat_id'):
                chat_id = context._chat_id
            else:
                return None
        
        message = context.bot.send_message(chat_id=chat_id, text=text)
        self.auto_delete_message(context, chat_id, message.message_id, delay)
        return message
    
    @admin_required
    def start_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/startå‘½ä»¤"""
        try:
            user_info = self.pan_client.get_user_info()
            if not user_info:
                self.send_auto_delete_message(update, context, "âŒ æ— æ³•è·å–ç”¨æˆ·ä¿¡æ¯")
                return
            
            # è®¡ç®—è¿è¡Œæ—¶é—´
            uptime = datetime.now() - self.start_time
            days = uptime.days
            hours, remainder = divmod(uptime.seconds, 3600)
            minutes, seconds = divmod(remainder, 60)
            
            # æ ¼å¼åŒ–æ‰‹æœºå·ç å’ŒUID
            phone = user_info.get("passport", "")
            if phone and len(phone) > 7:
                phone = phone[:3] + "*" * 4 + phone[-4:]
            
            uid = str(user_info.get("uid", ""))
            if uid and len(uid) > 6:
                uid = uid[:3] + "*" * (len(uid) - 6) + uid[-3:]
            
            # æ ¼å¼åŒ–å­˜å‚¨ç©ºé—´
            def format_size(size_bytes):
                if size_bytes >= 1024 ** 4:
                    return f"{size_bytes / (1024 ** 4):.2f} TB"
                elif size_bytes >= 1024 ** 3:
                    return f"{size_bytes / (1024 ** 3):.2f} GB"
                elif size_bytes >= 1024 ** 2:
                    return f"{size_bytes / (1024 ** 2):.2f} MB"
                else:
                    return f"{size_bytes / 1024:.2f} KB"
            
            space_permanent = format_size(user_info.get("spacePermanent", 0))
            space_used = format_size(user_info.get("spaceUsed", 0))
            direct_traffic = format_size(user_info.get("directTraffic", 0))
            
            # æ„å»ºæ¶ˆæ¯
            export_dirs = ", ".join(EXPORT_BASE_DIRS) if EXPORT_BASE_DIRS else "æ ¹ç›®å½•"
            message = (
                f"ğŸš€ 123äº‘ç›˜ç”¨æˆ·ä¿¡æ¯ | {'ğŸ‘‘ å°Šäº«è´¦æˆ·' if user_info.get('vip', False) else 'ğŸ”’ æ™®é€šè´¦æˆ·'}\n"
                f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                f"ğŸ‘¤ æ˜µç§°: {user_info.get('nickname', 'æœªçŸ¥')}\n"
                f"ğŸ†” è´¦æˆ·ID: {uid}\n"
                f"ğŸ“± æ‰‹æœºå·ç : {phone}\n\n"
                f"ğŸ’¾ å­˜å‚¨ç©ºé—´\n"
                f"â”œ æ°¸ä¹…: {space_permanent}\n"
                f"â”” å·²ç”¨: {space_used}\n\n"
                f"ğŸ“¡ æµé‡ä¿¡æ¯\n"
                f"â”” ç›´é“¾: {direct_traffic}\n"
                f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
                f"âš™ï¸ å½“å‰é…ç½®:\n"
                f"â”œ ä¿å­˜ç›®å½•: {DEFAULT_SAVE_DIR or 'æ ¹ç›®å½•'}\n"
                f"â”œ å¯¼å‡ºç›®å½•: {export_dirs}\n"
                f"â”œ æœç´¢æ·±åº¦: {SEARCH_MAX_DEPTH}å±‚\n"
                f"â”” æ•°æ®ç¼“å­˜: {len(self.pan_client.directory_cache)}\n\n"
                f"ğŸ¤– æœºå™¨äººæ§åˆ¶ä¸­å¿ƒ\n"
                f"â–«ï¸ /export å¯¼å‡ºæ–‡ä»¶\n"
                f"â–«ï¸ /sync_full å…¨é‡åŒæ­¥\n"
                f"â–«ï¸ /clear_trash æ¸…ç©ºå›æ”¶ç«™\n\n"
                f"â±ï¸ å·²è¿è¡Œ: {days}å¤©{hours}å°æ—¶{minutes}åˆ†{seconds}ç§’\n"
                f"ğŸ“¦ Version: {VERSION}"
            )

            update.message.reply_text(message)
            logger.info("å·²å‘é€ç”¨æˆ·ä¿¡æ¯")
        except Exception as e:
            logger.error(f"å¤„ç†/startå‘½ä»¤å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, "âŒ è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥")

    def search_database_by_name(self, name_pattern):
        """åœ¨æ•°æ®åº“ä¸­è¿›è¡Œæ¨¡ç³Šæœç´¢"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                conn.row_factory = sqlite3.Row
                c = conn.cursor()
                c.execute("SELECT * FROM directory_cache WHERE filename LIKE ? ORDER BY filename", (f'%{name_pattern}%',))
                rows = c.fetchall()
                logger.info(f"æ•°æ®åº“ä¸­æ‰¾åˆ° {len(rows)} ä¸ªåŒ¹é…é¡¹: '{name_pattern}'")
                return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"æ•°æ®åº“æœç´¢å¤±è´¥: {str(e)}")
            return []

    @admin_required
    def export_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/exportå‘½ä»¤"""
        search_query = " ".join(context.args) if context.args else ""
        if not search_query:
            self.send_auto_delete_message(update, context, "âŒ è¯·æŒ‡å®šæ–‡ä»¶å¤¹åç§°ï¼æ ¼å¼: /export <æ–‡ä»¶å¤¹åç§°>")
            return
        
        self.send_auto_delete_message(update, context, f"ğŸ” æ­£åœ¨æœç´¢æ–‡ä»¶å¤¹: '{search_query}'...")
        
        try:
            results = self.search_database_by_name(search_query)
            if not results:
                self.send_auto_delete_message(update, context, f"âŒ æœªæ‰¾åˆ°åŒ…å« '{search_query}' çš„æ–‡ä»¶å¤¹")
                return
            
            context.user_data['export_search_results'] = results
            context.user_data['export_selected_indices'] = set()
            
            keyboard = []
            max_buttons = 40
            for i, result in enumerate(results[:max_buttons]):
                filename = result["filename"]
                display_name = filename if len(filename) <= 50 else f"{filename[:47]}..."
                keyboard.append([
                    InlineKeyboardButton(f"{i+1}. {display_name}", callback_data=f"export_toggle_{i}")
                ])
            
            action_buttons = [
                InlineKeyboardButton("âœ… å…¨é€‰", callback_data="export_select_all"),
                InlineKeyboardButton("âŒ å–æ¶ˆå…¨é€‰", callback_data="export_deselect_all"),
                InlineKeyboardButton("ğŸš€ å¼€å§‹å¯¼å‡º", callback_data="export_confirm"),
                InlineKeyboardButton("âŒ å–æ¶ˆæ“ä½œ", callback_data="export_cancel")
            ]
            
            keyboard.append(action_buttons[:2])
            keyboard.append(action_buttons[2:])
            reply_markup = InlineKeyboardMarkup(keyboard)
            
            message = update.message.reply_text(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…é¡¹\nè¯·é€‰æ‹©è¦å¯¼å‡ºçš„æ–‡ä»¶å¤¹:", reply_markup=reply_markup)
            context.user_data['export_message_id'] = message.message_id
            
            job_context = {
                "chat_id": update.message.chat_id,
                "user_data": context.user_data
            }
            context.job_queue.run_once(
                self.export_timeout, 
                60, 
                context=job_context,
                name=f"export_timeout_{message.message_id}"
            )
        except Exception as e:
            logger.error(f"æœç´¢æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ æœç´¢å¤±è´¥: {str(e)}")

    def export_choice_callback(self, update: Update, context: CallbackContext):
        """å¤„ç†å¯¼å‡ºé€‰æ‹©çš„å›è°ƒ"""
        query = update.callback_query
        query.answer()
        data = query.data
        
        results = context.user_data.get('export_search_results', [])
        selected_indices = context.user_data.get('export_selected_indices', set())
        
        if not results:
            query.edit_message_text("âŒ é€‰æ‹©è¶…æ—¶ï¼Œè¯·é‡æ–°æœç´¢")
            return
        
        if data.startswith("export_toggle_"):
            try:
                index = int(data.split("_")[2])
                if index in selected_indices:
                    selected_indices.remove(index)
                else:
                    selected_indices.add(index)
            except (ValueError, IndexError):
                pass
        elif data == "export_select_all":
            selected_indices = set(range(len(results)))
        elif data == "export_deselect_all":
            selected_indices = set()
        elif data == "export_confirm":
            self.process_export_selection(update, context, selected_indices)
            return
        elif data == "export_cancel":
            query.edit_message_text("âŒ å¯¼å‡ºæ“ä½œå·²å–æ¶ˆ")
            self.cleanup_export_context(context.user_data)
            return
        
        context.user_data['export_selected_indices'] = selected_indices
        self.update_export_message(update, context, results, selected_indices)
    
    def update_export_message(self, update: Update, context: CallbackContext, results, selected_indices):
        """æ›´æ–°å¯¼å‡ºé€‰æ‹©æ¶ˆæ¯"""
        query = update.callback_query
        selected_count = len(selected_indices)
        
        keyboard = []
        max_buttons = 40
        for i, result in enumerate(results[:max_buttons]):
            filename = result["filename"]
            display_name = filename if len(filename) <= 50 else f"{filename[:47]}..."
            prefix = "âœ… " if i in selected_indices else "â¬œ "
            keyboard.append([
                InlineKeyboardButton(f"{prefix}{i+1}. {display_name}", callback_data=f"export_toggle_{i}")
            ])
        
        action_buttons = [
            InlineKeyboardButton("âœ… å…¨é€‰", callback_data="export_select_all"),
            InlineKeyboardButton("âŒ å–æ¶ˆå…¨é€‰", callback_data="export_deselect_all"),
            InlineKeyboardButton(f"ğŸš€ å¯¼å‡º({selected_count})", callback_data="export_confirm"),
            InlineKeyboardButton("âŒ å–æ¶ˆ", callback_data="export_cancel")
        ]
        
        keyboard.append(action_buttons[:2])
        keyboard.append(action_buttons[2:])
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        query.edit_message_text(
            text=f"âœ… æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…é¡¹\nå·²é€‰æ‹© {selected_count} ä¸ªæ–‡ä»¶å¤¹:",
            reply_markup=reply_markup
        )
    
    def export_timeout(self, context: CallbackContext):
        """å¯¼å‡ºé€‰æ‹©è¶…æ—¶å¤„ç†"""
        job = context.job
        if not job or not job.context:
            return
        
        job_context = job.context
        chat_id = job_context.get("chat_id")
        user_data = job_context.get("user_data", {})

        if not chat_id:
            return
        
        if 'export_message_id' in user_data:
            message_id = user_data['export_message_id']
            try:
                self.updater.bot.edit_message_text(chat_id=chat_id, message_id=message_id, text="â±ï¸ æ“ä½œè¶…æ—¶ï¼Œå¯¼å‡ºå·²è‡ªåŠ¨å–æ¶ˆ")
            except Exception:
                pass
        
        self.cleanup_export_context(user_data)
    
    def cleanup_export_context(self, user_data: dict):
        """æ¸…ç†å¯¼å‡ºç›¸å…³çš„ä¸Šä¸‹æ–‡æ•°æ®"""
        keys_to_remove = ['export_search_results', 'export_selected_indices', 'export_message_id']
        for key in keys_to_remove:
            if key in user_data:
                del user_data[key]
    
    def process_export_selection(self, update: Update, context: CallbackContext, selected_indices):
        """å¤„ç†é€‰æ‹©çš„å¯¼å‡ºä»»åŠ¡ï¼Œæ·»åŠ æ¶ˆæ¯æ’¤å›"""
        query = update.callback_query
        results = context.user_data.get('export_search_results', [])
        if not results or not selected_indices:
            query.edit_message_text("âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶å¤¹")
            return
            
        # å‘é€ä¸´æ—¶æ¶ˆæ¯å¹¶è®¾ç½®è‡ªåŠ¨åˆ é™¤
        query.edit_message_text(f"â³ å¼€å§‹å¯¼å‡º {len(selected_indices)} ä¸ªæ–‡ä»¶å¤¹...")
        self.auto_delete_message(context, query.message.chat_id, query.message.message_id, 3)
        
        if 'export_message_id' in context.user_data:
            message_id = context.user_data['export_message_id']
            job_name = f"export_timeout_{message_id}"
            for job in context.job_queue.get_jobs_by_name(job_name):
                job.schedule_removal()
        
        total = len(selected_indices)
        # ç”¨äºå­˜å‚¨æ‰€æœ‰è¿›åº¦æ¶ˆæ¯çš„ID
        progress_messages = []
        
        for i, idx in enumerate(selected_indices):
            selected_folder = results[idx]
            folder_id = selected_folder["file_id"]
            folder_name = selected_folder["filename"]
            folder_path = selected_folder["full_path"]
            
            logger.info(f"å¤„ç†æ–‡ä»¶å¤¹ [{i+1}/{total}]: {folder_name} (ID: {folder_id})")
            
            # æ¯å¤„ç†3ä¸ªæ–‡ä»¶å¤¹æ›´æ–°ä¸€æ¬¡è¿›åº¦
            if i % 3 == 0:
                try:
                    # å‘é€è¿›åº¦æ¶ˆæ¯å¹¶è®°å½•æ¶ˆæ¯ID
                    msg = context.bot.send_message(
                        chat_id=query.message.chat_id,
                        text=f"â³ æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹ [{i+1}/{total}]:\n"
                             f"â”œ åç§°: {folder_name}\n"
                             f"â”” è·¯å¾„: {folder_path}"
                    )
                    progress_messages.append(msg.message_id)
                except Exception as e:
                    logger.warning(f"å‘é€è¿›åº¦æ¶ˆæ¯å¤±è´¥: {str(e)}")
            
            files = self.pan_client.get_directory_files(folder_id, folder_name)
            if not files:
                logger.warning(f"æ–‡ä»¶å¤¹ä¸ºç©º: {folder_name}")
                continue
            
            logger.info(f"æ–‡ä»¶å¤¹ '{folder_name}' ä¸­æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
            
            json_data = {
                "commonPath": folder_name,
                "usesBase62EtagsInExport": False,
                "files": [
                    {"path": file_info["path"], "etag": file_info["etag"], "size": file_info["size"]}
                    for file_info in files
                ]
            }
            
            clean_folder_name = re.sub(r'[\\/*?:"<>|]', "", folder_name)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_name = f"{clean_folder_name}_{timestamp}.json"
            
            with open(file_name, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            user_info = self.pan_client.get_user_info()
            nickname = user_info.get("nickname", "æœªçŸ¥ç”¨æˆ·") if user_info else "æœªçŸ¥ç”¨æˆ·"
            caption = (
                f"âœ¨æ¥è‡ªï¼š{nickname}çš„åˆ†äº«\n\n"
                f"ğŸ“ æ–‡ä»¶å: {clean_folder_name}\n"
                f"ğŸ“ æ–‡ä»¶æ•°: {len(files)}\n\n"
                f"â¤ï¸ 123å› æ‚¨åˆ†äº«æ›´å®Œç¾ï¼"
            )
            
            with open(file_name, "rb") as f:
                context.bot.send_document(
                    chat_id=query.message.chat_id,
                    document=f,
                    filename=file_name,
                    caption=caption
                )
            
            os.remove(file_name)
            logger.info(f"å·²å‘é€å¯¼å‡ºæ–‡ä»¶: {file_name}")
        
        # å¯¼å‡ºå®Œæˆååˆ é™¤æ‰€æœ‰è¿›åº¦æ¶ˆæ¯
        chat_id = query.message.chat_id
        for msg_id in progress_messages:
            try:
                context.bot.delete_message(chat_id=chat_id, message_id=msg_id)
            except Exception as e:
                logger.warning(f"åˆ é™¤è¿›åº¦æ¶ˆæ¯å¤±è´¥: {str(e)}")
        
        #context.bot.send_message(chat_id=chat_id, text=f"âœ… å¯¼å‡ºå®Œæˆï¼å…±å¤„ç† {total} ä¸ªæ–‡ä»¶å¤¹")
        self.cleanup_export_context(context.user_data)
 
    @admin_required
    def handle_document(self, update: Update, context: CallbackContext):
        """å¤„ç†æ–‡æ¡£æ¶ˆæ¯"""
        document = update.message.document
        file_name = document.file_name
        
        if document.mime_type != "application/json" and not file_name.endswith(".json"):
            self.send_auto_delete_message(update, context, "âŒ è¯·å‘é€JSONæ ¼å¼çš„æ–‡ä»¶ï¼")
            return
        
        logger.info(f"æ”¶åˆ°JSONæ–‡ä»¶: {file_name}")
        self.send_auto_delete_message(update, context, "ğŸ“¥ æ”¶åˆ°JSONæ–‡ä»¶ï¼Œå¼€å§‹ä¸‹è½½å¹¶è§£æ...")
        
        file = context.bot.get_file(document.file_id)
        file_path = f"temp_{document.file_id}.json"
        file.download(file_path)
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                json_data = json.load(f)
            os.remove(file_path)
            logger.info(f"è§£æJSONæ–‡ä»¶: {file_name}")
            self.process_json_file(update, context, json_data)
        except Exception as e:
            logger.error(f"å¤„ç†JSONæ–‡ä»¶å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    @admin_required
    def process_fast_link(self, update: Update, context: CallbackContext, share_link):
        """å¤„ç†ç§’ä¼ é“¾æ¥è½¬å­˜"""
        try:
            logger.info(f"å¤„ç†ç§’ä¼ é“¾æ¥: {share_link[:50]}...")
            files = FastLinkProcessor.parse_share_link(share_link)
            if not files:
                logger.warning("æ— æ³•è§£æç§’ä¼ é“¾æ¥æˆ–é“¾æ¥ä¸­æ— æœ‰æ•ˆæ–‡ä»¶ä¿¡æ¯")
                self.send_auto_delete_message(update, context, "âŒ æ— æ³•è§£æç§’ä¼ é“¾æ¥")
                return
            
            logger.info(f"å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
            self.send_auto_delete_message(update, context, f"âœ… è§£ææˆåŠŸï¼æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è½¬å­˜...")
            results, filtered_count, elapsed_time, original_total_count, original_total_size = self.transfer_files(update, context, files)
            self.send_transfer_results(update, context, results, filtered_count, elapsed_time, original_total_count, original_total_size)
        except Exception as e:
            logger.error(f"å¤„ç†ç§’ä¼ é“¾æ¥å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†ç§’ä¼ é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
    
    @admin_required
    def process_json_file(self, update: Update, context: CallbackContext, json_data):
        """å¤„ç†JSONæ–‡ä»¶è½¬å­˜"""
        try:
            if not isinstance(json_data, dict) or not json_data.get("files"):
                logger.warning("JSONæ ¼å¼æ— æ•ˆï¼Œç¼ºå°‘fileså­—æ®µ")
                self.send_auto_delete_message(update, context, "âŒ JSONæ ¼å¼æ— æ•ˆ")
                return
            
            common_path = json_data.get("commonPath", "").strip()
            if common_path.endswith('/'):
                common_path = common_path[:-1]
            
            files = []
            for file_info in json_data["files"]:
                file_path = file_info.get("path", "")
                if common_path:
                    file_path = f"{common_path}/{file_path}"
                if not is_allowed_file(file_path):
                    continue
                files.append({
                    "etag": file_info.get("etag", ""),
                    "size": int(file_info.get("size", 0)),
                    "file_name": file_path,
                    "is_v2_etag": json_data.get("usesBase62EtagsInExport", False)
                })
            
            logger.info(f"å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
            self.send_auto_delete_message(update, context, f"âœ… è§£ææˆåŠŸï¼æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è½¬å­˜...")
            results, filtered_count, elapsed_time, original_total_count, original_total_size = self.transfer_files(update, context, files)
            self.send_transfer_results(update, context, results, filtered_count, elapsed_time, original_total_count, original_total_size)
        except Exception as e:
            logger.error(f"å¤„ç†JSONæ–‡ä»¶å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    def transfer_files(self, update: Update, context: CallbackContext, files):
        """è½¬å­˜æ–‡ä»¶åˆ—è¡¨"""
        start_time = time.time()
        results = []
        original_total_count = len(files)
        original_total_size = sum(file_info["size"] for file_info in files)
        filtered_count = 0
        folder_cache = {}
        RATE_LIMIT = TRANSFER_RATE_LIMIT
        last_request_time = time.time()
        
        logger.info(f"å¼€å§‹è½¬å­˜ {original_total_count} ä¸ªæ–‡ä»¶...")
        
        for i, file_info in enumerate(files):
            file_path = file_info["file_name"]
            file_size = file_info["size"]
            
            if not is_allowed_file(file_path):
                logger.info(f"è·³è¿‡éè§†é¢‘/å­—å¹•æ–‡ä»¶: {file_path}")
                filtered_count += 1
                continue
                
            # æ¯å¤„ç†10ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if i % 10 == 0:
                logger.info(f"å¤„ç†æ–‡ä»¶ [{i+1}/{original_total_count}]: {file_path}")
                self.send_auto_delete_message(
                    update, context, 
                    f"â³ æ­£åœ¨å¤„ç†æ–‡ä»¶ [{i+1}/{original_total_count}]\næ–‡ä»¶å: {os.path.basename(file_path)}",
                    delay=5
                )
                
            elapsed = time.time() - last_request_time
            required_delay = max(0, 1.0/RATE_LIMIT - elapsed)
            if required_delay > 0:
                time.sleep(required_delay)
            
            try:
                path_parts = file_path.split('/')
                file_name = path_parts.pop()
                parent_id = self.pan_client.default_save_dir_id
                
                current_path = ""
                for part in path_parts:
                    if not part:
                        continue
                    current_path = f"{current_path}/{part}" if current_path else part
                    cache_key = f"{parent_id}/{current_path}"
                    
                    if cache_key in folder_cache:
                        parent_id = folder_cache[cache_key]
                        continue
                    
                    time.sleep(0.3)
                    folder = self.pan_client.create_folder(parent_id, part)
                    if folder:
                        folder_id = folder["FileId"]
                        folder_cache[cache_key] = folder_id
                        parent_id = folder_id
                
                etag = file_info["etag"]
                if file_info.get("is_v2_etag", False):
                    etag = FastLinkProcessor.optimized_etag_to_hex(etag, True)
                
                last_request_time = time.time()
                result = self.pan_client.rapid_upload(etag, file_size, file_name, parent_id)
                
                if result:
                    results.append({
                        "success": True,
                        "file_name": file_path,
                        "size": file_size,
                        "file_id": result["FileId"]
                    })
                else:
                    results.append({
                        "success": False,
                        "file_name": file_path,
                        "size": file_size,
                        "error": "ç§’ä¼ å¤±è´¥"
                    })
                    time.sleep(1.5)
            except (requests.exceptions.ConnectionError, ConnectionResetError) as e:
                results.append({
                    "success": False,
                    "file_name": file_path,
                    "size": file_size,
                    "error": f"ç½‘ç»œé”™è¯¯: {str(e)}"
                })
                time.sleep(3.0)
            except Exception as e:
                results.append({
                    "success": False,
                    "file_name": file_path,
                    "size": file_size,
                    "error": str(e)
                })
                time.sleep(2.0)
        
        elapsed_time = time.time() - start_time
        logger.info(f"æ–‡ä»¶è½¬å­˜å®Œæˆï¼ŒæˆåŠŸ: {sum(1 for r in results if r['success'])}, å¤±è´¥: {len(results) - sum(1 for r in results if r['success'])}")
        return results, filtered_count, elapsed_time, original_total_count, original_total_size
    
    def send_transfer_results(self, update: Update, context: CallbackContext, 
                             results, filtered_count, elapsed_time, 
                             original_total_count, original_total_size):
        """å‘é€è½¬å­˜ç»“æœ"""
        success_count = sum(1 for r in results if r["success"])
        failed_count = len(results) - success_count
        
        original_total_size_gb = original_total_size / (1024 ** 3)
        success_size = sum(r["size"] for r in results if r["success"])
        success_size_gb = success_size / (1024 ** 3)
        
        hours, remainder = divmod(elapsed_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        time_str = f"{int(minutes)}åˆ†{int(seconds)}ç§’"
        if hours > 0:
            time_str = f"{int(hours)}å°æ—¶{time_str}"
        
        result_text = (
            f"ğŸ“Š è½¬å­˜å®Œæˆï¼\n"
            f"â”œ æ–‡ä»¶æ•°é‡: {original_total_count}\n"
            f"â”œ æ–‡ä»¶å¤§å°: {original_total_size_gb:.2f} GB\n"
            f"â”œ æˆåŠŸæ•°é‡: {success_count} (å¤§å°: {success_size_gb:.2f} GB)\n"
            f"â”œ å¤±è´¥æ•°é‡: {failed_count}\n"
            f"â”œ ä¿å­˜ç›®å½•: {DEFAULT_SAVE_DIR or 'æ ¹ç›®å½•'}\n"
            f"â”” è€—æ—¶: {time_str}\n"
        )
        
        if failed_count > 0:
            failed_files = []
            for result in results:
                if not result["success"]:
                    file_name = result["file_name"]
                    failed_files.append(f"â€¢ {file_name}: {result['error']}")
            failed_text = "\n".join(failed_files[:10])
            result_text += f"\nâŒ å¤±è´¥æ–‡ä»¶:\n{failed_text}"
            if failed_count > 10:
                result_text += f"\n...åŠå…¶ä»– {failed_count - 10} ä¸ªå¤±è´¥æ–‡ä»¶"
        
        context.bot.send_message(chat_id=update.message.chat_id, text=result_text)
        logger.info("å·²å‘é€è½¬å­˜ç»“æœ")
    
    @admin_required
    def sync_full_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/sync_fullå‘½ä»¤"""
        keyboard = [[
            InlineKeyboardButton("âœ… ç¡®è®¤", callback_data='sync_full_confirm'),
            InlineKeyboardButton("âŒ å–æ¶ˆ", callback_data='sync_full_cancel')
        ]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        message = update.message.reply_text(
            "âš ï¸ ç¡®è®¤è¦æ‰§è¡Œå…¨é‡åŒæ­¥å—ï¼Ÿ\nè¿™å°†æ›´æ–°æ•´ä¸ªåª’ä½“åº“çš„ç›®å½•ç¼“å­˜ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚",
            reply_markup=reply_markup
        )
        context.user_data['confirmation_message_id'] = message.message_id
        logger.info("æ”¶åˆ°/sync_fullå‘½ä»¤")

    def button_callback(self, update: Update, context: CallbackContext):
        """å¤„ç†æŒ‰é’®å›è°ƒ"""
        query = update.callback_query
        query.answer()
        data = query.data
        
        if data.startswith("export_"):
            self.export_choice_callback(update, context)
        elif data.startswith("sync_full_"):
            chat_id = query.message.chat_id
            message_id = query.message.message_id
            try:
                context.bot.delete_message(chat_id=chat_id, message_id=message_id)
            except Exception:
                pass
            
            if data == 'sync_full_confirm':
                self.execute_full_sync(update, context)
            else:
                context.bot.send_message(chat_id=chat_id, text="âŒ å…¨é‡åŒæ­¥å·²å–æ¶ˆ")
                logger.info("å…¨é‡åŒæ­¥å·²å–æ¶ˆ")

    def execute_full_sync(self, update: Update, context: CallbackContext):
        """æ‰§è¡Œå…¨é‡åŒæ­¥"""
        chat_id = getattr(context, '_chat_id', None)
        self.send_auto_delete_message(update, context, "ğŸ”„ æ­£åœ¨æ‰§è¡Œå…¨é‡åŒæ­¥...", chat_id=chat_id)
        logger.info("å¼€å§‹æ‰§è¡Œå…¨é‡åŒæ­¥")
        
        try:
            start_time = time.time()
            update_count = self.pan_client.full_sync_directory_cache()
            elapsed = time.time() - start_time
            self.send_auto_delete_message(
                update, context, 
                f"âœ… å…¨é‡åŒæ­¥å®Œæˆï¼\nâ”œ æ›´æ–°ç›®å½•: {update_count} ä¸ª\nâ”” è€—æ—¶: {elapsed:.2f}ç§’",
                chat_id=chat_id
            )
            logger.info(f"å…¨é‡åŒæ­¥å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
        except Exception as e:
            logger.error(f"å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}")
            self.send_auto_delete_message(update, context, "âŒ å…¨é‡åŒæ­¥å¤±è´¥", chat_id=chat_id)
            
        if hasattr(context, '_chat_id'):
            del context._chat_id

    @admin_required
    def clear_trash_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/clear_trashå‘½ä»¤"""
        logger.info("æ”¶åˆ°/clear_trashå‘½ä»¤")
        self.send_auto_delete_message(update, context, "ğŸ”„ æ­£åœ¨æ¸…ç©ºå›æ”¶ç«™...")
        try:
            if self.pan_client.clear_trash():
                self.send_auto_delete_message(update, context, "âœ… å›æ”¶ç«™å·²æˆåŠŸæ¸…ç©º", delay=5)
                logger.info("å›æ”¶ç«™å·²æ¸…ç©º")
            else:
                self.send_auto_delete_message(update, context, "âŒ æ¸…ç©ºå›æ”¶ç«™å¤±è´¥", delay=5)
                logger.warning("æ¸…ç©ºå›æ”¶ç«™å¤±è´¥")
        except Exception as e:
            logger.error(f"æ¸…ç©ºå›æ”¶ç«™å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, "âŒ æ¸…ç©ºå›æ”¶ç«™æ—¶å‡ºé”™", delay=5)

    @admin_required
    def process_share_link(self, update: Update, context: CallbackContext, share_url):
        """å¤„ç†123äº‘ç›˜åˆ†äº«é“¾æ¥ï¼ˆä¿ç•™ç›®å½•ç»“æ„ï¼‰"""
        try:
            logger.info(f"å¤„ç†åˆ†äº«é“¾æ¥: {share_url}")
            self.send_auto_delete_message(update, context, "ğŸ”— æ­£åœ¨è§£æåˆ†äº«é“¾æ¥...")
            
            # åœ¨åå°çº¿ç¨‹ä¸­å¤„ç†è½¬å­˜
            def do_share_transfer():
                try:
                    start_time = time.time()
                    # æ³¨æ„ï¼šç°åœ¨è¿”å›å››ä¸ªå€¼ï¼ŒåŒ…æ‹¬æ€»å¤§å°
                    success, failure, results, total_size = self.pan_client.save_share_files(
                        share_url, 
                        self.pan_client.default_save_dir_id
                    )
                    elapsed = time.time() - start_time
                    
                    # è®¡ç®—æ€»å¤§å°ï¼ˆGBï¼‰
                    total_size_gb = total_size / (1024 ** 3)
                    
                    # æ„å»ºç»“æœæ¶ˆæ¯
                    message = (
                        f"ğŸ“¦ åˆ†äº«é“¾æ¥è½¬å­˜å®Œæˆï¼\n"
                        f"â”œ æˆåŠŸ: {success} æ–‡ä»¶\n"
                        f"â”œ å¤±è´¥: {failure} æ–‡ä»¶\n"
                        f"â”œ æ€»å¤§å°: {total_size_gb:.2f} GB\n"
                        f"â”œ ä¿å­˜åˆ°: {DEFAULT_SAVE_DIR}\n"
                        f"â”” è€—æ—¶: {elapsed:.1f}ç§’"
                    )
                    
                    context.bot.send_message(
                        chat_id=update.message.chat_id, 
                        text=message
                    )
                    
                    # å¦‚æœæœ‰å¤±è´¥ï¼Œå‘é€å¤±è´¥è¯¦æƒ…
                    if failure > 0:
                        failed_list = "\n".join(
                            [f"â€¢ {r['file_name']}: {r.get('error', 'æœªçŸ¥é”™è¯¯')}" 
                             for r in results if not r['success']][:5]
                        )
                        if failure > 5:
                            failed_list += f"\n...åŠå…¶ä»–{failure-5}ä¸ªæ–‡ä»¶"
                        
                        context.bot.send_message(
                            chat_id=update.message.chat_id,
                            text=f"âŒ å¤±è´¥æ–‡ä»¶:\n{failed_list}",
                            parse_mode="Markdown"
                        )
                    
                except Exception as e:
                    logger.error(f"å¤„ç†åˆ†äº«é“¾æ¥å‡ºé”™: {str(e)}")
                    self.send_auto_delete_message(
                        update, context, 
                        f"âŒ å¤„ç†åˆ†äº«é“¾æ¥æ—¶å‡ºé”™: {str(e)}",
                        chat_id=update.message.chat_id
                    )
            
            # å¯åŠ¨åå°çº¿ç¨‹å¤„ç†
            threading.Thread(target=do_share_transfer).start()
            self.send_auto_delete_message(
                update, context, 
                "â³ æ­£åœ¨åå°è½¬å­˜æ–‡ä»¶å¹¶ä¿ç•™ç›®å½•ç»“æ„ï¼Œè¯·ç¨å€™...\nå®Œæˆåä¼šé€šçŸ¥ç»“æœ",
                delay=5
            )
            
        except Exception as e:
            logger.error(f"å¤„ç†åˆ†äº«é“¾æ¥å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†åˆ†äº«é“¾æ¥æ—¶å‡ºé”™: {str(e)}")

    @admin_required
    def handle_text(self, update: Update, context: CallbackContext):
        """å¤„ç†æ–‡æœ¬æ¶ˆæ¯ - ä»…ä¿ç•™ç§’ä¼ é“¾æ¥å¤„ç†"""
        text = update.message.text.strip()
        
        if (text.startswith(LEGACY_FOLDER_LINK_PREFIX_V1) or 
            text.startswith(LEGACY_FOLDER_LINK_PREFIX_V2) or 
            text.startswith(COMMON_PATH_LINK_PREFIX_V1) or 
            text.startswith(COMMON_PATH_LINK_PREFIX_V2) or
            ('#' in text and '$' in text)):
            self.send_auto_delete_message(update, context, "ğŸ” æ£€æµ‹åˆ°ç§’ä¼ é“¾æ¥ï¼Œå¼€å§‹è§£æ...")
            self.process_fast_link(update, context, text)
        # å¤„ç†123äº‘ç›˜åˆ†äº«é“¾æ¥ï¼ˆä½¿ç”¨æ”¹è¿›çš„æ­£åˆ™åŒ¹é…ï¼‰
        elif re.search(r'https?://(?:[a-zA-Z0-9-]+\.)*123[a-zA-Z0-9-]*\.[a-z]{2,6}/s/[a-zA-Z0-9\-_]+', text):
            self.send_auto_delete_message(update, context, "ğŸ”— æ£€æµ‹åˆ°123äº‘ç›˜åˆ†äº«é“¾æ¥ï¼Œå¼€å§‹è§£æ...")
            self.process_share_link(update, context, text)

def main():
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    BOT_TOKEN = os.getenv("TG_BOT_TOKEN","")
    CLIENT_ID = os.getenv("PAN_CLIENT_ID","")
    CLIENT_SECRET = os.getenv("PAN_CLIENT_SECRET","")
    ADMIN_USER_IDS = [int(id.strip()) for id in os.getenv("TG_ADMIN_USER_IDS", "").split(",") if id.strip()]
    
    if not BOT_TOKEN:
        logger.error("âŒ ç¯å¢ƒå˜é‡ TG_BOT_TOKEN æœªè®¾ç½®")
        return
    
    if not CLIENT_ID:
        logger.error("âŒ ç¯å¢ƒå˜é‡ PAN_CLIENT_ID æœªè®¾ç½®")
        return
    
    if not CLIENT_SECRET:
        logger.error("âŒ ç¯å¢ƒå˜é‡ PAN_CLIENT_SECRET æœªè®¾ç½®")
        return
    
    logger.info("åˆå§‹åŒ–123äº‘ç›˜å®¢æˆ·ç«¯...")
    pan_client = Pan123Client(CLIENT_ID, CLIENT_SECRET)
    
    if not pan_client.token_manager.access_token:
        logger.error("âŒ æ— æ³•è·å–æœ‰æ•ˆçš„Token")
        return
    
    logger.info("åˆå§‹åŒ–Telegramæœºå™¨äºº...")
    bot_handler = TelegramBotHandler(BOT_TOKEN, pan_client, ADMIN_USER_IDS)
    logger.info("æœºå™¨äººå¯åŠ¨ä¸­...")
    bot_handler.start()

if __name__ == "__main__":
    main()
