import warnings
warnings.filterwarnings("ignore", message="python-telegram-bot is using upstream urllib3.*")
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*", category=UserWarning)
import os
import re
import json
import time
import logging
import requests
import sqlite3
import threading
from contextlib import closing
from datetime import datetime, timedelta, timezone
from telegram import Update, BotCommand, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Updater, 
    MessageHandler, 
    Filters, 
    CallbackContext, 
    CommandHandler,
    CallbackQueryHandler
)
from functools import wraps
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import binascii

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å¿½ç•¥ç¬¬ä¸‰æ–¹åº“çš„è­¦å‘Š
logging.getLogger("telegram").setLevel(logging.WARNING)
logging.getLogger("apscheduler").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

# ====================== é…ç½®åŒºåŸŸ ======================
# æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (ä½¿ç”¨ç›¸å¯¹è·¯å¾„)
DB_PATH = os.getenv("DB_PATH", "bot123.db")

# 123äº‘ç›˜APIé…ç½®
PAN_HOST = "https://www.123pan.com"
API_PATHS = {
    "TOKEN": "/api/v1/access_token",
    "USER_INFO": "/api/v1/user/info",  # è·å–ç”¨æˆ·ä¿¡æ¯
    "LIST_FILES_V2": "/api/v2/file/list",
    "FILE_INFOS": "/api/v1/file/infos",
    "UPLOAD_REQUEST": "/b/api/file/upload_request",
}

# å¼€æ”¾å¹³å°åœ°å€
OPEN_API_HOST = "https://open-api.123pan.com"

# ç§’ä¼ é“¾æ¥å‰ç¼€
LEGACY_FOLDER_LINK_PREFIX_V1 = "123FSLinkV1$"
LEGACY_FOLDER_LINK_PREFIX_V2 = "123FSLinkV2$"
COMMON_PATH_LINK_PREFIX_V1 = "123FLCPV1$"
COMMON_PATH_LINK_PREFIX_V2 = "123FLCPV2$"
COMMON_PATH_DELIMITER = "%"

# Base62å­—ç¬¦é›†
BASE62_CHARS = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"

# ç¯å¢ƒå˜é‡é…ç½®
DEFAULT_SAVE_DIR = os.getenv("DEFAULT_SAVE_DIR", "").strip()  # é»˜è®¤ä¿å­˜ç›®å½•
EXPORT_BASE_DIRS = [d.strip() for d in os.getenv("EXPORT_BASE_DIR", "").split(';') if d.strip()]  # å¤šä¸ªå¯¼å‡ºåŸºç›®å½•
SEARCH_MAX_DEPTH = int(os.getenv("SEARCH_MAX_DEPTH", ""))         # æœç´¢æ–‡ä»¶å¤¹çš„æœ€å¤§æ·±åº¦
# =====================================================

def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    try:
        with closing(sqlite3.connect(DB_PATH)) as conn:
            c = conn.cursor()
            # åˆ›å»ºTokenç¼“å­˜è¡¨
            c.execute('''CREATE TABLE IF NOT EXISTS token_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                access_token TEXT NOT NULL,
                client_id TEXT NOT NULL,
                client_secret TEXT NOT NULL,
                expired_at TIMESTAMP NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )''')
            
            # åˆ›å»ºç›®å½•ç¼“å­˜è¡¨
            c.execute('''CREATE TABLE IF NOT EXISTS directory_cache (
                file_id INTEGER PRIMARY KEY,
                filename TEXT NOT NULL,
                parent_id INTEGER NOT NULL,
                full_path TEXT NOT NULL,
                base_dir_id INTEGER NOT NULL,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )''')
            
            # åˆ›å»ºç´¢å¼•ä»¥åŠ é€Ÿæœç´¢
            c.execute('''CREATE INDEX IF NOT EXISTS idx_filename ON directory_cache (filename)''')
            c.execute('''CREATE INDEX IF NOT EXISTS idx_full_path ON directory_cache (full_path)''')
            c.execute('''CREATE INDEX IF NOT EXISTS idx_base_dir ON directory_cache (base_dir_id)''')
            
            conn.commit()
            #logger.info(f"æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {DB_PATH}")
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")

init_db()

class TokenManager:
    """ç®¡ç†API tokençš„è·å–å’Œç¼“å­˜"""
    def __init__(self, client_id, client_secret):
        self.client_id = client_id
        self.client_secret = client_secret
        self.session = self._create_session()
        self.access_token = None
        self.token_expiry = None
        self.start_time = datetime.now()  # è®°å½•å¯åŠ¨æ—¶é—´
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½Token
        if not self.load_token_from_cache():
            logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜Tokenï¼Œå°†è·å–æ–°Token")
            self.get_new_token()
    
    def _create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=100,
            pool_maxsize=100
        )
        
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        
        # ç¦ç”¨SSLéªŒè¯
        session.verify = False
        return session
    
    def load_token_from_cache(self):
        """ä»æ•°æ®åº“åŠ è½½ç¼“å­˜çš„Token"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                c.execute("SELECT access_token, client_id, client_secret, expired_at FROM token_cache ORDER BY id DESC LIMIT 1")
                row = c.fetchone()
                
                if row:
                    token, cached_id, cached_secret, expired_at_str = row
                    expired_at = datetime.fromisoformat(expired_at_str).astimezone(timezone.utc)
                    now = datetime.now(timezone.utc)
                    
                    # æ£€æŸ¥Tokenæ˜¯å¦æœ‰æ•ˆä¸”å‡­è¯æœªå˜åŒ–
                    if (expired_at > now + timedelta(minutes=5) and
                        self.client_id == cached_id and 
                        self.client_secret == cached_secret):
                        
                        self.access_token = token
                        self.token_expiry = expired_at
                        
                        expires_in = int((expired_at - now).total_seconds())
                        logger.info(
                            "ä½¿ç”¨ç¼“å­˜Token\n"
                            "â”œâ”€ æœ‰æ•ˆæœŸè‡³ï¼š%s (UTC)\n"
                            "â””â”€ å‰©ä½™æ—¶é—´ï¼š%då°æ—¶%dåˆ†é’Ÿ",
                            expired_at.strftime("%Y-%m-%d %H:%M:%S"),
                            expires_in // 3600,
                            (expires_in % 3600) // 60
                        )
                        return True
                    else:
                        logger.info("ç¼“å­˜Tokenæ— æ•ˆæˆ–å·²è¿‡æœŸ")
                else:
                    logger.info("æœªæ‰¾åˆ°ç¼“å­˜Token")
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜Tokenå¤±è´¥: {str(e)}")
        return False
    
    def save_token_to_cache(self, access_token, expired_at):
        """ä¿å­˜Tokenåˆ°æ•°æ®åº“"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                # æ¸…é™¤æ—§Token
                c.execute("DELETE FROM token_cache")
                # æ’å…¥æ–°Token
                c.execute('''INSERT INTO token_cache 
                           (access_token, client_id, client_secret, expired_at)
                           VALUES (?,?,?,?)''',
                           (access_token, self.client_id, self.client_secret, expired_at.isoformat()))
                conn.commit()
                logger.info("Tokenå·²ä¿å­˜åˆ°ç¼“å­˜")
                return True
        except Exception as e:
            logger.error(f"ä¿å­˜Tokenåˆ°ç¼“å­˜å¤±è´¥: {str(e)}")
        return False
    
    def get_new_token(self):
        """è·å–æ–°tokenï¼ˆä½¿ç”¨å¼€æ”¾å¹³å°APIï¼‰"""
        try:
            logger.info("æ­£åœ¨è·å–æ–°Token...")
            url = f"{OPEN_API_HOST}{API_PATHS['TOKEN']}"
            payload = {
                "clientID": self.client_id,
                "clientSecret": self.client_secret
            }
            
            headers = {
                "Content-Type": "application/json",
                "Platform": "open_platform"
            }
            
            response = self.session.post(
                url,
                json=payload,
                headers=headers,
                timeout=20
            )
            
            if response.status_code != 200:
                logger.error(f"è®¤è¯å¤±è´¥: {response.status_code}")
                return False
            
            data = response.json()
            
            if data.get("code") != 0:
                logger.error(f"APIé”™è¯¯: {data.get('code')} - {data.get('message')}")
                return False
            
            # æå–å¹¶ä¿å­˜token
            self.access_token = data["data"]["accessToken"]
            
            # è§£æè¿‡æœŸæ—¶é—´å­—ç¬¦ä¸²
            expired_at_str = data["data"]["expiredAt"]
            
            # ä¿®å¤æ—¶é—´è§£æé—®é¢˜
            if expired_at_str.endswith('Z'):
                self.token_expiry = datetime.fromisoformat(expired_at_str[:-1]).replace(tzinfo=timezone.utc)
            elif '+' in expired_at_str or '-' in expired_at_str:
                dt = datetime.fromisoformat(expired_at_str)
                self.token_expiry = dt.astimezone(timezone.utc)
            else:
                self.token_expiry = datetime.fromisoformat(expired_at_str).replace(tzinfo=timezone.utc)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.save_token_to_cache(self.access_token, self.token_expiry):
                logger.info(f"æ›´æ–°Token\nâ””â”€æœ‰æ•ˆæœŸè‡³: {self.token_expiry} (UTC)")
                return True
            else:
                logger.error("Tokenä¿å­˜åˆ°ç¼“å­˜å¤±è´¥")
                return False
            
        except Exception as e:
            logger.error(f"è·å–Tokenå¤±è´¥: {str(e)}")
            return False
    
    def ensure_token_valid(self):
        """ç¡®ä¿tokenæœ‰æ•ˆ"""
        current_time = datetime.now(timezone.utc)
        if not self.access_token or not self.token_expiry or current_time >= self.token_expiry - timedelta(minutes=5):
            logger.info("Tokenæ— æ•ˆæˆ–å³å°†è¿‡æœŸï¼Œåˆ·æ–°ä¸­...")
            return self.get_new_token()
        return True
    
    def get_auth_header(self):
        """è·å–è®¤è¯å¤´"""
        if not self.ensure_token_valid():
            raise Exception("æ— æ³•è·å–æœ‰æ•ˆçš„Token")
        return {
            "Authorization": f"Bearer {self.access_token}",
            "Platform": "open_platform",
            "Content-Type": "application/json"
        }

class Pan123Client:
    def __init__(self, client_id, client_secret):
        self.token_manager = TokenManager(client_id, client_secret)
        self.session = self._create_session()
        self.last_api_call = 0  # è®°å½•æœ€åä¸€æ¬¡APIè°ƒç”¨æ—¶é—´
        self.api_rate_limit = 2  # é™ä½APIè°ƒç”¨é¢‘ç‡
        self.retry_delay = 2.0  # å¢åŠ é™æµæ—¶é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        
        # åˆå§‹åŒ–é»˜è®¤ç›®å½•ID
        self.default_save_dir_id = 0  # æ ¹ç›®å½•
        self.export_base_dir_ids = []   # å­˜å‚¨å¤šä¸ªåŸºç›®å½•ID
        self.export_base_dir_map = {0: "æ ¹ç›®å½•"}  # åŸºç›®å½•IDåˆ°è·¯å¾„çš„æ˜ å°„
        
        # è®¾ç½®é»˜è®¤ä¿å­˜ç›®å½•
        if DEFAULT_SAVE_DIR:
            self.default_save_dir_id = self.get_or_create_directory(DEFAULT_SAVE_DIR)
            logger.info(f"é»˜è®¤ä¿å­˜ç›®å½•å·²è®¾ç½®: '{DEFAULT_SAVE_DIR}' (ID: {self.default_save_dir_id})")
        
        # è®¾ç½®å¤šä¸ªå¯¼å‡ºåŸºç›®å½•
        for base_dir in EXPORT_BASE_DIRS:
            base_dir_id = self.get_or_create_directory(base_dir)
            self.export_base_dir_ids.append(base_dir_id)
            self.export_base_dir_map[base_dir_id] = base_dir
            logger.info(f"å¯¼å‡ºåŸºç›®å½•å·²è®¾ç½®: '{base_dir}' (ID: {base_dir_id})")
        
        # è®¾ç½®æœç´¢æœ€å¤§æ·±åº¦
        self.search_max_depth = SEARCH_MAX_DEPTH
        logger.info(f"æœç´¢æœ€å¤§æ·±åº¦å·²è®¾ç½®: {self.search_max_depth} å±‚")
        
        # åˆå§‹åŒ–ç›®å½•ç¼“å­˜
        self.directory_cache = {}
        self.load_directory_cache()
    
    def _create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=100,
            pool_maxsize=100
        )
        
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        
        # ç¦ç”¨SSLéªŒè¯
        session.verify = False
        return session
    
    def get_or_create_directory(self, path):
        """è·å–æˆ–åˆ›å»ºç›®å½•è·¯å¾„"""
        parent_id = 0  # ä»æ ¹ç›®å½•å¼€å§‹
        parts = path.strip('/').split('/')
        
        for part in parts:
            if not part:
                continue
                
            # æœç´¢ç›®å½•
            folder_info = self.search_folder(part, parent_id)
            
            if folder_info:
                parent_id = folder_info["fileId"]
                logger.debug(f"æ‰¾åˆ°ç›®å½•: '{part}' (ID: {parent_id})")
            else:
                # åˆ›å»ºç›®å½•
                folder = self.create_folder(parent_id, part)
                if not folder:
                    logger.error(f"æ— æ³•åˆ›å»ºç›®å½•: '{part}'")
                    return parent_id  # è¿”å›ä¸Šä¸€çº§å¯ç”¨ç›®å½•
                parent_id = folder["FileId"]
                logger.info(f"å·²åˆ›å»ºç›®å½•: '{part}' (ID: {parent_id})")
        
        return parent_id
    
    def search_folder(self, folder_name, parent_id=0):
        """åœ¨æŒ‡å®šçˆ¶ç›®å½•ä¸‹æœç´¢æ–‡ä»¶å¤¹ï¼ˆéé€’å½’ï¼‰"""
        try:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": parent_id,
                "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                "limit": 100,
                "lastFileId": 0
            }
            headers = self.token_manager.get_auth_header()
            
            response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
            if not response or response.status_code != 200:
                return None
                
            data = response.json()
            if data.get("code") != 0:
                return None
                
            # æ£€æŸ¥å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹
            for item in data["data"].get("fileList", []):
                if item["type"] == 1 and item["filename"] == folder_name:
                    return {
                        "fileId": item["fileId"],
                        "filename": item["filename"]
                    }
                    
        except Exception as e:
            logger.error(f"æœç´¢ç›®å½•å‡ºé”™: {str(e)}")
            
        return None

    # æ·»åŠ APIè°ƒç”¨æ§åˆ¶æ–¹æ³•
    def _call_api(self, method, url, **kwargs):
        """æ§åˆ¶APIè°ƒç”¨é¢‘ç‡ï¼Œé¿å…é™æµ"""
        retry_count = 0
        max_retries = 3
        
        while retry_count < max_retries:
            try:
                # è®¡ç®—è·ç¦»ä¸Šæ¬¡è°ƒç”¨çš„æ—¶é—´
                elapsed = time.time() - self.last_api_call
                required_delay = 1.0 / self.api_rate_limit
                
                # å¦‚æœè°ƒç”¨è¿‡å¿«ï¼Œç­‰å¾…è¶³å¤Ÿçš„æ—¶é—´
                if elapsed < required_delay:
                    wait_time = required_delay - elapsed
                    logger.debug(f"APIè°ƒç”¨è¿‡å¿«ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                    time.sleep(wait_time)
                
                # å‘é€APIè¯·æ±‚
                response = self.session.request(method, url, **kwargs)
                self.last_api_call = time.time()
                
                # æ£€æŸ¥æ˜¯å¦è¢«é™æµ
                if response.status_code == 429:
                    logger.warning(f"APIé™æµï¼Œç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                    time.sleep(self.retry_delay)
                    continue
                
                return response
                
            except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:
                retry_count += 1
                logger.error(f"âŒ SSL/è¿æ¥é”™è¯¯: {str(e)}ï¼Œé‡è¯• {retry_count}/{max_retries}")
                time.sleep(2 ** retry_count)  # æŒ‡æ•°é€€é¿
            except Exception as e:
                logger.error(f"APIè°ƒç”¨å‡ºé”™: {str(e)}")
                return None
        
        logger.error(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}")
        return None
    
    def _get_auth_headers(self):
        """è·å–è®¤è¯å¤´ï¼ˆæ·»åŠ åŸå§‹è„šæœ¬ä¸­çš„é¢å¤–å¤´ä¿¡æ¯ï¼‰"""
        auth_header = self.token_manager.get_auth_header()
        return {
            **auth_header,
            "platform": "web",
            "App-Version": "3",
            "Origin": PAN_HOST,
            "Referer": f"{PAN_HOST}/",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
        }
    
    def get_user_info(self):
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        try:
            if not self.token_manager.ensure_token_valid():
                logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„Token")
                return None
                
            url = f"{OPEN_API_HOST}{API_PATHS['USER_INFO']}"
            headers = self.token_manager.get_auth_header()
            
            # ä½¿ç”¨é™æµä¿æŠ¤çš„APIè°ƒç”¨
            response = self._call_api("GET", url, headers=headers, timeout=30)
            if not response or response.status_code != 200:
                logger.error(f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: HTTP {response.status_code if response else 'æ— å“åº”'}")
                return None
                
            data = response.json()
            if data.get("code") != 0:
                logger.error(f"APIé”™è¯¯: {data.get('code')} - {data.get('message')}")
                return None
                
            return data.get("data")
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ä¿¡æ¯å‡ºé”™: {str(e)}")
            return None
    
    def create_folder(self, parent_id, folder_name, retry_count=3):
        """åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        logger.info(f"åˆ›å»ºæ–‡ä»¶å¤¹: '{folder_name}' (çˆ¶ID: {parent_id})")
        
        for attempt in range(retry_count):
            try:
                url = f"{PAN_HOST}{API_PATHS['UPLOAD_REQUEST']}"
                payload = {
                    "driveId": 0,
                    "etag": "",
                    "fileName": folder_name,
                    "parentFileId": int(parent_id),
                    "size": 0,
                    "type": 1,
                    "NotReuse": True,
                    "RequestSource": None,
                    "duplicate": 1,
                    "event": "newCreateFolder",
                    "operateType": 1
                }
                headers = self._get_auth_headers()
                
                # ä½¿ç”¨æ›´å¥å£®çš„è¯·æ±‚æ–¹å¼
                response = self.session.post(
                    url, 
                    json=payload, 
                    headers=headers, 
                    timeout=20,
                    verify=False  # æ˜ç¡®ç¦ç”¨SSLéªŒè¯
                )
                
                data = response.json()
                
                if data.get("code") == 0 and data["data"].get("Info", {}).get("FileId"):
                    folder_id = data["data"]["Info"]["FileId"]
                    logger.info(f"âœ… æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ: '{folder_name}' (ID: {folder_id})")
                    return data["data"]["Info"]
                else:
                    error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {error_msg}")
                    if attempt < retry_count - 1:
                        time.sleep(1)  # ç­‰å¾…åé‡è¯•
                        continue
                    return None
            except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:
                logger.error(f"âŒ SSL/è¿æ¥é”™è¯¯: {str(e)}")
                if attempt < retry_count - 1:
                    logger.info(f"ç­‰å¾…1ç§’åé‡è¯• ({attempt+1}/{retry_count})...")
                    time.sleep(1)
                    continue
                return None
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºæ–‡ä»¶å¤¹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                if attempt < retry_count - 1:
                    time.sleep(1)
                    continue
                return None
        return None
    
    def rapid_upload(self, etag, size, file_name, parent_id, retry_count=3):
        """ç§’ä¼ æ–‡ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        logger.info(f"å°è¯•ç§’ä¼ æ–‡ä»¶: '{file_name}' (å¤§å°: {size} bytes, çˆ¶ID: {parent_id})")
        
        # ä¿å­˜åŸå§‹Etag
        original_etag = etag
        
        # å¦‚æœEtagé•¿åº¦æ˜¯32ä½ä¸”æ˜¯åå…­è¿›åˆ¶ï¼Œç›´æ¥ä½¿ç”¨
        if len(etag) == 32 and all(c in '0123456789abcdef' for c in etag.lower()):
            logger.info(f"Etagæ˜¯æœ‰æ•ˆçš„MD5æ ¼å¼: {etag}")
        else:
            # å°è¯•è½¬æ¢ä¸ºMD5æ ¼å¼
            etag = FastLinkProcessor.optimized_etag_to_hex(etag, True)
            logger.info(f"è½¬æ¢åEtag: {etag}")
        
        for attempt in range(retry_count):
            try:
                url = f"{PAN_HOST}{API_PATHS['UPLOAD_REQUEST']}"
                payload = {
                    "driveId": 0,
                    "etag": etag,
                    "fileName": file_name,
                    "parentFileId": int(parent_id),
                    "size": int(size),
                    "type": 0,
                    "NotReuse": False,
                    "RequestSource": None,
                    "duplicate": 1,
                    "event": "rapidUpload",
                    "operateType": 1
                }
                headers = self._get_auth_headers()
                
                # ä½¿ç”¨æ›´å¥å£®çš„è¯·æ±‚æ–¹å¼
                response = self.session.post(
                    url, 
                    json=payload, 
                    headers=headers, 
                    timeout=20,
                    verify=False  # æ˜ç¡®ç¦ç”¨SSLéªŒè¯
                )
                
                data = response.json()
                
                if data.get("code") == 0 and data["data"].get("Info", {}).get("FileId"):
                    file_id = data["data"]["Info"]["FileId"]
                    logger.info(f"âœ… æ–‡ä»¶ç§’ä¼ æˆåŠŸ: '{file_name}' (ID: {file_id})")
                    return data["data"]["Info"]
                else:
                    error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ æ–‡ä»¶ç§’ä¼ å¤±è´¥: {error_msg}")
                    
                    # å¦‚æœæ˜¯Etagæ ¼å¼é—®é¢˜ï¼Œå°è¯•ä½¿ç”¨åŸå§‹Etag
                    if "etag" in error_msg.lower() and etag != original_etag:
                        logger.info(f"âš ï¸ å°è¯•ä½¿ç”¨åŸå§‹Etag: {original_etag}")
                        etag = original_etag  # ä¸‹æ¬¡é‡è¯•ä½¿ç”¨åŸå§‹Etag
                        continue
                    
                    if attempt < retry_count - 1:
                        time.sleep(1)
                        continue
                    return None
            except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:
                logger.error(f"âŒ SSL/è¿æ¥é”™è¯¯: {str(e)}")
                if attempt < retry_count - 1:
                    logger.info(f"ç­‰å¾…1ç§’åé‡è¯• ({attempt+1}/{retry_count})...")
                    time.sleep(1)
                    continue
                return None
            except Exception as e:
                logger.error(f"âŒ ç§’ä¼ è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                if attempt < retry_count - 1:
                    time.sleep(1)
                    continue
                return None
        return None
    
    def load_directory_cache(self):
        """ä»æ•°æ®åº“åŠ è½½ç›®å½•ç¼“å­˜"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                conn.row_factory = sqlite3.Row
                c = conn.cursor()
                
                if not self.export_base_dir_ids:
                    c.execute("SELECT * FROM directory_cache")
                else:
                    placeholders = ','.join(['?'] * len(self.export_base_dir_ids))
                    c.execute(f"SELECT * FROM directory_cache WHERE base_dir_id IN ({placeholders})", 
                              self.export_base_dir_ids)
                
                rows = c.fetchall()
                
                for row in rows:
                    file_id = row["file_id"]
                    self.directory_cache[file_id] = dict(row)
                
                logger.info(f"å·²åŠ è½½ {len(rows)} ä¸ªç›®å½•ç¼“å­˜ (å¯¼å‡ºåŸºç›®å½•ID: {self.export_base_dir_ids})")
        except Exception as e:
            logger.error(f"åŠ è½½ç›®å½•ç¼“å­˜å¤±è´¥: {str(e)}")
    
    def update_directory_cache(self, file_id, filename, parent_id, full_path, base_dir_id):
        """æ›´æ–°ç›®å½•ç¼“å­˜"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if file_id in self.directory_cache:
                existing = self.directory_cache[file_id]
                if (existing["filename"] == filename and 
                    existing["parent_id"] == parent_id and 
                    existing["full_path"] == full_path and
                    existing["base_dir_id"] == base_dir_id):
                    return False  # æ— å˜åŒ–ï¼Œæ— éœ€æ›´æ–°
            
            # æ›´æ–°å†…å­˜ç¼“å­˜
            cache_entry = {
                "file_id": file_id,
                "filename": filename,
                "parent_id": parent_id,
                "full_path": full_path,
                "base_dir_id": base_dir_id
            }
            self.directory_cache[file_id] = cache_entry
            
            # æ›´æ–°æ•°æ®åº“
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                # ä½¿ç”¨INSERT OR REPLACEç¡®ä¿æ›´æ–°
                c.execute('''INSERT OR REPLACE INTO directory_cache 
                            (file_id, filename, parent_id, full_path, base_dir_id) 
                            VALUES (?,?,?,?,?)''',
                          (file_id, filename, parent_id, full_path, base_dir_id))
                conn.commit()
            
            logger.info(f"æ›´æ–°ç›®å½•ç¼“å­˜: {filename} (ID: {file_id}, è·¯å¾„: {full_path}, åŸºç›®å½•ID: {base_dir_id})")
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°ç›®å½•ç¼“å­˜å¤±è´¥: {str(e)}")
            return False
    
    def remove_from_directory_cache(self, file_id):
        """ä»ç¼“å­˜ä¸­ç§»é™¤ç›®å½•"""
        try:
            if file_id in self.directory_cache:
                del self.directory_cache[file_id]
            
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                c.execute("DELETE FROM directory_cache WHERE file_id = ?", (file_id,))
                conn.commit()
            
            logger.info(f"å·²ä»ç¼“å­˜ä¸­ç§»é™¤ç›®å½•: {file_id}")
            return True
        except Exception as e:
            logger.error(f"ä»ç¼“å­˜ä¸­ç§»é™¤ç›®å½•å¤±è´¥: {str(e)}")
            return False
    
    def search_in_cache(self, folder_name, parent_id=None):
        """åœ¨ç¼“å­˜ä¸­æœç´¢ç›®å½•"""
        results = []
        for file_id, cache in self.directory_cache.items():
            if cache["filename"] == folder_name:
                # å¦‚æœæŒ‡å®šäº†çˆ¶ç›®å½•IDï¼Œåˆ™æ£€æŸ¥æ˜¯å¦åŒ¹é…
                if parent_id is not None and cache["parent_id"] != parent_id:
                    continue
                results.append(cache)
        
        # æŒ‰è·¯å¾„é•¿åº¦æ’åºï¼ˆè¾ƒçŸ­çš„è·¯å¾„å¯èƒ½æ›´æ¥è¿‘æ ¹ç›®å½•ï¼‰
        results.sort(key=lambda x: len(x["full_path"]))
        
        logger.debug(f"åœ¨ç¼“å­˜ä¸­æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç›®å½•: '{folder_name}'")
        return results
    
    def full_sync_directory_cache(self):
        """å…¨é‡åŒæ­¥ç›®å½•ç¼“å­˜"""
        logger.info("å¼€å§‹å…¨é‡åŒæ­¥ç›®å½•ç¼“å­˜...")
        
        try:
            update_count = 0
            # åŒæ­¥æ ¹ç›®å½•
            update_count += self.sync_directory(0, "æ ¹ç›®å½•", base_dir_id=0)
            
            # åŒæ­¥æ‰€æœ‰å¯¼å‡ºåŸºç›®å½•
            for base_dir_id in self.export_base_dir_ids:
                base_dir_path = self.export_base_dir_map.get(base_dir_id, f"åŸºç›®å½•({base_dir_id})")
                update_count += self.sync_directory(base_dir_id, base_dir_path, base_dir_id=base_dir_id)
                
            logger.info(f"å…¨é‡åŒæ­¥å®Œæˆï¼Œæ›´æ–° {update_count} ä¸ªç›®å½•")
            return update_count
        except Exception as e:
            logger.error(f"å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}")
            return 0
    
    def sync_directory(self, directory_id, current_path, base_dir_id, current_depth=0):
        """åŒæ­¥æŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•"""
        logger.info(f"å¼€å§‹åŒæ­¥ç›®å½•: '{current_path}' (ID: {directory_id}, æ·±åº¦: {current_depth})")
        update_count = 0
        last_file_id = 0
        
        while True:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": directory_id,
                "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                "limit": 100,
                "lastFileId": last_file_id
            }
            headers = self.token_manager.get_auth_header()
            
            try:
                logger.debug(f"è¯·æ±‚ç›®å½•åˆ—è¡¨: {url}, å‚æ•°: {params}")
                response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
                
                if not response or response.status_code != 200:
                    logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: HTTP {response.status_code if response else 'æ— å“åº”'}")
                    break
                
                data = response.json()
                if data.get("code") != 0:
                    logger.error(f"APIé”™è¯¯: {data.get('code')} - {data.get('message')}")
                    break
                
                # å¤„ç†å½“å‰é¡µçš„æ–‡ä»¶
                for item in data["data"].get("fileList", []):
                    # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                    if item.get("trashed", 1) != 0:
                        continue
                    
                    # æ„å»ºæ–‡ä»¶è·¯å¾„
                    item_path = f"{current_path}/{item['filename']}" if current_path else item['filename']
                    
                    if item["type"] == 1:  # æ–‡ä»¶å¤¹
                        # æ›´æ–°ç¼“å­˜
                        updated = self.update_directory_cache(
                            item["fileId"],
                            item["filename"],
                            directory_id,
                            item_path,
                            base_dir_id
                        )
                        if updated:
                            update_count += 1
                            logger.info(f"æ›´æ–°ç›®å½•ç¼“å­˜: {item['filename']} (ID: {item['fileId']}, è·¯å¾„: {item_path})")
                        
                        # é€’å½’åŒæ­¥å­ç›®å½•ï¼ˆåœ¨æ·±åº¦é™åˆ¶å†…ï¼‰
                        if current_depth < self.search_max_depth:
                            update_count += self.sync_directory(
                                item["fileId"],
                                item_path,
                                base_dir_id,
                                current_depth + 1
                            )
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
                last_file_id = data["data"].get("lastFileId", -1)
                if last_file_id == -1:
                    break
                    
            except Exception as e:
                logger.error(f"åŒæ­¥ç›®å½•å‡ºé”™: {str(e)}", exc_info=True)
                break
        
        logger.info(f"åŒæ­¥å®Œæˆ: '{current_path}' (ID: {directory_id}), æ›´æ–° {update_count} ä¸ªç›®å½•")
        return update_count
    
    def search_folder_recursive(self, folder_name, parent_id=0, current_path="", current_depth=0):
        """é€’å½’æœç´¢æ•´ä¸ªäº‘ç›˜ç»“æ„ä¸­çš„æ–‡ä»¶å¤¹ï¼ˆå¸¦ç¼“å­˜ä¼˜å…ˆï¼‰"""
        # é¦–å…ˆå°è¯•åœ¨ç¼“å­˜ä¸­æœç´¢
        cached_results = self.search_in_cache(folder_name, parent_id)
        if cached_results:
            # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…ç»“æœ
            return {
                "fileId": cached_results[0]["file_id"],
                "filename": cached_results[0]["filename"],
                "path": cached_results[0]["full_path"],
                "from_cache": True
            }
        
        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå†æ‰§è¡Œé€’å½’æœç´¢
        return self._search_folder_recursive(folder_name, parent_id, current_path, current_depth)
    
    def _search_folder_recursive(self, folder_name, parent_id=0, current_path="", current_depth=0):
        """å®é™…é€’å½’æœç´¢å®ç°"""
        # å¦‚æœå½“å‰æ·±åº¦è¶…è¿‡æœ€å¤§æ·±åº¦ï¼Œåˆ™åœæ­¢é€’å½’
        if current_depth > self.search_max_depth:
            logger.info(f"å·²è¾¾åˆ°æœ€å¤§æœç´¢æ·±åº¦ {self.search_max_depth}ï¼Œåœæ­¢é€’å½’")
            return None
            
        logger.info(f"æœç´¢æ–‡ä»¶å¤¹: '{folder_name}' (æ·±åº¦: {current_depth}/{self.search_max_depth}, çˆ¶ID: {parent_id}, å½“å‰è·¯å¾„: '{current_path}')")
        
        # ç¡®ä¿tokenæœ‰æ•ˆ
        if not self.token_manager.ensure_token_valid():
            logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„Token")
            return None
        
        # ä½¿ç”¨V2 APIè·å–ç›®å½•å†…å®¹
        last_file_id = 0
        while True:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": parent_id,
                "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                "limit": 100,
                "lastFileId": last_file_id
            }
            headers = self.token_manager.get_auth_header()
            
            try:
                # ä½¿ç”¨é™æµä¿æŠ¤çš„APIè°ƒç”¨
                response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
                if not response or response.status_code != 200:
                    return None
                
                data = response.json()
                if data.get("code") != 0:
                    return None
                
                # æ£€æŸ¥å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹
                for item in data["data"].get("fileList", []):
                    if item["type"] != 1:  # è·³è¿‡éæ–‡ä»¶å¤¹
                        continue
                        
                    item_path = f"{current_path}/{item['filename']}" if current_path else item['filename']
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡æ–‡ä»¶å¤¹
                    if item["filename"] == folder_name:
                        logger.info(f"âœ… æ‰¾åˆ°æ–‡ä»¶å¤¹: {folder_name} (ID: {item['fileId']}, è·¯å¾„: '{item_path}')")
                        
                        # æ›´æ–°ç¼“å­˜
                        self.update_directory_cache(
                            item["fileId"],
                            item["filename"],
                            parent_id,
                            item_path,
                            # åŸºç›®å½•IDæœªçŸ¥ï¼Œæš‚æ—¶è®¾ä¸º0
                            0
                        )
                        
                        return {
                            "fileId": item["fileId"],
                            "filename": item["filename"],
                            "path": item_path,
                            "from_cache": False
                        }
                    
                    # é€’å½’æœç´¢å­ç›®å½•ï¼ˆä»…åœ¨æ·±åº¦é™åˆ¶å†…ï¼‰
                    if current_depth < self.search_max_depth:
                        time.sleep(0.1)  # å¢åŠ å»¶è¿Ÿé¿å…é™æµ
                        found_folder = self._search_folder_recursive(
                            folder_name,
                            item["fileId"],
                            item_path,
                            current_depth + 1
                        )
                        if found_folder:
                            return found_folder
                    else:
                        logger.debug(f"è·³è¿‡æ·±åº¦ {current_depth+1} çš„ç›®å½•: '{item['filename']}' (è¶…å‡ºæœç´¢æ·±åº¦é™åˆ¶)")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
                last_file_id = data["data"].get("lastFileId", -1)
                if last_file_id == -1:
                    break
                    
            except Exception as e:
                logger.error(f"æœç´¢æ–‡ä»¶å¤¹å‡ºé”™: {str(e)}")
                return None
        
        return None
    
    def get_directory_files(self, directory_id=0, base_path="", current_path=""):
        """
        è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆä½¿ç”¨V2 APIï¼‰
        base_path: åŸºç¡€è·¯å¾„ï¼ˆæœç´¢åˆ°çš„æ–‡ä»¶å¤¹åç§°ï¼‰
        current_path: å½“å‰ç›¸å¯¹è·¯å¾„
        """
        logger.info(f"è·å–ç›®å½•å†…å®¹ (ID: {directory_id}, åŸºç¡€è·¯å¾„: '{base_path}', å½“å‰è·¯å¾„: '{current_path}')")
        all_files = []
        
        # ç¡®ä¿tokenæœ‰æ•ˆ
        if not self.token_manager.ensure_token_valid():
            logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„Token")
            return []
        
        # ä½¿ç”¨V2 APIè·å–ç›®å½•å†…å®¹
        last_file_id = 0  # åˆå§‹å€¼ä¸º0
        while True:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": directory_id,
                "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                "limit": 100,   # æœ€å¤§ä¸è¶…è¿‡100
                "lastFileId": last_file_id
            }
            headers = self.token_manager.get_auth_header()
            
            try:
                logger.debug(f"è¯·æ±‚ç›®å½•åˆ—è¡¨: {url}, å‚æ•°: {params}")
                
                # ä½¿ç”¨é™æµä¿æŠ¤çš„APIè°ƒç”¨
                response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
                if not response:
                    logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥")
                    return all_files
                
                # è°ƒè¯•æ—¥å¿—
                logger.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                if response.status_code != 200:
                    logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}")
                    return all_files
                
                try:
                    data = response.json()
                except json.JSONDecodeError as e:
                    logger.error(f"å“åº”JSONè§£æå¤±è´¥: {str(e)}")
                    logger.error(f"å®Œæ•´å“åº”: {response.text}")
                    return all_files
                
                if data.get("code") != 0:
                    error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                    
                    # å¦‚æœæ˜¯é™æµé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
                    if "æ“ä½œé¢‘ç¹" in error_msg or "é™æµ" in error_msg:
                        logger.warning(f"APIé™æµ: {error_msg}, ç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                        time.sleep(self.retry_delay)
                        continue
                    
                    logger.error(f"APIé”™è¯¯: {error_msg}")
                    return all_files
                
                # å¤„ç†å½“å‰é¡µçš„æ–‡ä»¶
                for item in data["data"].get("fileList", []):
                    # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                    if item.get("trashed", 1) != 0:
                        continue
                    
                    # æ„å»ºæ–‡ä»¶ç›¸å¯¹è·¯å¾„
                    if current_path:
                        file_path = f"{current_path}/{item['filename']}"
                    else:
                        file_path = item['filename']
                    
                    if item["type"] == 0:  # æ–‡ä»¶
                        file_info = {
                            "path": file_path,  # å­˜å‚¨å®Œæ•´ç›¸å¯¹è·¯å¾„
                            "etag": item["etag"],
                            "size": item["size"]
                        }
                        all_files.append(file_info)
                    elif item["type"] == 1:  # æ–‡ä»¶å¤¹
                        # æ„å»ºå­ç›®å½•è·¯å¾„
                        if current_path:
                            sub_path = f"{current_path}/{item['filename']}"
                        else:
                            sub_path = item['filename']
                        
                        # é€’å½’è·å–å­ç›®å½•ï¼ˆæ·»åŠ å»¶è¿Ÿé¿å…é™æµï¼‰
                        time.sleep(0.5)  # å¢åŠ å»¶è¿Ÿ
                        sub_files = self.get_directory_files(
                            item["fileId"],
                            base_path,
                            sub_path
                        )
                        all_files.extend(sub_files)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
                last_file_id = data["data"].get("lastFileId", -1)
                if last_file_id == -1:
                    break
                    
            except Exception as e:
                logger.error(f"è·å–ç›®å½•åˆ—è¡¨å‡ºé”™: {str(e)}", exc_info=True)
                return all_files
        
        logger.info(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶ (ID: {directory_id})")
        return all_files

class FastLinkProcessor:
    @staticmethod
    def parse_share_link(share_link):
        """è§£æç§’ä¼ é“¾æ¥"""
        logger.info("è§£æç§’ä¼ é“¾æ¥...")
        common_base_path = ""
        is_common_path_format = False
        is_v2_etag_format = False
        
        if share_link.startswith(COMMON_PATH_LINK_PREFIX_V2):
            is_common_path_format = True
            is_v2_etag_format = True
            share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V2):]
        elif share_link.startswith(COMMON_PATH_LINK_PREFIX_V1):
            is_common_path_format = True
            share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V1):]
        elif share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V2):
            is_v2_etag_format = True
            share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V2):]
        elif share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V1):
            share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V1):]
        
        if is_common_path_format:
            delimiter_pos = share_link.find(COMMON_PATH_DELIMITER)
            if delimiter_pos > -1:
                common_base_path = share_link[:delimiter_pos]
                share_link = share_link[delimiter_pos + 1:]
        
        files = []
        for s_link in share_link.split('$'):
            if not s_link:
                continue
            parts = s_link.split('#')
            if len(parts) < 3:
                continue
            
            etag = parts[0]
            size = parts[1]
            file_path = '#'.join(parts[2:])
            
            if is_common_path_format and common_base_path:
                file_path = common_base_path + file_path
            
            files.append({
                "etag": etag,
                "size": int(size),
                "file_name": file_path,
                "is_v2_etag": is_v2_etag_format
            })
        
        logger.info(f"è§£æåˆ° {len(files)} ä¸ªæ–‡ä»¶")
        return files
    
    @staticmethod
    def optimized_etag_to_hex(optimized_etag, is_v2_etag):
        """å°†ä¼˜åŒ–åçš„ETagè½¬æ¢ä¸ºåå…­è¿›åˆ¶æ ¼å¼"""
        if not is_v2_etag:
            return optimized_etag
        
        try:
            logger.debug(f"è½¬æ¢V2 ETag: {optimized_etag}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„MD5æ ¼å¼ï¼ˆ32ä½åå…­è¿›åˆ¶ï¼‰
            if len(optimized_etag) == 32 and all(c in '0123456789abcdefABCDEF' for c in optimized_etag):
                logger.debug(f"ETagå·²ç»æ˜¯æœ‰æ•ˆçš„MD5æ ¼å¼: {optimized_etag}")
                return optimized_etag.lower()
            
            # è½¬æ¢Base62åˆ°åå…­è¿›åˆ¶
            num = 0
            for char in optimized_etag:
                if char not in BASE62_CHARS:
                    logger.error(f"âŒ ETagåŒ…å«æ— æ•ˆå­—ç¬¦: {char}")
                    return optimized_etag
                num = num * 62 + BASE62_CHARS.index(char)
            
            # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å¹¶ç¡®ä¿32ä½
            hex_str = hex(num)[2:].lower()
            if len(hex_str) > 32:
                # å–å32ä½
                hex_str = hex_str[-32:]
                logger.warning(f"ETagè½¬æ¢åé•¿åº¦è¶…è¿‡32ä½ï¼Œæˆªæ–­ä¸º: {hex_str}")
            elif len(hex_str) < 32:
                # å‰é¢è¡¥é›¶
                hex_str = hex_str.zfill(32)
                logger.debug(f"ETagè½¬æ¢åä¸è¶³32ä½ï¼Œè¡¥é›¶å: {hex_str}")
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„MD5
            if len(hex_str) != 32 or not all(c in '0123456789abcdef' for c in hex_str):
                logger.error(f"âŒ è½¬æ¢åETagæ ¼å¼æ— æ•ˆ: {hex_str}")
                return optimized_etag
            
            logger.debug(f"è½¬æ¢åETag: {hex_str}")
            return hex_str
        except Exception as e:
            logger.error(f"âŒ ETagè½¬æ¢å¤±è´¥: {str(e)}")
            return optimized_etag

class TelegramBotHandler:
    def __init__(self, token, pan_client, allowed_user_ids):
        self.token = token
        self.pan_client = pan_client
        self.allowed_user_ids = allowed_user_ids
        self.updater = Updater(token, use_context=True)
        self.dispatcher = self.updater.dispatcher
        self.start_time = pan_client.token_manager.start_time  # è®°å½•å¯åŠ¨æ—¶é—´
        
        # æ³¨å†Œå¤„ç†ç¨‹åº
        self.dispatcher.add_handler(CommandHandler("start", self.start_command))
        self.dispatcher.add_handler(CommandHandler("export", self.export_command))
        self.dispatcher.add_handler(CommandHandler("sync_full", self.sync_full_command))
        self.dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, self.handle_text))
        self.dispatcher.add_handler(MessageHandler(Filters.document, self.handle_document))
        self.dispatcher.add_handler(CallbackQueryHandler(self.button_callback))
        
        # è®¾ç½®èœå•å‘½ä»¤
        self.set_menu_commands()
    
    def set_menu_commands(self):
        """è®¾ç½®Telegram Botèœå•å‘½ä»¤ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        commands = [
            BotCommand("start", "ç”¨æˆ·ä¿¡æ¯"),
            BotCommand("export", "å¯¼å‡ºç§’ä¼ æ–‡ä»¶"),
            BotCommand("sync_full", "å…¨é‡åŒæ­¥"),
        ]
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                self.updater.bot.set_my_commands(commands)
                logger.info("å·²è®¾ç½®Telegram Botèœå•å‘½ä»¤")
                return
            except Exception as e:
                logger.error(f"è®¾ç½®èœå•å‘½ä»¤å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                else:
                    logger.error("æ— æ³•è®¾ç½®èœå•å‘½ä»¤ï¼Œå°†ç»§ç»­è¿è¡Œä½†ä¸æ˜¾ç¤ºèœå•")
    
    def start(self):
        """å¯åŠ¨æœºå™¨äºº"""
        try:
            self.updater.start_polling()
            logger.info("ğŸ¤– æœºå™¨äººå·²å¯åŠ¨ï¼Œç­‰å¾…æ¶ˆæ¯...")
            logger.info(f"ç®¡ç†å‘˜ç”¨æˆ·ID: {self.allowed_user_ids}")
            self.updater.idle()
        except Exception as e:
            logger.error(f"å¯åŠ¨æœºå™¨äººå¤±è´¥: {str(e)}")
    
    # ç®¡ç†å‘˜æƒé™æ£€æŸ¥è£…é¥°å™¨
    def admin_required(func):
        @wraps(func)
        def wrapper(self, update: Update, context: CallbackContext, *args, **kwargs):
            user_id = update.message.from_user.id
            if user_id not in self.allowed_user_ids:
                #logger.warning(f"ç”¨æˆ· {user_id} å°è¯•è®¿é—®ä½†æ— æƒé™")
                #update.message.reply_text("ğŸš« æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äºº")
                return
            return func(self, update, context, *args, **kwargs)
        return wrapper
    
    def auto_delete_message(self, context, chat_id, message_id, delay=60):
        """è‡ªåŠ¨åˆ é™¤æ¶ˆæ¯"""
        def delete():
            try:
                context.bot.delete_message(chat_id=chat_id, message_id=message_id)
                logger.debug(f"å·²è‡ªåŠ¨åˆ é™¤æ¶ˆæ¯: {message_id}")
            except Exception as e:
                if "message to delete not found" not in str(e).lower():
                    logger.error(f"åˆ é™¤æ¶ˆæ¯å¤±è´¥: {str(e)}")
        
        # ä½¿ç”¨çº¿ç¨‹å»¶è¿Ÿæ‰§è¡Œ
        threading.Timer(delay, delete).start()
    
    def send_auto_delete_message(self, update, context, text, delay=60, chat_id=None):
        """å‘é€è‡ªåŠ¨åˆ é™¤çš„æ¶ˆæ¯"""
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ chat_id
        if chat_id is None:
            # å°è¯•ä»ä¸åŒæ¥æºè·å– chat_id
            if update and update.message:
                chat_id = update.message.chat_id
            elif update and update.callback_query and update.callback_query.message:
                chat_id = update.callback_query.message.chat_id
            elif context and hasattr(context, '_chat_id'):
                chat_id = context._chat_id
            else:
                logger.error("æ— æ³•ç¡®å®š chat_idï¼Œæ— æ³•å‘é€æ¶ˆæ¯")
                return None
        
        message = context.bot.send_message(chat_id=chat_id, text=text)
        self.auto_delete_message(context, chat_id, message.message_id, delay)
        return message
    
    @admin_required
    def start_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/startå‘½ä»¤ï¼Œæ˜¾ç¤ºç”¨æˆ·ä¿¡æ¯å’Œæœºå™¨äººçŠ¶æ€"""
        logger.info("æ”¶åˆ°/startå‘½ä»¤")
        
        try:
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_info = self.pan_client.get_user_info()
            if not user_info:
                self.send_auto_delete_message(update, context, "âŒ æ— æ³•è·å–ç”¨æˆ·ä¿¡æ¯ï¼Œè¯·ç¨åå†è¯•")
                return
            
            # è®¡ç®—è¿è¡Œæ—¶é—´
            uptime = datetime.now() - self.start_time
            days = uptime.days
            hours, remainder = divmod(uptime.seconds, 3600)
            minutes, seconds = divmod(remainder, 60)
            
            # æ ¼å¼åŒ–æ‰‹æœºå·ç å’ŒUID
            phone = user_info.get("passport", "")
            if phone and len(phone) > 7:
                phone = phone[:3] + "*" * 4 + phone[-4:]
            
            uid = str(user_info.get("uid", ""))
            if uid and len(uid) > 6:
                uid = uid[:3] + "*" * (len(uid) - 6) + uid[-3:]
            
            # æ ¼å¼åŒ–å­˜å‚¨ç©ºé—´
            def format_size(size_bytes):
                if size_bytes >= 1024 ** 4:  # TB
                    return f"{size_bytes / (1024 ** 4):.2f} TB"
                elif size_bytes >= 1024 ** 3:  # GB
                    return f"{size_bytes / (1024 ** 3):.2f} GB"
                elif size_bytes >= 1024 ** 2:  # MB
                    return f"{size_bytes / (1024 ** 2):.2f} MB"
                else:  # KB
                    return f"{size_bytes / 1024:.2f} KB"
            
            space_permanent = format_size(user_info.get("spacePermanent", 0))
            space_used = format_size(user_info.get("spaceUsed", 0))
            direct_traffic = format_size(user_info.get("directTraffic", 0))
            
            # æ„å»ºæ¶ˆæ¯
            export_dirs = ", ".join(EXPORT_BASE_DIRS) if EXPORT_BASE_DIRS else "æ ¹ç›®å½•"
            message = (
                f"ğŸš€ 123äº‘ç›˜ç”¨æˆ·ä¿¡æ¯ | {'ğŸ‘‘ å°Šäº«è´¦æˆ·' if user_info.get('vip', False) else 'ğŸ”’ æ™®é€šè´¦æˆ·'}\n"
                f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                f"ğŸ‘¤ æ˜µç§°: {user_info.get('nickname', 'æœªçŸ¥')}\n"
                f"ğŸ†” è´¦æˆ·ID: {uid}\n"
                f"ğŸ“± æ‰‹æœºå·ç : {phone}\n\n"
                f"ğŸ’¾ å­˜å‚¨ç©ºé—´\n"
                f"â”œ æ°¸ä¹…: {space_permanent}\n"
                f"â”” å·²ç”¨: {space_used}\n\n"
                f"ğŸ“¡ æµé‡ä¿¡æ¯\n"
                f"â”” ç›´é“¾: {direct_traffic}\n"
                f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
                f"âš™ï¸ å½“å‰é…ç½®:\n"
                f"â”œ ä¿å­˜ç›®å½•: {DEFAULT_SAVE_DIR or 'æ ¹ç›®å½•'}\n"
                f"â”œ å¯¼å‡ºç›®å½•: {export_dirs}\n"
                f"â”œ æœç´¢æ·±åº¦: {SEARCH_MAX_DEPTH}å±‚\n"
                f"â”” æ•°æ®ç¼“å­˜: {len(self.pan_client.directory_cache)}\n\n"
                f"ğŸ¤– æœºå™¨äººæ§åˆ¶ä¸­å¿ƒ\n"
                f"â–«ï¸ /export å¯¼å‡ºæ–‡ä»¶\n"
                f"â–«ï¸ /sync_full å…¨é‡åŒæ­¥\n\n"
                f"â±ï¸ å·²è¿è¡Œ: {days}å¤©{hours}å°æ—¶{minutes}åˆ†{seconds}ç§’"
            )
            
            # å‘é€æ¶ˆæ¯ï¼ˆä¸è‡ªåŠ¨åˆ é™¤ï¼‰
            update.message.reply_text(message)
            logger.info("å·²å‘é€ç”¨æˆ·ä¿¡æ¯")
            
        except Exception as e:
            logger.error(f"å¤„ç†/startå‘½ä»¤å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, "âŒ è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åå†è¯•")

    def search_database_by_name(self, name_pattern):
        """åœ¨æ•°æ®åº“ä¸­è¿›è¡Œæ¨¡ç³Šæœç´¢"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                conn.row_factory = sqlite3.Row
                c = conn.cursor()
                
                # ä½¿ç”¨LIKEè¿›è¡Œæ¨¡ç³ŠåŒ¹é…ï¼Œæ”¯æŒéƒ¨åˆ†åŒ¹é…
                c.execute(
                    "SELECT * FROM directory_cache WHERE filename LIKE ? ORDER BY filename",
                    (f'%{name_pattern}%',)
                )
                
                rows = c.fetchall()
                logger.info(f"æ•°æ®åº“ä¸­æ‰¾åˆ° {len(rows)} ä¸ªåŒ¹é…é¡¹: '{name_pattern}'")
                
                return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"æ•°æ®åº“æœç´¢å¤±è´¥: {str(e)}")
            return []

    @admin_required
    def export_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/exportå‘½ä»¤ï¼Œä½¿ç”¨æŒ‰é’®é€‰æ‹©æ–‡ä»¶å¤¹"""
        logger.info("æ”¶åˆ°/exportå‘½ä»¤")
        
        # è·å–å‘½ä»¤å‚æ•°
        search_query = " ".join(context.args) if context.args else ""
        
        if not search_query:
            self.send_auto_delete_message(update, context, "âŒ è¯·æŒ‡å®šè¦æœç´¢çš„æ–‡ä»¶å¤¹åç§°ï¼æ ¼å¼: /export <æ–‡ä»¶å¤¹åç§°>")
            return
        
        self.send_auto_delete_message(update, context, f"ğŸ” æ­£åœ¨æœç´¢æ–‡ä»¶å¤¹: '{search_query}'...")
        
        try:
            # åœ¨æ•°æ®åº“ä¸­è¿›è¡Œæ¨¡ç³Šæœç´¢
            results = self.search_database_by_name(search_query)
            
            if not results:
                self.send_auto_delete_message(update, context, f"âŒ æœªæ‰¾åˆ°åŒ…å« '{search_query}' çš„æ–‡ä»¶å¤¹")
                return
            
            # ä¿å­˜ç»“æœåˆ°ä¸Šä¸‹æ–‡
            context.user_data['export_search_results'] = results
            context.user_data['export_selected_indices'] = set()  # å­˜å‚¨ç”¨æˆ·é€‰æ‹©çš„ç´¢å¼•
            
            # åˆ›å»ºæŒ‰é’®é”®ç›˜
            keyboard = []
            max_buttons = 40  # Telegramæœ€å¤šæ”¯æŒ100ä¸ªæŒ‰é’®ï¼Œæˆ‘ä»¬é™åˆ¶ä¸º40ä¸ª
            
            # æ·»åŠ æ–‡ä»¶å¤¹é€‰æ‹©æŒ‰é’®
            for i, result in enumerate(results[:max_buttons]):
                filename = result["filename"]
                # æˆªæ–­è¿‡é•¿çš„æ–‡ä»¶å
                display_name = filename if len(filename) <= 50 else f"{filename[:47]}..."
                keyboard.append([
                    InlineKeyboardButton(
                        f"{i+1}. {display_name}", 
                        callback_data=f"export_toggle_{i}"
                    )
                ])
            
            # æ·»åŠ æ“ä½œæŒ‰é’®
            action_buttons = [
                InlineKeyboardButton("âœ… å…¨é€‰", callback_data="export_select_all"),
                InlineKeyboardButton("âŒ å–æ¶ˆå…¨é€‰", callback_data="export_deselect_all"),
                InlineKeyboardButton("ğŸš€ å¼€å§‹å¯¼å‡º", callback_data="export_confirm"),
                InlineKeyboardButton("âŒ å–æ¶ˆæ“ä½œ", callback_data="export_cancel")
            ]
            
            # åˆ†ä¸¤è¡Œæ’åˆ—æ“ä½œæŒ‰é’®
            keyboard.append(action_buttons[:2])
            keyboard.append(action_buttons[2:])
            
            reply_markup = InlineKeyboardMarkup(keyboard)
            
            # å‘é€é€‰æ‹©æ¶ˆæ¯
            message = update.message.reply_text(
                f"âœ… æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…é¡¹\nè¯·é€‰æ‹©è¦å¯¼å‡ºçš„æ–‡ä»¶å¤¹:",
                reply_markup=reply_markup
            )
            
            # ä¿å­˜æ¶ˆæ¯IDç”¨äºåç»­æ›´æ–°
            context.user_data['export_message_id'] = message.message_id
            
            # è®¾ç½®60ç§’è¶…æ—¶å®šæ—¶å™¨
            job_context = {
                "chat_id": update.message.chat_id,
                "user_data": context.user_data
            }
            context.job_queue.run_once(
                self.export_timeout, 
                60, 
                context=job_context,
                name=f"export_timeout_{message.message_id}"
            )
            
        except Exception as e:
            logger.error(f"æœç´¢æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ æœç´¢å¤±è´¥: {str(e)}")
    
    def export_choice_callback(self, update: Update, context: CallbackContext):
        """å¤„ç†å¯¼å‡ºé€‰æ‹©çš„å›è°ƒ"""
        query = update.callback_query
        query.answer()
        
        data = query.data
        chat_id = query.message.chat_id
        message_id = query.message.message_id
        
        # è·å–ä¸Šä¸‹æ–‡æ•°æ®
        results = context.user_data.get('export_search_results', [])
        selected_indices = context.user_data.get('export_selected_indices', set())
        
        if not results:
            query.edit_message_text("âŒ é€‰æ‹©è¶…æ—¶æˆ–ç»“æœå·²è¿‡æœŸï¼Œè¯·é‡æ–°æœç´¢")
            return
        
        # å¤„ç†ä¸åŒç±»å‹çš„å›è°ƒ
        if data.startswith("export_toggle_"):
            # åˆ‡æ¢é€‰æ‹©çŠ¶æ€
            try:
                index = int(data.split("_")[2])
                if index < len(results):
                    if index in selected_indices:
                        selected_indices.remove(index)
                    else:
                        selected_indices.add(index)
            except (ValueError, IndexError):
                pass
        
        elif data == "export_select_all":
            # å…¨é€‰
            selected_indices = set(range(len(results)))
        
        elif data == "export_deselect_all":
            # å–æ¶ˆå…¨é€‰
            selected_indices = set()
        
        elif data == "export_confirm":
            # ç¡®è®¤å¯¼å‡º
            self.process_export_selection(update, context, selected_indices)
            return
        
        elif data == "export_cancel":
            # å–æ¶ˆæ“ä½œ
            query.edit_message_text("âŒ å¯¼å‡ºæ“ä½œå·²å–æ¶ˆ")
            self.cleanup_export_context(context.user_data)
            return
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        context.user_data['export_selected_indices'] = selected_indices
        
        # æ›´æ–°æ¶ˆæ¯
        self.update_export_message(update, context, results, selected_indices)
    
    def update_export_message(self, update: Update, context: CallbackContext, results, selected_indices):
        """æ›´æ–°å¯¼å‡ºé€‰æ‹©æ¶ˆæ¯"""
        query = update.callback_query
        selected_count = len(selected_indices)
        
        # åˆ›å»ºæ–°é”®ç›˜ï¼ˆä¿ç•™åŸæœ‰ç»“æ„ï¼‰
        keyboard = []
        max_buttons = 40
        
        # æ·»åŠ æ–‡ä»¶å¤¹é€‰æ‹©æŒ‰é’®ï¼ˆæ›´æ–°é€‰ä¸­çŠ¶æ€ï¼‰
        for i, result in enumerate(results[:max_buttons]):
            filename = result["filename"]
            display_name = filename if len(filename) <= 50 else f"{filename[:47]}..."
            
            # æ·»åŠ é€‰ä¸­æ ‡è®°
            prefix = "âœ… " if i in selected_indices else "â¬œ "
            keyboard.append([
                InlineKeyboardButton(
                    f"{prefix}{i+1}. {display_name}", 
                    callback_data=f"export_toggle_{i}"
                )
            ])
        
        # æ·»åŠ æ“ä½œæŒ‰é’®
        action_buttons = [
            InlineKeyboardButton("âœ… å…¨é€‰", callback_data="export_select_all"),
            InlineKeyboardButton("âŒ å–æ¶ˆå…¨é€‰", callback_data="export_deselect_all"),
            InlineKeyboardButton(f"ğŸš€ å¯¼å‡º({selected_count})", callback_data="export_confirm"),
            InlineKeyboardButton("âŒ å–æ¶ˆ", callback_data="export_cancel")
        ]
        
        # åˆ†ä¸¤è¡Œæ’åˆ—æ“ä½œæŒ‰é’®
        keyboard.append(action_buttons[:2])
        keyboard.append(action_buttons[2:])
        
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        # æ›´æ–°æ¶ˆæ¯
        query.edit_message_text(
            text=f"âœ… æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…é¡¹\nå·²é€‰æ‹© {selected_count} ä¸ªæ–‡ä»¶å¤¹:",
            reply_markup=reply_markup
        )
    
    def export_timeout(self, context: CallbackContext):
        """å¯¼å‡ºé€‰æ‹©è¶…æ—¶å¤„ç†"""
        job = context.job
        if not job or not job.context:
            logger.warning("è¶…æ—¶ä»»åŠ¡ç¼ºå°‘ä¸Šä¸‹æ–‡æ•°æ®")
            return
        
        job_context = job.context
        chat_id = job_context.get("chat_id")
        user_data = job_context.get("user_data", {})

        if not chat_id:
            logger.warning("è¶…æ—¶ä»»åŠ¡ç¼ºå°‘ chat_id")
            return
        
        # è·å–æ¶ˆæ¯ID
        if 'export_message_id' in user_data:
            message_id = user_data['export_message_id']

            try:
                # ç¼–è¾‘æ¶ˆæ¯ä¸ºè¶…æ—¶æç¤º
                self.updater.bot.edit_message_text(
                    chat_id=chat_id,
                    message_id=message_id,
                    text="â±ï¸ æ“ä½œè¶…æ—¶ï¼Œå¯¼å‡ºå·²è‡ªåŠ¨å–æ¶ˆ"
                )
            except Exception as e:
                error_msg = str(e).lower()
                if "message to edit not found" in error_msg:
                    logger.debug("æ¶ˆæ¯å·²è¢«ç”¨æˆ·åˆ é™¤ï¼Œæ— éœ€å¤„ç†")
                else:
                    logger.warning(f"ç¼–è¾‘è¶…æ—¶æ¶ˆæ¯å¤±è´¥: {str(e)}")
        
        # æ¸…ç†ä¸Šä¸‹æ–‡
        self.cleanup_export_context(user_data)
    
    def cleanup_export_context(self, user_data: dict):
        """æ¸…ç†å¯¼å‡ºç›¸å…³çš„ä¸Šä¸‹æ–‡æ•°æ®"""
        keys_to_remove = [
            'export_search_results', 
            'export_selected_indices', 
            'export_message_id'
        ]
        
        for key in keys_to_remove:
            if key in user_data:
                del user_data[key]
    
    def process_export_selection(self, update: Update, context: CallbackContext, selected_indices):
        """å¤„ç†é€‰æ‹©çš„å¯¼å‡ºä»»åŠ¡"""
        query = update.callback_query
        
        # è·å–ä¿å­˜çš„æœç´¢ç»“æœ
        results = context.user_data.get('export_search_results', [])
        if not results:
            query.edit_message_text("âŒ é€‰æ‹©è¶…æ—¶æˆ–ç»“æœå·²è¿‡æœŸï¼Œè¯·é‡æ–°æœç´¢")
            return
            
        # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†æ–‡ä»¶å¤¹
        if not selected_indices:
            query.edit_message_text("âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶å¤¹")
            return
            
        # ç¼–è¾‘æ¶ˆæ¯æ˜¾ç¤ºå¤„ç†ä¸­
        query.edit_message_text(f"â³ å¼€å§‹å¯¼å‡º {len(selected_indices)} ä¸ªæ–‡ä»¶å¤¹...")
        
        # å–æ¶ˆè¶…æ—¶ä»»åŠ¡
        if 'export_message_id' in context.user_data:
            message_id = context.user_data['export_message_id']
            job_name = f"export_timeout_{message_id}"
            
            # æŸ¥æ‰¾å¹¶å–æ¶ˆä»»åŠ¡
            current_jobs = context.job_queue.get_jobs_by_name(job_name)
            for job in current_jobs:
                job.schedule_removal()
        
        # å¤„ç†é€‰ä¸­çš„æ–‡ä»¶å¤¹
        total = len(selected_indices)
        
        for i, idx in enumerate(selected_indices):
            # è·å–é€‰ä¸­çš„æ–‡ä»¶å¤¹
            selected_folder = results[idx]
            folder_id = selected_folder["file_id"]
            folder_name = selected_folder["filename"]
            folder_path = selected_folder["full_path"]
            
            # æ›´æ–°å¤„ç†è¿›åº¦
            if i % 3 == 0:  # æ¯å¤„ç†3ä¸ªæ–‡ä»¶å¤¹æ›´æ–°ä¸€æ¬¡è¿›åº¦
                try:
                    query.edit_message_text(
                        f"â³ æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹ [{i+1}/{total}]:\n"
                        f"â”œ åç§°: {folder_name}\n"
                        f"â”” è·¯å¾„: {folder_path}"
                    )
                except:
                    pass
            
            # è·å–æ–‡ä»¶å¤¹å†…å®¹
            files = self.pan_client.get_directory_files(folder_id, folder_name)
            
            if not files:
                logger.warning(f"æ–‡ä»¶å¤¹ä¸ºç©º: {folder_name}")
                continue
            
            # åˆ›å»ºJSONç»“æ„
            json_data = {
                "commonPath": folder_name,
                "usesBase62EtagsInExport": False,
                "files": [
                    {
                        "path": file_info["path"],
                        "etag": file_info["etag"],
                        "size": file_info["size"]
                    }
                    for file_info in files
                ]
            }
            
            # æ¸…ç†æ–‡ä»¶å¤¹åç§°
            clean_folder_name = re.sub(r'[\\/*?:"<>|]', "", folder_name)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_name = f"{clean_folder_name}_{timestamp}.json"
            
            # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
            with open(file_name, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_info = self.pan_client.get_user_info()
            nickname = user_info.get("nickname", "æœªçŸ¥ç”¨æˆ·") if user_info else "æœªçŸ¥ç”¨æˆ·"
            is_vip = user_info.get("vip", False) if user_info else False
            vip_status = "ğŸ‘‘ å°Šäº«ä¼šå‘˜" if is_vip else "ğŸ”’ æ™®é€šç”¨æˆ·"
            
            # åˆ›å»ºåˆ†äº«ä¿¡æ¯
            caption = (
                f"âœ¨æ¥è‡ªï¼š{nickname}çš„åˆ†äº«\n\n"
                f"ğŸ“ æ–‡ä»¶å: {clean_folder_name}\n"
                f"ğŸ“ æ–‡ä»¶æ•°: {len(files)}\n\n"
                f"â¤ï¸ 123å› æ‚¨åˆ†äº«æ›´å®Œç¾ï¼"
            )
            
            # å‘é€æ–‡ä»¶
            with open(file_name, "rb") as f:
                context.bot.send_document(
                    chat_id=query.message.chat_id,
                    document=f,
                    filename=file_name,
                    caption=caption
                )
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.remove(file_name)
            logger.info(f"å·²å‘é€å¯¼å‡ºæ–‡ä»¶: {file_name}")
        
        # å‘é€å®Œæˆæ¶ˆæ¯
        context.bot.send_message(
            chat_id=query.message.chat_id,
            text=f"âœ… å¯¼å‡ºå®Œæˆï¼å…±å¤„ç† {total} ä¸ªæ–‡ä»¶å¤¹"
        )
        
        # æ¸…ç†ä¸Šä¸‹æ–‡
        self.cleanup_export_context(context.user_data)
 
    
    def handle_document(self, update: Update, context: CallbackContext):
        """å¤„ç†æ–‡æ¡£æ¶ˆæ¯ï¼ˆJSONæ–‡ä»¶ï¼‰"""
        document = update.message.document
        user_id = update.message.from_user.id
        file_name = document.file_name
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ–‡ä»¶
        if document.mime_type != "application/json" and not file_name.endswith(".json"):
            self.send_auto_delete_message(update, context, "âŒ è¯·å‘é€JSONæ ¼å¼çš„æ–‡ä»¶ï¼")
            return
        
        logger.info(f"æ”¶åˆ°JSONæ–‡ä»¶: {file_name}")
        self.send_auto_delete_message(update, context, "ğŸ“¥ æ”¶åˆ°JSONæ–‡ä»¶ï¼Œå¼€å§‹ä¸‹è½½å¹¶è§£æ...")
        
        # ä¸‹è½½æ–‡ä»¶
        file = context.bot.get_file(document.file_id)
        file_path = f"temp_{user_id}_{document.file_id}.json"
        file.download(file_path)
        
        # è¯»å–å¹¶è§£æJSON
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                json_data = json.load(f)
            os.remove(file_path)
            
            logger.info(f"è§£æJSONæ–‡ä»¶: {file_name}")
            self.process_json_file(update, context, json_data)
        except Exception as e:
            logger.error(f"âŒ å¤„ç†JSONæ–‡ä»¶å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    @admin_required
    def process_fast_link(self, update: Update, context: CallbackContext, share_link):
        """å¤„ç†ç§’ä¼ é“¾æ¥è½¬å­˜"""
        try:
            files = FastLinkProcessor.parse_share_link(share_link)
            if not files:
                logger.warning("æ— æ³•è§£æç§’ä¼ é“¾æ¥æˆ–é“¾æ¥ä¸­æ— æœ‰æ•ˆæ–‡ä»¶ä¿¡æ¯")
                self.send_auto_delete_message(update, context, "âŒ æ— æ³•è§£æç§’ä¼ é“¾æ¥æˆ–é“¾æ¥ä¸­æ— æœ‰æ•ˆæ–‡ä»¶ä¿¡æ¯")
                return
            
            logger.info(f"å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
            self.send_auto_delete_message(update, context, f"âœ… è§£ææˆåŠŸï¼æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è½¬å­˜...")
            
            # è½¬å­˜æ–‡ä»¶
            results = self.transfer_files(update, context, files)
            
            # å‘é€ç»“æœ
            self.send_transfer_results(update, context, results)
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ç§’ä¼ é“¾æ¥å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†ç§’ä¼ é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
    
    @admin_required
    def process_json_file(self, update: Update, context: CallbackContext, json_data):
        """å¤„ç†JSONæ–‡ä»¶è½¬å­˜"""
        try:
            if not isinstance(json_data, dict) or not json_data.get("files"):
                logger.warning("JSONæ ¼å¼æ— æ•ˆï¼Œç¼ºå°‘fileså­—æ®µ")
                self.send_auto_delete_message(update, context, "âŒ JSONæ ¼å¼æ— æ•ˆï¼Œç¼ºå°‘fileså­—æ®µ")
                return
            
            common_path = json_data.get("commonPath", "").strip()
            if common_path.endswith('/'):
                common_path = common_path[:-1]
            
            files = []
            for file_info in json_data["files"]:
                file_path = file_info.get("path", "")
                if common_path:
                    file_path = f"{common_path}/{file_path}"
                
                files.append({
                    "etag": file_info.get("etag", ""),
                    "size": int(file_info.get("size", 0)),
                    "file_name": file_path,
                    "is_v2_etag": json_data.get("usesBase62EtagsInExport", False)
                })
            
            logger.info(f"å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
            self.send_auto_delete_message(update, context, f"âœ… è§£ææˆåŠŸï¼æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è½¬å­˜...")
            
            # è½¬å­˜æ–‡ä»¶
            results = self.transfer_files(update, context, files)
            
            # å‘é€ç»“æœ
            self.send_transfer_results(update, context, results)
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†JSONæ–‡ä»¶å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    
    def transfer_files(self, update: Update, context: CallbackContext, files):
        """è½¬å­˜æ–‡ä»¶åˆ—è¡¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        logger.info(f"å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
        results = []
        total_files = len(files)
        root_dir_id = self.pan_client.default_save_dir_id  # ä½¿ç”¨é…ç½®çš„é»˜è®¤ä¿å­˜ç›®å½•
        
        # åˆ›å»ºæ–‡ä»¶å¤¹ç¼“å­˜
        folder_cache = {}
        
        for i, file_info in enumerate(files):
            file_path = file_info["file_name"]
            logger.info(f"å¤„ç†æ–‡ä»¶ [{i+1}/{total_files}]: {file_path}")
            
            try:
                # å¤„ç†æ–‡ä»¶è·¯å¾„
                path_parts = file_path.split('/')
                file_name = path_parts.pop()
                parent_id = root_dir_id
                
                # åˆ›å»ºç›®å½•ç»“æ„
                current_path = ""
                for part in path_parts:
                    if not part:
                        continue
                    
                    current_path = f"{current_path}/{part}" if current_path else part
                    cache_key = f"{parent_id}/{current_path}"
                    
                    # æ£€æŸ¥ç¼“å­˜
                    if cache_key in folder_cache:
                        parent_id = folder_cache[cache_key]
                        continue
                    
                    # åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•ï¼‰
                    folder = self.pan_client.create_folder(parent_id, part)
                    if not folder:
                        logger.warning(f"âš ï¸ åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {part}ï¼Œå°†ä½¿ç”¨æ ¹ç›®å½•")
                        parent_id = root_dir_id
                    else:
                        folder_id = folder["FileId"]
                        folder_cache[cache_key] = folder_id
                        parent_id = folder_id
                
                # å¤„ç†ETag
                etag = file_info["etag"]
                if file_info.get("is_v2_etag", False):
                    etag = FastLinkProcessor.optimized_etag_to_hex(etag, True)
                
                # ç§’ä¼ æ–‡ä»¶ï¼ˆå¸¦é‡è¯•ï¼‰
                result = self.pan_client.rapid_upload(
                    etag, 
                    file_info["size"],
                    file_name,
                    parent_id
                )
                
                if result:
                    results.append({
                        "success": True,
                        "file_name": file_path,
                        "file_id": result["FileId"]
                    })
                    logger.info(f"âœ… æ–‡ä»¶è½¬å­˜æˆåŠŸ: {file_path}")
                else:
                    results.append({
                        "success": False,
                        "file_name": file_path,
                        "error": "ç§’ä¼ å¤±è´¥"
                    })
                    logger.error(f"âŒ æ–‡ä»¶è½¬å­˜å¤±è´¥: {file_path}")
            except Exception as e:
                logger.error(f"âŒ è½¬å­˜æ–‡ä»¶ {file_path} å‡ºé”™: {str(e)}")
                results.append({
                    "success": False,
                    "file_name": file_path,
                    "error": str(e)
                })
        
        logger.info(f"æ–‡ä»¶è½¬å­˜å®Œæˆï¼ŒæˆåŠŸ: {sum(1 for r in results if r['success'])}, å¤±è´¥: {len(results) - sum(1 for r in results if r['success'])}")
        return results
    
    def send_transfer_results(self, update: Update, context: CallbackContext, results):
        """å‘é€è½¬å­˜ç»“æœï¼ŒåŒ…å«å¤±è´¥æ–‡ä»¶è¯¦æƒ…ï¼ˆæ­¤æ¶ˆæ¯ä¸è‡ªåŠ¨åˆ é™¤ï¼‰"""
        success_count = sum(1 for r in results if r["success"])
        failed_count = len(results) - success_count
        
        # æ„å»ºåŸºç¡€ç»“æœæ–‡æœ¬
        result_text = (
            f"ğŸ“Š è½¬å­˜å®Œæˆï¼\n"
            f"âœ… æˆåŠŸ: {success_count}\n"
            f"âŒ å¤±è´¥: {failed_count}\n"
            f"ğŸ“ ä¿å­˜ç›®å½•: {DEFAULT_SAVE_DIR or 'æ ¹ç›®å½•'}"
        )
        
        # æ·»åŠ å¤±è´¥æ–‡ä»¶è¯¦æƒ…
        if failed_count > 0:
            failed_files = []
            for result in results:
                if not result["success"]:
                    # ç®€åŒ–æ–‡ä»¶åæ˜¾ç¤º
                    file_name = result["file_name"]
                    if len(file_name) > 50:
                        file_name = f"...{file_name[-47]}" if file_name else "æœªçŸ¥æ–‡ä»¶"
                    
                    failed_files.append(f"â€¢ {file_name}: {result['error']}")
            
            result_text += "\n\nâŒ å¤±è´¥æ–‡ä»¶:\n" + "\n".join(failed_files[:10])  # æœ€å¤šæ˜¾ç¤º10ä¸ªå¤±è´¥æ–‡ä»¶
            
            if failed_count > 10:
                result_text += f"\n...åŠå…¶ä»– {failed_count - 10} ä¸ªå¤±è´¥æ–‡ä»¶"
        
        # ä½¿ç”¨æ™®é€šæ¶ˆæ¯å‘é€ï¼ˆä¸è‡ªåŠ¨åˆ é™¤ï¼‰
        chat_id = update.message.chat_id
        context.bot.send_message(chat_id=chat_id, text=result_text)
    
    @admin_required
    def sync_full_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/sync_fullå‘½ä»¤ï¼Œå…¨é‡åŒæ­¥ç›®å½•ç¼“å­˜ï¼ˆå¸¦æŒ‰é’®ç¡®è®¤ï¼‰"""
        logger.info("æ”¶åˆ°/sync_fullå‘½ä»¤")
        
        # åˆ›å»ºæŒ‰é’®
        keyboard = [
            [
                InlineKeyboardButton("âœ… ç¡®è®¤", callback_data='sync_full_confirm'),
                InlineKeyboardButton("âŒ å–æ¶ˆ", callback_data='sync_full_cancel')
            ]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        # å‘é€æ¶ˆæ¯
        message = update.message.reply_text(
            "âš ï¸ ç¡®è®¤è¦æ‰§è¡Œå…¨é‡åŒæ­¥å—ï¼Ÿ\n"
            "è¿™å°†æ›´æ–°æ•´ä¸ªåª’ä½“åº“çš„ç›®å½•ç¼“å­˜ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚",
            reply_markup=reply_markup
        )
        
        # ä¿å­˜æ¶ˆæ¯IDï¼Œç”¨äºåç»­åˆ é™¤
        context.user_data['confirmation_message_id'] = message.message_id

    def button_callback(self, update: Update, context: CallbackContext):
        """å¤„ç†æŒ‰é’®å›è°ƒ"""
        query = update.callback_query
        query.answer()
        
        data = query.data
        
        # æ ¹æ®å›è°ƒç±»å‹åˆ†å‘å¤„ç†
        if data.startswith("export_"):
            self.export_choice_callback(update, context)
        elif data.startswith("sync_full_"):
            # åŸæœ‰çš„å…¨é‡åŒæ­¥å¤„ç†
            chat_id = query.message.chat_id
            message_id = query.message.message_id
            
            if data == 'sync_full_confirm':
                try:
                    context.bot.delete_message(chat_id=chat_id, message_id=message_id)
                except Exception as e:
                    logger.error(f"åˆ é™¤æ¶ˆæ¯å¤±è´¥: {str(e)}")
                self.execute_full_sync(update, context)
            elif data == 'sync_full_cancel':
                try:
                    context.bot.delete_message(chat_id=chat_id, message_id=message_id)
                except Exception as e:
                    logger.error(f"åˆ é™¤æ¶ˆæ¯å¤±è´¥: {str(e)}")
                context.bot.send_message(chat_id=chat_id, text="âŒ å…¨é‡åŒæ­¥å·²å–æ¶ˆ")

    def execute_full_sync(self, update: Update, context: CallbackContext):
        """æ‰§è¡Œå…¨é‡åŒæ­¥"""
        # å°è¯•ä»ä¸Šä¸‹æ–‡è·å– chat_id
        chat_id = getattr(context, '_chat_id', None)
        
        self.send_auto_delete_message(
            update, context, 
            "ğŸ”„ æ­£åœ¨æ‰§è¡Œå…¨é‡åŒæ­¥ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...",
            chat_id=chat_id
        )
        
        try:
            start_time = time.time()
            update_count = self.pan_client.full_sync_directory_cache()
            elapsed = time.time() - start_time
            
            self.send_auto_delete_message(
                update, context, 
                f"âœ… å…¨é‡åŒæ­¥å®Œæˆï¼\n"
                f"â”œ æ›´æ–°ç›®å½•: {update_count} ä¸ª\n"
                f"â”œ æ€»ç¼“å­˜æ•°: {len(self.pan_client.directory_cache)}\n"
                f"â”” è€—æ—¶: {elapsed:.2f}ç§’",
                chat_id=chat_id
            )
        except Exception as e:
            logger.error(f"å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}")
            self.send_auto_delete_message(
                update, context, 
                f"âŒ å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}",
                chat_id=chat_id
            )
            
        # æ¸…ç†ä¸Šä¸‹æ–‡
        if hasattr(context, '_chat_id'):
            del context._chat_id

    def handle_text(self, update: Update, context: CallbackContext):
        """å¤„ç†æ–‡æœ¬æ¶ˆæ¯ï¼ˆç§’ä¼ é“¾æ¥ï¼‰"""
        text = update.message.text.strip()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç§’ä¼ é“¾æ¥
        if (text.startswith(LEGACY_FOLDER_LINK_PREFIX_V1) or 
            text.startswith(LEGACY_FOLDER_LINK_PREFIX_V2) or 
            text.startswith(COMMON_PATH_LINK_PREFIX_V1) or 
            text.startswith(COMMON_PATH_LINK_PREFIX_V2) or
            ('#' in text and '$' in text)):  # æ›´å®½æ¾çš„åŒ¹é…
            logger.info(f"æ”¶åˆ°ç§’ä¼ é“¾æ¥: {text[:50]}...")
            self.send_auto_delete_message(update, context, "ğŸ” æ£€æµ‹åˆ°ç§’ä¼ é“¾æ¥ï¼Œå¼€å§‹è§£æ...")
            self.process_fast_link(update, context, text)

def main():
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    BOT_TOKEN = os.getenv("TG_BOT_TOKEN","")
    CLIENT_ID = os.getenv("PAN_CLIENT_ID","")
    CLIENT_SECRET = os.getenv("PAN_CLIENT_SECRET","")
    ADMIN_USER_IDS = [int(id.strip()) for id in os.getenv("TG_ADMIN_USER_IDS", "").split(",") if id.strip()]
    
    # æ£€æŸ¥é…ç½®æ˜¯å¦å®Œæ•´
    if not BOT_TOKEN:
        logger.error("âŒ ç¯å¢ƒå˜é‡ TG_BOT_TOKEN æœªè®¾ç½®")
        return
    
    if not CLIENT_ID:
        logger.error("âŒ ç¯å¢ƒå˜é‡ PAN_CLIENT_ID æœªè®¾ç½®")
        return
    
    if not CLIENT_SECRET:
        logger.error("âŒ ç¯å¢ƒå˜é‡ PAN_CLIENT_SECRET æœªè®¾ç½®")
        return
    
    if not ADMIN_USER_IDS:
        logger.warning("âš ï¸ ç¯å¢ƒå˜é‡ TG_ADMIN_USER_IDS æœªè®¾ç½®æˆ–ä¸ºç©ºï¼Œæœºå™¨äººå°†å¯¹æ‰€æœ‰ç”¨æˆ·å¼€æ”¾")
    
    # è®°å½•é…ç½®ä¿¡æ¯
    #logger.info(f"è½¬å­˜ç›®å½•: {DEFAULT_SAVE_DIR or 'æ ¹ç›®å½•'}")
    #logger.info(f"å¯¼å‡ºåŸºç›®å½•: {', '.join(EXPORT_BASE_DIRS) if EXPORT_BASE_DIRS else 'æ ¹ç›®å½•'}")
    #logger.info(f"æœç´¢æœ€å¤§æ·±åº¦: {SEARCH_MAX_DEPTH}å±‚")
    
    logger.info("åˆå§‹åŒ–123äº‘ç›˜å®¢æˆ·ç«¯...")
    pan_client = Pan123Client(CLIENT_ID, CLIENT_SECRET)
    
    # ç¡®ä¿Tokenå·²åŠ è½½æˆ–è·å–
    if not pan_client.token_manager.access_token:
        logger.error("âŒ æ— æ³•è·å–æœ‰æ•ˆçš„Tokenï¼Œè¯·æ£€æŸ¥å‡­è¯")
        return
    
    logger.info("åˆå§‹åŒ–Telegramæœºå™¨äºº...")
    bot_handler = TelegramBotHandler(BOT_TOKEN, pan_client, ADMIN_USER_IDS)
    
    # å¯åŠ¨æœºå™¨äºº
    logger.info("æœºå™¨äººå¯åŠ¨ä¸­...")
    bot_handler.start()

if __name__ == "__main__":
    main()
