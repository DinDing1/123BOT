import warnings
warnings.filterwarnings("ignore", message="python-telegram-bot is using upstream urllib3.*")
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*", category=UserWarning)
import os
import re
import json
import time
import logging
import requests
import sqlite3
import threading
from contextlib import closing
from datetime import datetime, timedelta, timezone
from telegram import Update, BotCommand
from telegram.ext import (
    Updater, 
    MessageHandler, 
    Filters, 
    CallbackContext, 
    CommandHandler
)
from functools import wraps
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å¿½ç•¥ç¬¬ä¸‰æ–¹åº“çš„è­¦å‘Š
logging.getLogger("telegram").setLevel(logging.WARNING)
logging.getLogger("apscheduler").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

# ====================== é…ç½®åŒºåŸŸ ======================
# æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (ä½¿ç”¨ç›¸å¯¹è·¯å¾„)
DB_PATH = os.getenv("DB_PATH", "bot123.db")

# 123äº‘ç›˜APIé…ç½®
PAN_HOST = "https://www.123pan.com"
API_PATHS = {
    "TOKEN": "/api/v1/access_token",
    "USER_INFO": "/api/v1/user/info",  # è·å–ç”¨æˆ·ä¿¡æ¯
    "LIST_FILES_V2": "/api/v2/file/list",
    "FILE_INFOS": "/api/v1/file/infos",
    "UPLOAD_REQUEST": "/b/api/file/upload_request",
}

# å¼€æ”¾å¹³å°åœ°å€
OPEN_API_HOST = "https://open-api.123pan.com"

# ç§’ä¼ é“¾æ¥å‰ç¼€
LEGACY_FOLDER_LINK_PREFIX_V1 = "123FSLinkV1$"
LEGACY_FOLDER_LINK_PREFIX_V2 = "123FSLinkV2$"
COMMON_PATH_LINK_PREFIX_V1 = "123FLCPV1$"
COMMON_PATH_LINK_PREFIX_V2 = "123FLCPV2$"
COMMON_PATH_DELIMITER = "%"

# Base62å­—ç¬¦é›†
BASE62_CHARS = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"

# ç¯å¢ƒå˜é‡é…ç½®
DEFAULT_SAVE_DIR = os.getenv("DEFAULT_SAVE_DIR", "").strip()  # é»˜è®¤ä¿å­˜ç›®å½•
EXPORT_BASE_DIR = os.getenv("EXPORT_BASE_DIR", "").strip()    # å¯¼å‡ºæ“ä½œçš„åŸºç›®å½•
SEARCH_MAX_DEPTH = int(os.getenv("SEARCH_MAX_DEPTH", ""))         # æœç´¢æ–‡ä»¶å¤¹çš„æœ€å¤§æ·±åº¦
# =====================================================

def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    try:
        with closing(sqlite3.connect(DB_PATH)) as conn:
            c = conn.cursor()
            # åˆ›å»ºTokenç¼“å­˜è¡¨
            c.execute('''CREATE TABLE IF NOT EXISTS token_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                access_token TEXT NOT NULL,
                client_id TEXT NOT NULL,
                client_secret TEXT NOT NULL,
                expired_at TIMESTAMP NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )''')
            
            # åˆ›å»ºç›®å½•ç¼“å­˜è¡¨
            c.execute('''CREATE TABLE IF NOT EXISTS directory_cache (
                file_id INTEGER PRIMARY KEY,
                filename TEXT NOT NULL,
                parent_id INTEGER NOT NULL,
                full_path TEXT NOT NULL,
                base_dir_id INTEGER NOT NULL,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )''')
            
            # åˆ›å»ºç´¢å¼•ä»¥åŠ é€Ÿæœç´¢
            c.execute('''CREATE INDEX IF NOT EXISTS idx_filename ON directory_cache (filename)''')
            c.execute('''CREATE INDEX IF NOT EXISTS idx_full_path ON directory_cache (full_path)''')
            c.execute('''CREATE INDEX IF NOT EXISTS idx_base_dir ON directory_cache (base_dir_id)''')
            
            conn.commit()
            #logger.info(f"æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {DB_PATH}")
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")

init_db()

class TokenManager:
    """ç®¡ç†API tokençš„è·å–å’Œç¼“å­˜"""
    def __init__(self, client_id, client_secret):
        self.client_id = client_id
        self.client_secret = client_secret
        self.session = self._create_session()
        self.access_token = None
        self.token_expiry = None
        self.start_time = datetime.now()  # è®°å½•å¯åŠ¨æ—¶é—´
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½Token
        if not self.load_token_from_cache():
            logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜Tokenï¼Œå°†è·å–æ–°Token")
            self.get_new_token()
    
    def _create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=100,
            pool_maxsize=100
        )
        
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        
        # ç¦ç”¨SSLéªŒè¯
        session.verify = False
        return session
    
    def load_token_from_cache(self):
        """ä»æ•°æ®åº“åŠ è½½ç¼“å­˜çš„Token"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                c.execute("SELECT access_token, client_id, client_secret, expired_at FROM token_cache ORDER BY id DESC LIMIT 1")
                row = c.fetchone()
                
                if row:
                    token, cached_id, cached_secret, expired_at_str = row
                    expired_at = datetime.fromisoformat(expired_at_str).astimezone(timezone.utc)
                    now = datetime.now(timezone.utc)
                    
                    # æ£€æŸ¥Tokenæ˜¯å¦æœ‰æ•ˆä¸”å‡­è¯æœªå˜åŒ–
                    if (expired_at > now + timedelta(minutes=5) and
                        self.client_id == cached_id and 
                        self.client_secret == cached_secret):
                        
                        self.access_token = token
                        self.token_expiry = expired_at
                        
                        expires_in = int((expired_at - now).total_seconds())
                        logger.info(
                            "ä½¿ç”¨ç¼“å­˜Token\n"
                            "â”œâ”€ æœ‰æ•ˆæœŸè‡³ï¼š%s (UTC)\n"
                            "â””â”€ å‰©ä½™æ—¶é—´ï¼š%då°æ—¶%dåˆ†é’Ÿ",
                            expired_at.strftime("%Y-%m-%d %H:%M:%S"),
                            expires_in // 3600,
                            (expires_in % 3600) // 60
                        )
                        return True
                    else:
                        logger.info("ç¼“å­˜Tokenæ— æ•ˆæˆ–å·²è¿‡æœŸ")
                else:
                    logger.info("æœªæ‰¾åˆ°ç¼“å­˜Token")
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜Tokenå¤±è´¥: {str(e)}")
        return False
    
    def save_token_to_cache(self, access_token, expired_at):
        """ä¿å­˜Tokenåˆ°æ•°æ®åº“"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                # æ¸…é™¤æ—§Token
                c.execute("DELETE FROM token_cache")
                # æ’å…¥æ–°Token
                c.execute('''INSERT INTO token_cache 
                           (access_token, client_id, client_secret, expired_at)
                           VALUES (?,?,?,?)''',
                           (access_token, self.client_id, self.client_secret, expired_at.isoformat()))
                conn.commit()
                logger.info("Tokenå·²ä¿å­˜åˆ°ç¼“å­˜")
                return True
        except Exception as e:
            logger.error(f"ä¿å­˜Tokenåˆ°ç¼“å­˜å¤±è´¥: {str(e)}")
        return False
    
    def get_new_token(self):
        """è·å–æ–°tokenï¼ˆä½¿ç”¨å¼€æ”¾å¹³å°APIï¼‰"""
        try:
            logger.info("æ­£åœ¨è·å–æ–°Token...")
            url = f"{OPEN_API_HOST}{API_PATHS['TOKEN']}"
            payload = {
                "clientID": self.client_id,
                "clientSecret": self.client_secret
            }
            
            headers = {
                "Content-Type": "application/json",
                "Platform": "open_platform"
            }
            
            response = self.session.post(
                url,
                json=payload,
                headers=headers,
                timeout=20
            )
            
            if response.status_code != 200:
                logger.error(f"è®¤è¯å¤±è´¥: {response.status_code}")
                return False
            
            data = response.json()
            
            if data.get("code") != 0:
                logger.error(f"APIé”™è¯¯: {data.get('code')} - {data.get('message')}")
                return False
            
            # æå–å¹¶ä¿å­˜token
            self.access_token = data["data"]["accessToken"]
            
            # è§£æè¿‡æœŸæ—¶é—´å­—ç¬¦ä¸²
            expired_at_str = data["data"]["expiredAt"]
            
            # ä¿®å¤æ—¶é—´è§£æé—®é¢˜
            if expired_at_str.endswith('Z'):
                self.token_expiry = datetime.fromisoformat(expired_at_str[:-1]).replace(tzinfo=timezone.utc)
            elif '+' in expired_at_str or '-' in expired_at_str:
                dt = datetime.fromisoformat(expired_at_str)
                self.token_expiry = dt.astimezone(timezone.utc)
            else:
                self.token_expiry = datetime.fromisoformat(expired_at_str).replace(tzinfo=timezone.utc)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.save_token_to_cache(self.access_token, self.token_expiry):
                logger.info(f"æ›´æ–°Token\nâ””â”€æœ‰æ•ˆæœŸè‡³: {self.token_expiry} (UTC)")
                return True
            else:
                logger.error("Tokenä¿å­˜åˆ°ç¼“å­˜å¤±è´¥")
                return False
            
        except Exception as e:
            logger.error(f"è·å–Tokenå¤±è´¥: {str(e)}")
            return False
    
    def ensure_token_valid(self):
        """ç¡®ä¿tokenæœ‰æ•ˆ"""
        current_time = datetime.now(timezone.utc)
        if not self.access_token or not self.token_expiry or current_time >= self.token_expiry - timedelta(minutes=5):
            logger.info("Tokenæ— æ•ˆæˆ–å³å°†è¿‡æœŸï¼Œåˆ·æ–°ä¸­...")
            return self.get_new_token()
        return True
    
    def get_auth_header(self):
        """è·å–è®¤è¯å¤´"""
        if not self.ensure_token_valid():
            raise Exception("æ— æ³•è·å–æœ‰æ•ˆçš„Token")
        return {
            "Authorization": f"Bearer {self.access_token}",
            "Platform": "open_platform",
            "Content-Type": "application/json"
        }

class Pan123Client:
    def __init__(self, client_id, client_secret):
        self.token_manager = TokenManager(client_id, client_secret)
        self.session = self._create_session()
        self.last_api_call = 0  # è®°å½•æœ€åä¸€æ¬¡APIè°ƒç”¨æ—¶é—´
        self.api_rate_limit = 2  # é™ä½APIè°ƒç”¨é¢‘ç‡
        self.retry_delay = 2.0  # å¢åŠ é™æµæ—¶é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        
        # åˆå§‹åŒ–é»˜è®¤ç›®å½•ID
        self.default_save_dir_id = 0  # æ ¹ç›®å½•
        self.export_base_dir_id = 0   # æ ¹ç›®å½•
        
        # è®¾ç½®é»˜è®¤ä¿å­˜ç›®å½•
        if DEFAULT_SAVE_DIR:
            self.default_save_dir_id = self.get_or_create_directory(DEFAULT_SAVE_DIR)
            logger.info(f"é»˜è®¤ä¿å­˜ç›®å½•å·²è®¾ç½®: '{DEFAULT_SAVE_DIR}' (ID: {self.default_save_dir_id})")
        
        # è®¾ç½®å¯¼å‡ºåŸºç›®å½•
        if EXPORT_BASE_DIR:
            self.export_base_dir_id = self.get_or_create_directory(EXPORT_BASE_DIR)
            logger.info(f"å¯¼å‡ºåŸºç›®å½•å·²è®¾ç½®: '{EXPORT_BASE_DIR}' (ID: {self.export_base_dir_id})")
        
        # è®¾ç½®æœç´¢æœ€å¤§æ·±åº¦
        self.search_max_depth = SEARCH_MAX_DEPTH
        logger.info(f"æœç´¢æœ€å¤§æ·±åº¦å·²è®¾ç½®: {self.search_max_depth} å±‚")
        
        # åˆå§‹åŒ–ç›®å½•ç¼“å­˜
        self.directory_cache = {}
        self.load_directory_cache()
    
    def _create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=100,
            pool_maxsize=100
        )
        
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        
        # ç¦ç”¨SSLéªŒè¯
        session.verify = False
        return session
    
    def get_or_create_directory(self, path):
        """è·å–æˆ–åˆ›å»ºç›®å½•è·¯å¾„"""
        parent_id = 0  # ä»æ ¹ç›®å½•å¼€å§‹
        parts = path.strip('/').split('/')
        
        for part in parts:
            if not part:
                continue
                
            # æœç´¢ç›®å½•
            folder_info = self.search_folder(part, parent_id)
            
            if folder_info:
                parent_id = folder_info["fileId"]
                logger.debug(f"æ‰¾åˆ°ç›®å½•: '{part}' (ID: {parent_id})")
            else:
                # åˆ›å»ºç›®å½•
                folder = self.create_folder(parent_id, part)
                if not folder:
                    logger.error(f"æ— æ³•åˆ›å»ºç›®å½•: '{part}'")
                    return parent_id  # è¿”å›ä¸Šä¸€çº§å¯ç”¨ç›®å½•
                parent_id = folder["FileId"]
                logger.info(f"å·²åˆ›å»ºç›®å½•: '{part}' (ID: {parent_id})")
        
        return parent_id
    
    def search_folder(self, folder_name, parent_id=0):
        """åœ¨æŒ‡å®šçˆ¶ç›®å½•ä¸‹æœç´¢æ–‡ä»¶å¤¹ï¼ˆéé€’å½’ï¼‰"""
        try:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": parent_id,
                "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                "limit": 100,
                "lastFileId": 0
            }
            headers = self.token_manager.get_auth_header()
            
            response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
            if not response or response.status_code != 200:
                return None
                
            data = response.json()
            if data.get("code") != 0:
                return None
                
            # æ£€æŸ¥å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹
            for item in data["data"].get("fileList", []):
                if item["type"] == 1 and item["filename"] == folder_name:
                    return {
                        "fileId": item["fileId"],
                        "filename": item["filename"]
                    }
                    
        except Exception as e:
            logger.error(f"æœç´¢ç›®å½•å‡ºé”™: {str(e)}")
            
        return None

    # æ·»åŠ APIè°ƒç”¨æ§åˆ¶æ–¹æ³•
    def _call_api(self, method, url, **kwargs):
        """æ§åˆ¶APIè°ƒç”¨é¢‘ç‡ï¼Œé¿å…é™æµ"""
        retry_count = 0
        max_retries = 3
        
        while retry_count < max_retries:
            try:
                # è®¡ç®—è·ç¦»ä¸Šæ¬¡è°ƒç”¨çš„æ—¶é—´
                elapsed = time.time() - self.last_api_call
                required_delay = 1.0 / self.api_rate_limit
                
                # å¦‚æœè°ƒç”¨è¿‡å¿«ï¼Œç­‰å¾…è¶³å¤Ÿçš„æ—¶é—´
                if elapsed < required_delay:
                    wait_time = required_delay - elapsed
                    logger.debug(f"APIè°ƒç”¨è¿‡å¿«ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                    time.sleep(wait_time)
                
                # å‘é€APIè¯·æ±‚
                response = self.session.request(method, url, **kwargs)
                self.last_api_call = time.time()
                
                # æ£€æŸ¥æ˜¯å¦è¢«é™æµ
                if response.status_code == 429:
                    logger.warning(f"APIé™æµï¼Œç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                    time.sleep(self.retry_delay)
                    continue
                
                return response
                
            except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:
                retry_count += 1
                logger.error(f"âŒ SSL/è¿æ¥é”™è¯¯: {str(e)}ï¼Œé‡è¯• {retry_count}/{max_retries}")
                time.sleep(2 ** retry_count)  # æŒ‡æ•°é€€é¿
            except Exception as e:
                logger.error(f"APIè°ƒç”¨å‡ºé”™: {str(e)}")
                return None
        
        logger.error(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}")
        return None
    
    def _get_auth_headers(self):
        """è·å–è®¤è¯å¤´ï¼ˆæ·»åŠ åŸå§‹è„šæœ¬ä¸­çš„é¢å¤–å¤´ä¿¡æ¯ï¼‰"""
        auth_header = self.token_manager.get_auth_header()
        return {
            **auth_header,
            "platform": "web",
            "App-Version": "3",
            "Origin": PAN_HOST,
            "Referer": f"{PAN_HOST}/",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
        }
    
    def get_user_info(self):
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        try:
            if not self.token_manager.ensure_token_valid():
                logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„Token")
                return None
                
            url = f"{OPEN_API_HOST}{API_PATHS['USER_INFO']}"
            headers = self.token_manager.get_auth_header()
            
            # ä½¿ç”¨é™æµä¿æŠ¤çš„APIè°ƒç”¨
            response = self._call_api("GET", url, headers=headers, timeout=30)
            if not response or response.status_code != 200:
                logger.error(f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: HTTP {response.status_code if response else 'æ— å“åº”'}")
                return None
                
            data = response.json()
            if data.get("code") != 0:
                logger.error(f"APIé”™è¯¯: {data.get('code')} - {data.get('message')}")
                return None
                
            return data.get("data")
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ä¿¡æ¯å‡ºé”™: {str(e)}")
            return None
    
    def create_folder(self, parent_id, folder_name, retry_count=3):
        """åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        logger.info(f"åˆ›å»ºæ–‡ä»¶å¤¹: '{folder_name}' (çˆ¶ID: {parent_id})")
        
        for attempt in range(retry_count):
            try:
                url = f"{PAN_HOST}{API_PATHS['UPLOAD_REQUEST']}"
                payload = {
                    "driveId": 0,
                    "etag": "",
                    "fileName": folder_name,
                    "parentFileId": int(parent_id),
                    "size": 0,
                    "type": 1,
                    "NotReuse": True,
                    "RequestSource": None,
                    "duplicate": 1,
                    "event": "newCreateFolder",
                    "operateType": 1
                }
                headers = self._get_auth_headers()
                
                # ä½¿ç”¨æ›´å¥å£®çš„è¯·æ±‚æ–¹å¼
                response = self.session.post(
                    url, 
                    json=payload, 
                    headers=headers, 
                    timeout=20,
                    verify=False  # æ˜ç¡®ç¦ç”¨SSLéªŒè¯
                )
                
                data = response.json()
                
                if data.get("code") == 0 and data["data"].get("Info", {}).get("FileId"):
                    folder_id = data["data"]["Info"]["FileId"]
                    logger.info(f"âœ… æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ: '{folder_name}' (ID: {folder_id})")
                    return data["data"]["Info"]
                else:
                    error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {error_msg}")
                    if attempt < retry_count - 1:
                        time.sleep(1)  # ç­‰å¾…åé‡è¯•
                        continue
                    return None
            except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:
                logger.error(f"âŒ SSL/è¿æ¥é”™è¯¯: {str(e)}")
                if attempt < retry_count - 1:
                    logger.info(f"ç­‰å¾…1ç§’åé‡è¯• ({attempt+1}/{retry_count})...")
                    time.sleep(1)
                    continue
                return None
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºæ–‡ä»¶å¤¹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                if attempt < retry_count - 1:
                    time.sleep(1)
                    continue
                return None
        return None
    
    def rapid_upload(self, etag, size, file_name, parent_id, retry_count=3):
        """ç§’ä¼ æ–‡ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        logger.info(f"å°è¯•ç§’ä¼ æ–‡ä»¶: '{file_name}' (å¤§å°: {size} bytes, çˆ¶ID: {parent_id})")
        
        for attempt in range(retry_count):
            try:
                url = f"{PAN_HOST}{API_PATHS['UPLOAD_REQUEST']}"
                payload = {
                    "driveId": 0,
                    "etag": etag,
                    "fileName": file_name,
                    "parentFileId": int(parent_id),
                    "size": int(size),
                    "type": 0,
                    "NotReuse": False,
                    "RequestSource": None,
                    "duplicate": 1,
                    "event": "rapidUpload",
                    "operateType": 1
                }
                headers = self._get_auth_headers()
                
                # ä½¿ç”¨æ›´å¥å£®çš„è¯·æ±‚æ–¹å¼
                response = self.session.post(
                    url, 
                    json=payload, 
                    headers=headers, 
                    timeout=20,
                    verify=False  # æ˜ç¡®ç¦ç”¨SSLéªŒè¯
                )
                
                data = response.json()
                
                if data.get("code") == 0 and data["data"].get("Info", {}).get("FileId"):
                    file_id = data["data"]["Info"]["FileId"]
                    logger.info(f"âœ… æ–‡ä»¶ç§’ä¼ æˆåŠŸ: '{file_name}' (ID: {file_id})")
                    return data["data"]["Info"]
                else:
                    error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ æ–‡ä»¶ç§’ä¼ å¤±è´¥: {error_msg}")
                    if attempt < retry_count - 1:
                        time.sleep(1)
                        continue
                    return None
            except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:
                logger.error(f"âŒ SSL/è¿æ¥é”™è¯¯: {str(e)}")
                if attempt < retry_count - 1:
                    logger.info(f"ç­‰å¾…1ç§’åé‡è¯• ({attempt+1}/{retry_count})...")
                    time.sleep(1)
                    continue
                return None
            except Exception as e:
                logger.error(f"âŒ ç§’ä¼ è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                if attempt < retry_count - 1:
                    time.sleep(1)
                    continue
                return None
        return None
    
    def load_directory_cache(self):
        """ä»æ•°æ®åº“åŠ è½½ç›®å½•ç¼“å­˜"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                conn.row_factory = sqlite3.Row
                c = conn.cursor()
                c.execute("SELECT * FROM directory_cache WHERE base_dir_id = ?", (self.export_base_dir_id,))
                rows = c.fetchall()
                
                for row in rows:
                    file_id = row["file_id"]
                    self.directory_cache[file_id] = dict(row)
                
                logger.info(f"å·²åŠ è½½ {len(rows)} ä¸ªç›®å½•ç¼“å­˜ (å¯¼å‡ºåŸºç›®å½•ID: {self.export_base_dir_id})")
        except Exception as e:
            logger.error(f"åŠ è½½ç›®å½•ç¼“å­˜å¤±è´¥: {str(e)}")
    
    def update_directory_cache(self, file_id, filename, parent_id, full_path):
        """æ›´æ–°ç›®å½•ç¼“å­˜"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if file_id in self.directory_cache:
                existing = self.directory_cache[file_id]
                if (existing["filename"] == filename and 
                    existing["parent_id"] == parent_id and 
                    existing["full_path"] == full_path):
                    return False  # æ— å˜åŒ–ï¼Œæ— éœ€æ›´æ–°
            
            # æ›´æ–°å†…å­˜ç¼“å­˜
            cache_entry = {
                "file_id": file_id,
                "filename": filename,
                "parent_id": parent_id,
                "full_path": full_path,
                "base_dir_id": self.export_base_dir_id
            }
            self.directory_cache[file_id] = cache_entry
            
            # æ›´æ–°æ•°æ®åº“
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                # ä½¿ç”¨INSERT OR REPLACEç¡®ä¿æ›´æ–°
                c.execute('''INSERT OR REPLACE INTO directory_cache 
                            (file_id, filename, parent_id, full_path, base_dir_id) 
                            VALUES (?,?,?,?,?)''',
                          (file_id, filename, parent_id, full_path, self.export_base_dir_id))
                conn.commit()
            
            logger.info(f"æ›´æ–°ç›®å½•ç¼“å­˜: {filename} (ID: {file_id}, è·¯å¾„: {full_path})")
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°ç›®å½•ç¼“å­˜å¤±è´¥: {str(e)}")
            return False
    
    def remove_from_directory_cache(self, file_id):
        """ä»ç¼“å­˜ä¸­ç§»é™¤ç›®å½•"""
        try:
            if file_id in self.directory_cache:
                del self.directory_cache[file_id]
            
            with closing(sqlite3.connect(DB_PATH)) as conn:
                c = conn.cursor()
                c.execute("DELETE FROM directory_cache WHERE file_id = ?", (file_id,))
                conn.commit()
            
            logger.info(f"å·²ä»ç¼“å­˜ä¸­ç§»é™¤ç›®å½•: {file_id}")
            return True
        except Exception as e:
            logger.error(f"ä»ç¼“å­˜ä¸­ç§»é™¤ç›®å½•å¤±è´¥: {str(e)}")
            return False
    
    def search_in_cache(self, folder_name, parent_id=None):
        """åœ¨ç¼“å­˜ä¸­æœç´¢ç›®å½•"""
        results = []
        for file_id, cache in self.directory_cache.items():
            if cache["filename"] == folder_name:
                # å¦‚æœæŒ‡å®šäº†çˆ¶ç›®å½•IDï¼Œåˆ™æ£€æŸ¥æ˜¯å¦åŒ¹é…
                if parent_id is not None and cache["parent_id"] != parent_id:
                    continue
                results.append(cache)
        
        # æŒ‰è·¯å¾„é•¿åº¦æ’åºï¼ˆè¾ƒçŸ­çš„è·¯å¾„å¯èƒ½æ›´æ¥è¿‘æ ¹ç›®å½•ï¼‰
        results.sort(key=lambda x: len(x["full_path"]))
        
        logger.debug(f"åœ¨ç¼“å­˜ä¸­æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç›®å½•: '{folder_name}'")
        return results
    
    def full_sync_directory_cache(self):
        """å…¨é‡åŒæ­¥ç›®å½•ç¼“å­˜"""
        logger.info("å¼€å§‹å…¨é‡åŒæ­¥ç›®å½•ç¼“å­˜...")
        
        try:
            update_count = self.sync_directory(self.export_base_dir_id, EXPORT_BASE_DIR or "æ ¹ç›®å½•")
            logger.info(f"å…¨é‡åŒæ­¥å®Œæˆï¼Œæ›´æ–° {update_count} ä¸ªç›®å½•")
            return update_count
        except Exception as e:
            logger.error(f"å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}")
            return 0
    
    def sync_directory_by_path(self, path):
        """
        åŒæ­¥æŒ‡å®šè·¯å¾„çš„ç›®å½•
        :param path: ç›¸å¯¹äºå¯¼å‡ºåŸºç›®å½•çš„è·¯å¾„ï¼ˆå¦‚"ç”µè§†å‰§/å›½äº§å‰§"ï¼‰
        :return: æ›´æ–°çš„ç›®å½•æ•°é‡
        """
        logger.info(f"å¼€å§‹åŒæ­¥ç›®å½•è·¯å¾„: '{path}'")
        
        # åˆ†è§£è·¯å¾„
        parts = [p for p in path.split('/') if p]
        if not parts:
            logger.warning("è·¯å¾„ä¸ºç©ºï¼Œå°†åŒæ­¥æ•´ä¸ªåŸºç›®å½•")
            return self.full_sync_directory_cache()
        
        # ä»åŸºç›®å½•å¼€å§‹æŸ¥æ‰¾
        parent_id = self.export_base_dir_id
        current_path = EXPORT_BASE_DIR or "æ ¹ç›®å½•"
        found = True
        
        try:
            # é€çº§æŸ¥æ‰¾ç›®å½•
            for part in parts:
                # å…ˆåœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾
                cached = self.search_in_cache(part, parent_id)
                if cached:
                    parent_id = cached[0]["file_id"]
                    current_path = cached[0]["full_path"]
                    logger.debug(f"åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°ç›®å½•: {part} (ID: {parent_id})")
                    continue
                
                # ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œé€šè¿‡APIæŸ¥æ‰¾
                folder_info = self.search_folder(part, parent_id)
                if folder_info:
                    parent_id = folder_info["fileId"]
                    current_path = f"{current_path}/{part}" if current_path else part
                    logger.info(f"é€šè¿‡APIæ‰¾åˆ°ç›®å½•: {part} (ID: {parent_id})")
                    
                    # æ·»åŠ åˆ°ç¼“å­˜
                    self.update_directory_cache(
                        parent_id,
                        part,
                        folder_info.get("parent_id", 0),
                        current_path
                    )
                else:
                    logger.error(f"æ‰¾ä¸åˆ°ç›®å½•: {part} (çˆ¶ID: {parent_id})")
                    found = False
                    break
            
            if not found:
                return 0
            
            # åŒæ­¥æ‰¾åˆ°çš„ç›®å½•
            logger.info(f"å¼€å§‹åŒæ­¥ç›®å½•: '{current_path}' (ID: {parent_id})")
            count = self.sync_directory(parent_id, current_path)
            logger.info(f"ç›®å½•åŒæ­¥å®Œæˆ: '{path}', æ›´æ–° {count} ä¸ªç›®å½•")
            return count
        except Exception as e:
            logger.error(f"ç›®å½•åŒæ­¥å¤±è´¥: {str(e)}")
            return 0
    
    def sync_directory(self, directory_id, current_path, current_depth=0):
        """åŒæ­¥æŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•"""
        logger.info(f"å¼€å§‹åŒæ­¥ç›®å½•: '{current_path}' (ID: {directory_id}, æ·±åº¦: {current_depth})")
        update_count = 0
        last_file_id = 0
        
        while True:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": directory_id,
                "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                "limit": 100,
                "lastFileId": last_file_id
            }
            headers = self.token_manager.get_auth_header()
            
            try:
                logger.debug(f"è¯·æ±‚ç›®å½•åˆ—è¡¨: {url}, å‚æ•°: {params}")
                response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
                
                if not response or response.status_code != 200:
                    logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: HTTP {response.status_code if response else 'æ— å“åº”'}")
                    break
                
                data = response.json()
                if data.get("code") != 0:
                    logger.error(f"APIé”™è¯¯: {data.get('code')} - {data.get('message')}")
                    break
                
                # å¤„ç†å½“å‰é¡µçš„æ–‡ä»¶
                for item in data["data"].get("fileList", []):
                    # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                    if item.get("trashed", 1) != 0:
                        continue
                    
                    # æ„å»ºæ–‡ä»¶è·¯å¾„
                    item_path = f"{current_path}/{item['filename']}" if current_path else item['filename']
                    
                    if item["type"] == 1:  # æ–‡ä»¶å¤¹
                        # æ›´æ–°ç¼“å­˜
                        updated = self.update_directory_cache(
                            item["fileId"],
                            item["filename"],
                            directory_id,
                            item_path
                        )
                        if updated:
                            update_count += 1
                            logger.info(f"æ›´æ–°ç›®å½•ç¼“å­˜: {item['filename']} (ID: {item['fileId']}, è·¯å¾„: {item_path})")
                        
                        # é€’å½’åŒæ­¥å­ç›®å½•ï¼ˆåœ¨æ·±åº¦é™åˆ¶å†…ï¼‰
                        if current_depth < self.search_max_depth:
                            update_count += self.sync_directory(
                                item["fileId"],
                                item_path,
                                current_depth + 1
                            )
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
                last_file_id = data["data"].get("lastFileId", -1)
                if last_file_id == -1:
                    break
                    
            except Exception as e:
                logger.error(f"åŒæ­¥ç›®å½•å‡ºé”™: {str(e)}", exc_info=True)
                break
        
        logger.info(f"åŒæ­¥å®Œæˆ: '{current_path}' (ID: {directory_id}), æ›´æ–° {update_count} ä¸ªç›®å½•")
        return update_count
    
    def search_folder_recursive(self, folder_name, parent_id=0, current_path="", current_depth=0):
        """é€’å½’æœç´¢æ•´ä¸ªäº‘ç›˜ç»“æ„ä¸­çš„æ–‡ä»¶å¤¹ï¼ˆå¸¦ç¼“å­˜ä¼˜å…ˆï¼‰"""
        # é¦–å…ˆå°è¯•åœ¨ç¼“å­˜ä¸­æœç´¢
        cached_results = self.search_in_cache(folder_name, parent_id)
        if cached_results:
            # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…ç»“æœ
            return {
                "fileId": cached_results[0]["file_id"],
                "filename": cached_results[0]["filename"],
                "path": cached_results[0]["full_path"],
                "from_cache": True
            }
        
        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå†æ‰§è¡Œé€’å½’æœç´¢
        return self._search_folder_recursive(folder_name, parent_id, current_path, current_depth)
    
    def _search_folder_recursive(self, folder_name, parent_id=0, current_path="", current_depth=0):
        """å®é™…é€’å½’æœç´¢å®ç°"""
        # å¦‚æœå½“å‰æ·±åº¦è¶…è¿‡æœ€å¤§æ·±åº¦ï¼Œåˆ™åœæ­¢é€’å½’
        if current_depth > self.search_max_depth:
            logger.info(f"å·²è¾¾åˆ°æœ€å¤§æœç´¢æ·±åº¦ {self.search_max_depth}ï¼Œåœæ­¢é€’å½’")
            return None
            
        logger.info(f"æœç´¢æ–‡ä»¶å¤¹: '{folder_name}' (æ·±åº¦: {current_depth}/{self.search_max_depth}, çˆ¶ID: {parent_id}, å½“å‰è·¯å¾„: '{current_path}')")
        
        # ç¡®ä¿tokenæœ‰æ•ˆ
        if not self.token_manager.ensure_token_valid():
            logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„Token")
            return None
        
        # ä½¿ç”¨V2 APIè·å–ç›®å½•å†…å®¹
        last_file_id = 0
        while True:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": parent_id,
                "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                "limit": 100,
                "lastFileId": last_file_id
            }
            headers = self.token_manager.get_auth_header()
            
            try:
                # ä½¿ç”¨é™æµä¿æŠ¤çš„APIè°ƒç”¨
                response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
                if not response or response.status_code != 200:
                    return None
                
                data = response.json()
                if data.get("code") != 0:
                    return None
                
                # æ£€æŸ¥å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹
                for item in data["data"].get("fileList", []):
                    if item["type"] != 1:  # è·³è¿‡éæ–‡ä»¶å¤¹
                        continue
                        
                    item_path = f"{current_path}/{item['filename']}" if current_path else item['filename']
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡æ–‡ä»¶å¤¹
                    if item["filename"] == folder_name:
                        logger.info(f"âœ… æ‰¾åˆ°æ–‡ä»¶å¤¹: {folder_name} (ID: {item['fileId']}, è·¯å¾„: '{item_path}')")
                        
                        # æ›´æ–°ç¼“å­˜
                        self.update_directory_cache(
                            item["fileId"],
                            item["filename"],
                            parent_id,
                            item_path
                        )
                        
                        return {
                            "fileId": item["fileId"],
                            "filename": item["filename"],
                            "path": item_path,
                            "from_cache": False
                        }
                    
                    # é€’å½’æœç´¢å­ç›®å½•ï¼ˆä»…åœ¨æ·±åº¦é™åˆ¶å†…ï¼‰
                    if current_depth < self.search_max_depth:
                        time.sleep(0.1)  # å¢åŠ å»¶è¿Ÿé¿å…é™æµ
                        found_folder = self._search_folder_recursive(
                            folder_name,
                            item["fileId"],
                            item_path,
                            current_depth + 1
                        )
                        if found_folder:
                            return found_folder
                    else:
                        logger.debug(f"è·³è¿‡æ·±åº¦ {current_depth+1} çš„ç›®å½•: '{item['filename']}' (è¶…å‡ºæœç´¢æ·±åº¦é™åˆ¶)")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
                last_file_id = data["data"].get("lastFileId", -1)
                if last_file_id == -1:
                    break
                    
            except Exception as e:
                logger.error(f"æœç´¢æ–‡ä»¶å¤¹å‡ºé”™: {str(e)}")
                return None
        
        return None
    
    def get_directory_files(self, directory_id=0, base_path="", current_path=""):
        """
        è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆä½¿ç”¨V2 APIï¼‰
        base_path: åŸºç¡€è·¯å¾„ï¼ˆæœç´¢åˆ°çš„æ–‡ä»¶å¤¹åç§°ï¼‰
        current_path: å½“å‰ç›¸å¯¹è·¯å¾„
        """
        logger.info(f"è·å–ç›®å½•å†…å®¹ (ID: {directory_id}, åŸºç¡€è·¯å¾„: '{base_path}', å½“å‰è·¯å¾„: '{current_path}')")
        all_files = []
        
        # ç¡®ä¿tokenæœ‰æ•ˆ
        if not self.token_manager.ensure_token_valid():
            logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„Token")
            return []
        
        # ä½¿ç”¨V2 APIè·å–ç›®å½•å†…å®¹
        last_file_id = 0  # åˆå§‹å€¼ä¸º0
        while True:
            url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
            params = {
                "parentFileId": directory_id,
                "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                "limit": 100,   # æœ€å¤§ä¸è¶…è¿‡100
                "lastFileId": last_file_id
            }
            headers = self.token_manager.get_auth_header()
            
            try:
                logger.debug(f"è¯·æ±‚ç›®å½•åˆ—è¡¨: {url}, å‚æ•°: {params}")
                
                # ä½¿ç”¨é™æµä¿æŠ¤çš„APIè°ƒç”¨
                response = self._call_api("GET", url, params=params, headers=headers, timeout=30)
                if not response:
                    logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥")
                    return all_files
                
                # è°ƒè¯•æ—¥å¿—
                logger.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                if response.status_code != 200:
                    logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}")
                    return all_files
                
                try:
                    data = response.json()
                except json.JSONDecodeError as e:
                    logger.error(f"å“åº”JSONè§£æå¤±è´¥: {str(e)}")
                    logger.error(f"å®Œæ•´å“åº”: {response.text}")
                    return all_files
                
                if data.get("code") != 0:
                    error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                    
                    # å¦‚æœæ˜¯é™æµé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
                    if "æ“ä½œé¢‘ç¹" in error_msg or "é™æµ" in error_msg:
                        logger.warning(f"APIé™æµ: {error_msg}, ç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                        time.sleep(self.retry_delay)
                        continue
                    
                    logger.error(f"APIé”™è¯¯: {error_msg}")
                    return all_files
                
                # å¤„ç†å½“å‰é¡µçš„æ–‡ä»¶
                for item in data["data"].get("fileList", []):
                    # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                    if item.get("trashed", 1) != 0:
                        continue
                    
                    # æ„å»ºæ–‡ä»¶ç›¸å¯¹è·¯å¾„
                    if current_path:
                        file_path = f"{current_path}/{item['filename']}"
                    else:
                        file_path = item['filename']
                    
                    if item["type"] == 0:  # æ–‡ä»¶
                        file_info = {
                            "path": file_path,  # å­˜å‚¨å®Œæ•´ç›¸å¯¹è·¯å¾„
                            "etag": item["etag"],
                            "size": item["size"]
                        }
                        all_files.append(file_info)
                    elif item["type"] == 1:  # æ–‡ä»¶å¤¹
                        # æ„å»ºå­ç›®å½•è·¯å¾„
                        if current_path:
                            sub_path = f"{current_path}/{item['filename']}"
                        else:
                            sub_path = item['filename']
                        
                        # é€’å½’è·å–å­ç›®å½•ï¼ˆæ·»åŠ å»¶è¿Ÿé¿å…é™æµï¼‰
                        time.sleep(0.5)  # å¢åŠ å»¶è¿Ÿ
                        sub_files = self.get_directory_files(
                            item["fileId"],
                            base_path,
                            sub_path
                        )
                        all_files.extend(sub_files)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
                last_file_id = data["data"].get("lastFileId", -1)
                if last_file_id == -1:
                    break
                    
            except Exception as e:
                logger.error(f"è·å–ç›®å½•åˆ—è¡¨å‡ºé”™: {str(e)}", exc_info=True)
                return all_files
        
        logger.info(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶ (ID: {directory_id})")
        return all_files

class FastLinkProcessor:
    @staticmethod
    def parse_share_link(share_link):
        """è§£æç§’ä¼ é“¾æ¥"""
        logger.info("è§£æç§’ä¼ é“¾æ¥...")
        common_base_path = ""
        is_common_path_format = False
        is_v2_etag_format = False
        
        if share_link.startswith(COMMON_PATH_LINK_PREFIX_V2):
            is_common_path_format = True
            is_v2_etag_format = True
            share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V2):]
        elif share_link.startswith(COMMON_PATH_LINK_PREFIX_V1):
            is_common_path_format = True
            share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V1):]
        
        if is_common_path_format:
            delimiter_pos = share_link.find(COMMON_PATH_DELIMITER)
            if delimiter_pos > -1:
                common_base_path = share_link[:delimiter_pos]
                share_link = share_link[delimiter_pos + 1:]
        
        if not is_common_path_format:
            if share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V2):
                is_v2_etag_format = True
                share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V2):]
            elif share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V1):
                share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V1):]
        
        files = []
        for s_link in share_link.split('$'):
            parts = s_link.split('#')
            if len(parts) < 3:
                continue
            
            etag = parts[0]
            size = parts[1]
            file_path = '#'.join(parts[2:])
            
            if is_common_path_format and common_base_path:
                file_path = common_base_path + file_path
            
            files.append({
                "etag": etag,
                "size": int(size),
                "file_name": file_path,
                "is_v2_etag": is_v2_etag_format
            })
        
        logger.info(f"è§£æåˆ° {len(files)} ä¸ªæ–‡ä»¶")
        return files
    
    @staticmethod
    def optimized_etag_to_hex(optimized_etag, is_v2_etag):
        """å°†ä¼˜åŒ–åçš„ETagè½¬æ¢ä¸ºåå…­è¿›åˆ¶æ ¼å¼"""
        if not is_v2_etag:
            return optimized_etag
        
        try:
            logger.debug(f"è½¬æ¢V2 ETag: {optimized_etag}")
            num = 0
            for char in optimized_etag:
                num = num * 62 + BASE62_CHARS.index(char)
            
            hex_str = hex(num)[2:].lower()
            
            if len(hex_str) < 32:
                hex_str = hex_str.zfill(32)
            
            logger.debug(f"è½¬æ¢åETag: {hex_str}")
            return hex_str
        except Exception as e:
            logger.error(f"âŒ ETagè½¬æ¢å¤±è´¥: {str(e)}")
            return optimized_etag

class TelegramBotHandler:
    def __init__(self, token, pan_client, allowed_user_ids):
        self.token = token
        self.pan_client = pan_client
        self.allowed_user_ids = allowed_user_ids
        self.updater = Updater(token, use_context=True)
        self.dispatcher = self.updater.dispatcher
        self.start_time = pan_client.token_manager.start_time  # è®°å½•å¯åŠ¨æ—¶é—´
        
        # æ³¨å†Œå¤„ç†ç¨‹åº
        self.dispatcher.add_handler(CommandHandler("start", self.start_command))
        self.dispatcher.add_handler(CommandHandler("export", self.export_command))
        self.dispatcher.add_handler(CommandHandler("sync_full", self.sync_full_command))
        self.dispatcher.add_handler(CommandHandler("sync", self.sync_command))
        self.dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, self.handle_text))
        self.dispatcher.add_handler(MessageHandler(Filters.document, self.handle_document))
        
        # è®¾ç½®èœå•å‘½ä»¤
        self.set_menu_commands()
    
    def set_menu_commands(self):
        """è®¾ç½®Telegram Botèœå•å‘½ä»¤ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        commands = [
            BotCommand("start", "ç”¨æˆ·ä¿¡æ¯"),
            BotCommand("export", "å¯¼å‡ºç§’ä¼ æ–‡ä»¶"),
            BotCommand("sync", "åŒæ­¥æŒ‡å®šç›®å½•"),
            BotCommand("sync_full", "å…¨é‡åŒæ­¥"),
        ]
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                self.updater.bot.set_my_commands(commands)
                logger.info("å·²è®¾ç½®Telegram Botèœå•å‘½ä»¤")
                return
            except Exception as e:
                logger.error(f"è®¾ç½®èœå•å‘½ä»¤å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                else:
                    logger.error("æ— æ³•è®¾ç½®èœå•å‘½ä»¤ï¼Œå°†ç»§ç»­è¿è¡Œä½†ä¸æ˜¾ç¤ºèœå•")
    
    def start(self):
        """å¯åŠ¨æœºå™¨äºº"""
        try:
            self.updater.start_polling()
            logger.info("ğŸ¤– æœºå™¨äººå·²å¯åŠ¨ï¼Œç­‰å¾…æ¶ˆæ¯...")
            logger.info(f"ç®¡ç†å‘˜ç”¨æˆ·ID: {self.allowed_user_ids}")
            self.updater.idle()
        except Exception as e:
            logger.error(f"å¯åŠ¨æœºå™¨äººå¤±è´¥: {str(e)}")
    
    # ç®¡ç†å‘˜æƒé™æ£€æŸ¥è£…é¥°å™¨
    def admin_required(func):
        @wraps(func)
        def wrapper(self, update: Update, context: CallbackContext, *args, **kwargs):
            user_id = update.message.from_user.id
            if user_id not in self.allowed_user_ids:
                #logger.warning(f"ç”¨æˆ· {user_id} å°è¯•è®¿é—®ä½†æ— æƒé™")
                #update.message.reply_text("ğŸš« æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äºº")
                return
            return func(self, update, context, *args, **kwargs)
        return wrapper
    
    def auto_delete_message(self, context, chat_id, message_id, delay=10):
        """è‡ªåŠ¨åˆ é™¤æ¶ˆæ¯"""
        def delete():
            try:
                context.bot.delete_message(chat_id=chat_id, message_id=message_id)
                logger.debug(f"å·²è‡ªåŠ¨åˆ é™¤æ¶ˆæ¯: {message_id}")
            except Exception as e:
                logger.error(f"åˆ é™¤æ¶ˆæ¯å¤±è´¥: {str(e)}")
        
        # ä½¿ç”¨çº¿ç¨‹å»¶è¿Ÿæ‰§è¡Œ
        threading.Timer(delay, delete).start()
    
    def send_auto_delete_message(self, update, context, text, delay=10, chat_id=None):
        """å‘é€è‡ªåŠ¨åˆ é™¤çš„æ¶ˆæ¯"""
        if chat_id is None:
            chat_id = update.message.chat_id
        
        message = context.bot.send_message(chat_id=chat_id, text=text)
        self.auto_delete_message(context, chat_id, message.message_id, delay)
        return message
    
    @admin_required
    def start_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/startå‘½ä»¤ï¼Œæ˜¾ç¤ºç”¨æˆ·ä¿¡æ¯å’Œæœºå™¨äººçŠ¶æ€"""
        logger.info("æ”¶åˆ°/startå‘½ä»¤")
        
        try:
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_info = self.pan_client.get_user_info()
            if not user_info:
                self.send_auto_delete_message(update, context, "âŒ æ— æ³•è·å–ç”¨æˆ·ä¿¡æ¯ï¼Œè¯·ç¨åå†è¯•")
                return
            
            # è®¡ç®—è¿è¡Œæ—¶é—´
            uptime = datetime.now() - self.start_time
            days = uptime.days
            hours, remainder = divmod(uptime.seconds, 3600)
            minutes, seconds = divmod(remainder, 60)
            
            # æ ¼å¼åŒ–æ‰‹æœºå·ç å’ŒUID
            phone = user_info.get("passport", "")
            if phone and len(phone) > 7:
                phone = phone[:3] + "*" * 4 + phone[-4:]
            
            uid = str(user_info.get("uid", ""))
            if uid and len(uid) > 6:
                uid = uid[:3] + "*" * (len(uid) - 6) + uid[-3:]
            
            # æ ¼å¼åŒ–å­˜å‚¨ç©ºé—´
            def format_size(size_bytes):
                if size_bytes >= 1024 ** 4:  # TB
                    return f"{size_bytes / (1024 ** 4):.2f} TB"
                elif size_bytes >= 1024 ** 3:  # GB
                    return f"{size_bytes / (1024 ** 3):.2f} GB"
                elif size_bytes >= 1024 ** 2:  # MB
                    return f"{size_bytes / (1024 ** 2):.2f} MB"
                else:  # KB
                    return f"{size_bytes / 1024:.2f} KB"
            
            space_permanent = format_size(user_info.get("spacePermanent", 0))
            space_used = format_size(user_info.get("spaceUsed", 0))
            direct_traffic = format_size(user_info.get("directTraffic", 0))
            
            # æ„å»ºæ¶ˆæ¯
            message = (
                f"ğŸš€ 123äº‘ç›˜ç”¨æˆ·ä¿¡æ¯ | {'ğŸ‘‘ å°Šäº«è´¦æˆ·' if user_info.get('vip', False) else 'ğŸ”’ æ™®é€šè´¦æˆ·'}\n"
                f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                f"ğŸ‘¤ æ˜µç§°: {user_info.get('nickname', 'æœªçŸ¥')}\n"
                f"ğŸ†” è´¦æˆ·ID: {uid}\n"
                f"ğŸ“± æ‰‹æœºå·ç : {phone}\n\n"
                f"ğŸ’¾ å­˜å‚¨ç©ºé—´\n"
                f"â”œ æ°¸ä¹…: {space_permanent}\n"
                f"â”” å·²ç”¨: {space_used}\n\n"
                f"ğŸ“¡ æµé‡ä¿¡æ¯\n"
                f"â”” ç›´é“¾: {direct_traffic}\n"
                f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
                f"âš™ï¸ å½“å‰é…ç½®:\n"
                f"â”œ ä¿å­˜ç›®å½•: {DEFAULT_SAVE_DIR or 'æ ¹ç›®å½•'}\n"
                f"â”œ å¯¼å‡ºç›®å½•: {EXPORT_BASE_DIR or 'æ ¹ç›®å½•'}\n"
                f"â”œ æœç´¢æ·±åº¦: {SEARCH_MAX_DEPTH}å±‚\n"
                f"â”” æ•°æ®ç¼“å­˜: {len(self.pan_client.directory_cache)}\n\n"
                f"ğŸ¤– æœºå™¨äººæ§åˆ¶ä¸­å¿ƒ\n"
                f"â–«ï¸ /export ç…çŠæ¦œ\n"
                f"â–«ï¸ /sync ç”µè§†å‰§/å›½äº§å‰§\n\n"
                f"â±ï¸ å·²è¿è¡Œ: {days}å¤©{hours}å°æ—¶{minutes}åˆ†{seconds}ç§’"
            )
            
            # å‘é€æ¶ˆæ¯ï¼ˆä¸è‡ªåŠ¨åˆ é™¤ï¼‰
            update.message.reply_text(message)
            logger.info("å·²å‘é€ç”¨æˆ·ä¿¡æ¯")
            
        except Exception as e:
            logger.error(f"å¤„ç†/startå‘½ä»¤å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, "âŒ è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åå†è¯•")
    
    @admin_required
    def export_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/exportå‘½ä»¤ï¼Œæ¨¡ç³ŠåŒ¹é…æ–‡ä»¶å¤¹å¹¶æä¾›é€‰æ‹©"""
        logger.info("æ”¶åˆ°/exportå‘½ä»¤")
        
        # è·å–å‘½ä»¤å‚æ•°ï¼ˆåˆå¹¶æ‰€æœ‰å‚æ•°ä¸ºæœç´¢å…³é”®è¯ï¼‰
        search_query = " ".join(context.args) if context.args else ""
        
        if not search_query:
            self.send_auto_delete_message(update, context, "âŒ è¯·æŒ‡å®šè¦æœç´¢çš„æ–‡ä»¶å¤¹åç§°ï¼æ ¼å¼: /export <æ–‡ä»¶å¤¹åç§°>")
            return
        
        self.send_auto_delete_message(update, context, f"ğŸ” æ­£åœ¨æœç´¢æ–‡ä»¶å¤¹: '{search_query}'...")
        
        try:
            # åœ¨æ•°æ®åº“ä¸­è¿›è¡Œæ¨¡ç³Šæœç´¢
            results = self.search_database_by_name(search_query)
            
            if not results:
                self.send_auto_delete_message(update, context, f"âŒ æœªæ‰¾åˆ°åŒ…å« '{search_query}' çš„æ–‡ä»¶å¤¹")
                return
            
            # æ ¼å¼åŒ–ç»“æœå¹¶ä¿å­˜åˆ°ä¸Šä¸‹æ–‡
            formatted_results = []
            response_lines = []
            
            # æ„å»ºå“åº”æ¶ˆæ¯
            response_lines.append(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…é¡¹:\n")
            
            for i, result in enumerate(results, start=1):
                file_id = result["file_id"]
                filename = result["filename"]
                full_path = result["full_path"]
                
                formatted_results.append({
                    "file_id": file_id,
                    "filename": filename,
                    "full_path": full_path
                })
                
                # æ·»åŠ ç»“æœåˆ°å“åº”æ¶ˆæ¯
                response_lines.append(
                    f"{i}. ğŸ“ åç§°: {filename}\n"
                    f"   ğŸ”¢ ID: {file_id}\n"
                    f"   ğŸ“‚ è·¯å¾„: {full_path}\n"
                )
            
            # æ·»åŠ é€‰æ‹©æç¤º
            response_lines.append("\nè¯·å›å¤åºå·é€‰æ‹©è¦å¯¼å‡ºçš„æ–‡ä»¶å¤¹")
            
            # å°†æ‰€æœ‰ç»“æœåˆå¹¶ä¸ºä¸€æ¡æ¶ˆæ¯å‘é€
            response_message = "\n".join(response_lines)
            update.message.reply_text(response_message)
            
            # ä¿å­˜ç»“æœåˆ°ä¸Šä¸‹æ–‡å¹¶æç¤ºç”¨æˆ·é€‰æ‹©
            context.user_data['export_search_results'] = formatted_results
            context.user_data['waiting_for_export_choice'] = True
            
        except Exception as e:
            logger.error(f"æœç´¢æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ æœç´¢å¤±è´¥: {str(e)}")
    
    def search_database_by_name(self, name_pattern):
        """åœ¨æ•°æ®åº“ä¸­è¿›è¡Œæ¨¡ç³Šæœç´¢"""
        try:
            with closing(sqlite3.connect(DB_PATH)) as conn:
                conn.row_factory = sqlite3.Row
                c = conn.cursor()
                
                # ä½¿ç”¨LIKEè¿›è¡Œæ¨¡ç³ŠåŒ¹é…ï¼Œæ”¯æŒéƒ¨åˆ†åŒ¹é…
                c.execute(
                    "SELECT * FROM directory_cache WHERE filename LIKE ? ORDER BY filename",
                    (f'%{name_pattern}%',)
                )
                
                rows = c.fetchall()
                logger.info(f"æ•°æ®åº“ä¸­æ‰¾åˆ° {len(rows)} ä¸ªåŒ¹é…é¡¹: '{name_pattern}'")
                
                return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"æ•°æ®åº“æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def handle_export_choice(self, update: Update, context: CallbackContext, choice: str):
        """å¤„ç†ç”¨æˆ·é€‰æ‹©çš„å¯¼å‡ºåºå·"""
        try:
            # è·å–ä¿å­˜çš„æœç´¢ç»“æœ
            results = context.user_data.get('export_search_results', [])
            if not results:
                self.send_auto_delete_message(update, context, "âŒ é€‰æ‹©è¶…æ—¶æˆ–ç»“æœå·²è¿‡æœŸï¼Œè¯·é‡æ–°æœç´¢")
                return
                
            # éªŒè¯ç”¨æˆ·è¾“å…¥
            try:
                choice_index = int(choice) - 1
                if choice_index < 0 or choice_index >= len(results):
                    self.send_auto_delete_message(update, context, "âŒ åºå·æ— æ•ˆï¼Œè¯·è¾“å…¥åˆ—è¡¨ä¸­çš„åºå·")
                    return
            except ValueError:
                self.send_auto_delete_message(update, context, "âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—åºå·")
                return
                
            # è·å–é€‰ä¸­çš„æ–‡ä»¶å¤¹
            selected_folder = results[choice_index]
            folder_id = selected_folder["file_id"]
            folder_name = selected_folder["filename"]
            folder_path = selected_folder["full_path"]
            
            # æ¸…é™¤ä¸Šä¸‹æ–‡çŠ¶æ€
            context.user_data.pop('export_search_results', None)
            context.user_data.pop('waiting_for_export_choice', None)
            
            # å¼€å§‹å¯¼å‡º
            self.send_auto_delete_message(
                update, context,
                f"âœ… å·²é€‰æ‹©: {folder_name}\n"
                f"â”œ ID: {folder_id}\n"
                f"â”” è·¯å¾„: {folder_path}\n"
                f"å¼€å§‹å¯¼å‡ºå†…å®¹..."
            )
            
            # è·å–æ–‡ä»¶å¤¹å†…å®¹
            files = self.pan_client.get_directory_files(folder_id, folder_name)
            
            if not files:
                self.send_auto_delete_message(update, context, "âš ï¸ è¯¥æ–‡ä»¶å¤¹ä¸ºç©º")
                return
            
            # åˆ›å»ºJSONç»“æ„
            json_data = {
                "commonPath": folder_name,
                "usesBase62EtagsInExport": False,
                "files": [
                    {
                        "path": file_info["path"],
                        "etag": file_info["etag"],
                        "size": file_info["size"]
                    }
                    for file_info in files
                ]
            }
            
            # æ¸…ç†æ–‡ä»¶å¤¹åç§°
            clean_folder_name = re.sub(r'[\\/*?:"<>|]', "", folder_name)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_name = f"{clean_folder_name}.json"
            
            # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
            with open(file_name, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_info = self.pan_client.get_user_info()
            nickname = user_info.get("nickname", "æœªçŸ¥ç”¨æˆ·") if user_info else "æœªçŸ¥ç”¨æˆ·"
            is_vip = user_info.get("vip", False) if user_info else False
            vip_status = "ğŸ‘‘ å°Šäº«ä¼šå‘˜" if is_vip else "ğŸ”’ æ™®é€šç”¨æˆ·"
            
            # åˆ›å»ºåˆ†äº«ä¿¡æ¯
            caption = (
                f"âœ¨æ¥è‡ªï¼š{nickname}çš„åˆ†äº«\n\n"
                f"ğŸ“ æ–‡ä»¶å: {clean_folder_name}\n"
                f"ğŸ“ æ–‡ä»¶æ•°: {len(files)}\n\n"
                f"â¤ï¸ 123å› æ‚¨åˆ†äº«æ›´å®Œç¾ï¼"
            )
            
            # å‘é€æ–‡ä»¶
            with open(file_name, "rb") as f:
                context.bot.send_document(
                    chat_id=update.message.chat_id,
                    document=f,
                    filename=file_name,
                    caption=caption
                )
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.remove(file_name)
            logger.info(f"å·²å‘é€å¯¼å‡ºæ–‡ä»¶: {file_name}")
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}")
    
    @admin_required
    def handle_document(self, update: Update, context: CallbackContext):
        """å¤„ç†æ–‡æ¡£æ¶ˆæ¯ï¼ˆJSONæ–‡ä»¶ï¼‰"""
        document = update.message.document
        user_id = update.message.from_user.id
        file_name = document.file_name
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ–‡ä»¶
        if document.mime_type != "application/json" and not file_name.endswith(".json"):
            self.send_auto_delete_message(update, context, "âŒ è¯·å‘é€JSONæ ¼å¼çš„æ–‡ä»¶ï¼")
            return
        
        logger.info(f"æ”¶åˆ°JSONæ–‡ä»¶: {file_name}")
        self.send_auto_delete_message(update, context, "ğŸ“¥ æ”¶åˆ°JSONæ–‡ä»¶ï¼Œå¼€å§‹ä¸‹è½½å¹¶è§£æ...")
        
        # ä¸‹è½½æ–‡ä»¶
        file = context.bot.get_file(document.file_id)
        file_path = f"temp_{user_id}_{document.file_id}.json"
        file.download(file_path)
        
        # è¯»å–å¹¶è§£æJSON
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                json_data = json.load(f)
            os.remove(file_path)
            
            logger.info(f"è§£æJSONæ–‡ä»¶: {file_name}")
            self.process_json_file(update, context, json_data)
        except Exception as e:
            logger.error(f"âŒ å¤„ç†JSONæ–‡ä»¶å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    @admin_required
    def process_fast_link(self, update: Update, context: CallbackContext, share_link):
        """å¤„ç†ç§’ä¼ é“¾æ¥è½¬å­˜"""
        try:
            files = FastLinkProcessor.parse_share_link(share_link)
            if not files:
                logger.warning("æ— æ³•è§£æç§’ä¼ é“¾æ¥æˆ–é“¾æ¥ä¸­æ— æœ‰æ•ˆæ–‡ä»¶ä¿¡æ¯")
                self.send_auto_delete_message(update, context, "âŒ æ— æ³•è§£æç§’ä¼ é“¾æ¥æˆ–é“¾æ¥ä¸­æ— æœ‰æ•ˆæ–‡ä»¶ä¿¡æ¯")
                return
            
            logger.info(f"å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
            self.send_auto_delete_message(update, context, f"âœ… è§£ææˆåŠŸï¼æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è½¬å­˜...")
            
            # è½¬å­˜æ–‡ä»¶
            results = self.transfer_files(update, context, files)
            
            # å‘é€ç»“æœ
            self.send_transfer_results(update, context, results)
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ç§’ä¼ é“¾æ¥å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†ç§’ä¼ é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
    
    @admin_required
    def process_json_file(self, update: Update, context: CallbackContext, json_data):
        """å¤„ç†JSONæ–‡ä»¶è½¬å­˜"""
        try:
            if not isinstance(json_data, dict) or not json_data.get("files"):
                logger.warning("JSONæ ¼å¼æ— æ•ˆï¼Œç¼ºå°‘fileså­—æ®µ")
                self.send_auto_delete_message(update, context, "âŒ JSONæ ¼å¼æ— æ•ˆï¼Œç¼ºå°‘fileså­—æ®µ")
                return
            
            common_path = json_data.get("commonPath", "").strip()
            if common_path.endswith('/'):
                common_path = common_path[:-1]
            
            files = []
            for file_info in json_data["files"]:
                file_path = file_info.get("path", "")
                if common_path:
                    file_path = f"{common_path}/{file_path}"
                
                files.append({
                    "etag": file_info.get("etag", ""),
                    "size": int(file_info.get("size", 0)),
                    "file_name": file_path,
                    "is_v2_etag": json_data.get("usesBase62EtagsInExport", False)
                })
            
            logger.info(f"å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
            self.send_auto_delete_message(update, context, f"âœ… è§£ææˆåŠŸï¼æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è½¬å­˜...")
            
            # è½¬å­˜æ–‡ä»¶
            results = self.transfer_files(update, context, files)
            
            # å‘é€ç»“æœ
            self.send_transfer_results(update, context, results)
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†JSONæ–‡ä»¶å‡ºé”™: {str(e)}")
            self.send_auto_delete_message(update, context, f"âŒ å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    
    def transfer_files(self, update: Update, context: CallbackContext, files):
        """è½¬å­˜æ–‡ä»¶åˆ—è¡¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        logger.info(f"å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
        results = []
        total_files = len(files)
        root_dir_id = self.pan_client.default_save_dir_id  # ä½¿ç”¨é…ç½®çš„é»˜è®¤ä¿å­˜ç›®å½•
        
        # åˆ›å»ºæ–‡ä»¶å¤¹ç¼“å­˜
        folder_cache = {}
        
        for i, file_info in enumerate(files):
            file_path = file_info["file_name"]
            logger.info(f"å¤„ç†æ–‡ä»¶ [{i+1}/{total_files}]: {file_path}")
            
            try:
                # å¤„ç†æ–‡ä»¶è·¯å¾„
                path_parts = file_path.split('/')
                file_name = path_parts.pop()
                parent_id = root_dir_id
                
                # åˆ›å»ºç›®å½•ç»“æ„
                current_path = ""
                for part in path_parts:
                    if not part:
                        continue
                    
                    current_path = f"{current_path}/{part}" if current_path else part
                    cache_key = f"{parent_id}/{current_path}"
                    
                    # æ£€æŸ¥ç¼“å­˜
                    if cache_key in folder_cache:
                        parent_id = folder_cache[cache_key]
                        continue
                    
                    # åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•ï¼‰
                    folder = self.pan_client.create_folder(parent_id, part)
                    if folder:
                        folder_id = folder["FileId"]
                        folder_cache[cache_key] = folder_id
                        parent_id = folder_id
                    else:
                        logger.warning(f"âš ï¸ åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {part}ï¼Œå°†ä½¿ç”¨æ ¹ç›®å½•")
                        parent_id = root_dir_id
                
                # å¤„ç†ETag
                etag = file_info["etag"]
                if file_info.get("is_v2_etag", False):
                    etag = FastLinkProcessor.optimized_etag_to_hex(etag, True)
                
                # ç§’ä¼ æ–‡ä»¶ï¼ˆå¸¦é‡è¯•ï¼‰
                result = self.pan_client.rapid_upload(
                    etag, 
                    file_info["size"],
                    file_name,
                    parent_id
                )
                
                if result:
                    results.append({
                        "success": True,
                        "file_name": file_path,
                        "file_id": result["FileId"]
                    })
                    logger.info(f"âœ… æ–‡ä»¶è½¬å­˜æˆåŠŸ: {file_path}")
                else:
                    results.append({
                        "success": False,
                        "file_name": file_path,
                        "error": "ç§’ä¼ å¤±è´¥"
                    })
                    logger.error(f"âŒ æ–‡ä»¶è½¬å­˜å¤±è´¥: {file_path}")
            except Exception as e:
                logger.error(f"âŒ è½¬å­˜æ–‡ä»¶ {file_path} å‡ºé”™: {str(e)}")
                results.append({
                    "success": False,
                    "file_name": file_path,
                    "error": str(e)
                })
        
        logger.info(f"æ–‡ä»¶è½¬å­˜å®Œæˆï¼ŒæˆåŠŸ: {sum(1 for r in results if r['success'])}, å¤±è´¥: {len(results) - sum(1 for r in results if r['success'])}")
        return results
    
    def send_transfer_results(self, update: Update, context: CallbackContext, results):
        """å‘é€è½¬å­˜ç»“æœï¼ŒåŒ…å«å¤±è´¥æ–‡ä»¶è¯¦æƒ…"""
        success_count = sum(1 for r in results if r["success"])
        failed_count = len(results) - success_count
        
        result_text = (
            f"ğŸ“Š è½¬å­˜å®Œæˆï¼\n"
            f"âœ… æˆåŠŸ: {success_count}\n"
            f"âŒ å¤±è´¥: {failed_count}\n"
            f"ğŸ“ ä¿å­˜ç›®å½•: {DEFAULT_SAVE_DIR or 'æ ¹ç›®å½•'}"
        )
        
        # æ·»åŠ å¤±è´¥æ–‡ä»¶è¯¦æƒ…
        if failed_count > 0:
            failed_files = []
            for result in results:
                if not result["success"]:
                    # ç®€åŒ–æ–‡ä»¶åæ˜¾ç¤º
                    file_name = result["file_name"]
                    if len(file_name) > 50:
                        file_name = f"...{file_name[-47]}"
                    
                    failed_files.append(f"â€¢ {file_name}: {result['error']}")
            
            result_text += "\n\nâŒ å¤±è´¥æ–‡ä»¶:\n" + "\n".join(failed_files[:10])  # æœ€å¤šæ˜¾ç¤º10ä¸ªå¤±è´¥æ–‡ä»¶
            
            if failed_count > 10:
                result_text += f"\n...åŠå…¶ä»– {failed_count - 10} ä¸ªå¤±è´¥æ–‡ä»¶"
        
        # å‘é€å¹¶è‡ªåŠ¨åˆ é™¤
        self.send_auto_delete_message(update, context, result_text)

    @admin_required
    def sync_full_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/sync_fullå‘½ä»¤ï¼Œå…¨é‡åŒæ­¥ç›®å½•ç¼“å­˜ï¼ˆå¸¦æ–‡å­—ç¡®è®¤ï¼‰"""
        logger.info("æ”¶åˆ°/sync_fullå‘½ä»¤")
        
        # å‘é€ç¡®è®¤æ¶ˆæ¯
        message = update.message.reply_text(
            "âš ï¸ ç¡®è®¤è¦æ‰§è¡Œå…¨é‡åŒæ­¥å—ï¼Ÿ\n"
            "è¿™å°†æ›´æ–°æ•´ä¸ªåª’ä½“åº“çš„ç›®å½•ç¼“å­˜ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚\n\n"
            "è¯·å›å¤ 'ç¡®è®¤' å¼€å§‹åŒæ­¥ï¼Œæˆ– 'å–æ¶ˆ' ä¸­æ­¢æ“ä½œã€‚\n"
            "â±ï¸ 60ç§’å†…æœªå›å¤å°†è‡ªåŠ¨å–æ¶ˆã€‚"
        )
        
        # è®¾ç½®ä¸Šä¸‹æ–‡çŠ¶æ€
        context.user_data['waiting_sync_confirmation'] = True
        context.user_data['sync_type'] = 'full'
        context.user_data['confirmation_message_id'] = message.message_id
        
        # è®¾ç½®60ç§’åè‡ªåŠ¨å–æ¶ˆ
        context.job_queue.run_once(
            self.cancel_sync_confirmation, 
            60, 
            context={
                'chat_id': update.message.chat_id,
                'message_id': message.message_id
            }
        )
    
    @admin_required
    def sync_command(self, update: Update, context: CallbackContext):
        """å¤„ç†/syncå‘½ä»¤ï¼ŒåŒæ­¥æŒ‡å®šç›®å½•ï¼ˆå¸¦æ–‡å­—ç¡®è®¤ï¼‰"""
        logger.info("æ”¶åˆ°/syncå‘½ä»¤")
        
        # è·å–å‘½ä»¤å‚æ•°
        path = " ".join(context.args) if context.args else ""
        
        if not path:
            self.send_auto_delete_message(update, context, "âŒ è¯·æŒ‡å®šè¦åŒæ­¥çš„ç›®å½•è·¯å¾„ï¼æ ¼å¼: /sync <ç›®å½•è·¯å¾„>")
            return
        
        # å‘é€ç¡®è®¤æ¶ˆæ¯
        message = update.message.reply_text(
            f"âš ï¸ ç¡®è®¤è¦åŒæ­¥ç›®å½•å—ï¼Ÿ\n"
            f"è·¯å¾„: '{path}'\n\n"
            f"è¯·å›å¤ 'ç¡®è®¤' å¼€å§‹åŒæ­¥ï¼Œæˆ– 'å–æ¶ˆ' ä¸­æ­¢æ“ä½œã€‚\n"
            f"â±ï¸ 60ç§’å†…æœªå›å¤å°†è‡ªåŠ¨å–æ¶ˆã€‚"
        )
        
        # è®¾ç½®ä¸Šä¸‹æ–‡çŠ¶æ€
        context.user_data['waiting_sync_confirmation'] = True
        context.user_data['sync_type'] = 'partial'
        context.user_data['sync_path'] = path
        context.user_data['confirmation_message_id'] = message.message_id
        
        # è®¾ç½®60ç§’åè‡ªåŠ¨å–æ¶ˆ
        context.job_queue.run_once(
            self.cancel_sync_confirmation, 
            60, 
            context={
                'chat_id': update.message.chat_id,
                'message_id': message.message_id
            }
        )
    
    def cancel_sync_confirmation(self, context: CallbackContext):
        """60ç§’åè‡ªåŠ¨å–æ¶ˆåŒæ­¥ç¡®è®¤"""
        job = context.job
        context_data = job.context
        
        chat_id = context_data['chat_id']
        message_id = context_data['message_id']
        
        try:
            # åˆ é™¤ç¡®è®¤æ¶ˆæ¯
            context.bot.delete_message(chat_id=chat_id, message_id=message_id)
            
            # å‘é€å–æ¶ˆé€šçŸ¥
            context.bot.send_message(
                chat_id=chat_id,
                text="â±ï¸ åŒæ­¥æ“ä½œå·²è‡ªåŠ¨å–æ¶ˆï¼ˆ60ç§’æœªç¡®è®¤ï¼‰"
            )
        except Exception as e:
            logger.error(f"å–æ¶ˆç¡®è®¤æ—¶å‡ºé”™: {str(e)}")
    
    def handle_text(self, update: Update, context: CallbackContext):
        """å¤„ç†æ–‡æœ¬æ¶ˆæ¯ï¼ˆç§’ä¼ é“¾æ¥æˆ–å¯¼å‡ºé€‰æ‹©æˆ–åŒæ­¥ç¡®è®¤ï¼‰"""
        text = update.message.text.strip().lower()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¼å‡ºé€‰æ‹©
        if context.user_data.get('waiting_for_export_choice', False):
            logger.info(f"æ”¶åˆ°å¯¼å‡ºé€‰æ‹©: {text}")
            self.handle_export_choice(update, context, text)
            return
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒæ­¥ç¡®è®¤
        if context.user_data.get('waiting_sync_confirmation', False):
            if text == 'ç¡®è®¤':
                self.process_sync_confirmation(update, context)
            elif text == 'å–æ¶ˆ':
                self.cancel_sync_operation(update, context)
            else:
                self.send_auto_delete_message(
                    update, context,
                    "âŒ æ— æ•ˆå›å¤ï¼Œè¯·å›å¤ 'ç¡®è®¤' æˆ– 'å–æ¶ˆ'",
                    delay=5
                )
            return
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç§’ä¼ é“¾æ¥
        if any(prefix in text for prefix in [
            LEGACY_FOLDER_LINK_PREFIX_V1,
            LEGACY_FOLDER_LINK_PREFIX_V2,
            COMMON_PATH_LINK_PREFIX_V1,
            COMMON_PATH_LINK_PREFIX_V2
        ]):
            logger.info(f"æ”¶åˆ°ç§’ä¼ é“¾æ¥: {text[:50]}...")
            self.send_auto_delete_message(update, context, "ğŸ” æ£€æµ‹åˆ°ç§’ä¼ é“¾æ¥ï¼Œå¼€å§‹è§£æ...")
            self.process_fast_link(update, context, text)
    
    def process_sync_confirmation(self, update: Update, context: CallbackContext):
        """å¤„ç†åŒæ­¥ç¡®è®¤"""
        # æ¸…é™¤çŠ¶æ€
        context.user_data.pop('waiting_sync_confirmation', None)
        message_id = context.user_data.pop('confirmation_message_id', None)
        
        # åˆ é™¤ç¡®è®¤æ¶ˆæ¯
        if message_id:
            try:
                context.bot.delete_message(
                    chat_id=update.message.chat_id,
                    message_id=message_id
                )
            except:
                pass
        
        # è·å–åŒæ­¥ç±»å‹
        sync_type = context.user_data.get('sync_type')
        path = context.user_data.get('sync_path', '')
        
        # æ‰§è¡ŒåŒæ­¥æ“ä½œ
        if sync_type == 'full':
            self.execute_full_sync(update, context)
        elif sync_type == 'partial':
            self.execute_partial_sync(update, context, path)
    
    def cancel_sync_operation(self, update: Update, context: CallbackContext):
        """å–æ¶ˆåŒæ­¥æ“ä½œ"""
        # æ¸…é™¤çŠ¶æ€
        context.user_data.pop('waiting_sync_confirmation', None)
        message_id = context.user_data.pop('confirmation_message_id', None)
        
        # åˆ é™¤ç¡®è®¤æ¶ˆæ¯
        if message_id:
            try:
                context.bot.delete_message(
                    chat_id=update.message.chat_id,
                    message_id=message_id
                )
            except:
                pass
        
        # å‘é€å–æ¶ˆé€šçŸ¥
        self.send_auto_delete_message(
            update, context,
            "âŒ åŒæ­¥æ“ä½œå·²å–æ¶ˆ",
            delay=10
        )
    
    def execute_full_sync(self, update: Update, context: CallbackContext):
        """æ‰§è¡Œå…¨é‡åŒæ­¥"""
        self.send_auto_delete_message(
            update, context, 
            "ğŸ”„ æ­£åœ¨æ‰§è¡Œå…¨é‡åŒæ­¥ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´..."
        )
        
        try:
            start_time = time.time()
            update_count = self.pan_client.full_sync_directory_cache()
            elapsed = time.time() - start_time
            
            self.send_auto_delete_message(
                update, context, 
                f"âœ… å…¨é‡åŒæ­¥å®Œæˆï¼\n"
                f"â”œ æ›´æ–°ç›®å½•: {update_count} ä¸ª\n"
                f"â”œ æ€»ç¼“å­˜æ•°: {len(self.pan_client.directory_cache)}\n"
                f"â”” è€—æ—¶: {elapsed:.2f}ç§’"
            )
        except Exception as e:
            logger.error(f"å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}")
            self.send_auto_delete_message(
                update, context, 
                f"âŒ å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}"
            )
    
    def execute_partial_sync(self, update: Update, context: CallbackContext, path: str):
        """æ‰§è¡Œç›®å½•åŒæ­¥"""
        self.send_auto_delete_message(
            update, context, 
            f"ğŸ”„ æ­£åœ¨åŒæ­¥ç›®å½•: '{path}'..."
        )
        
        try:
            start_time = time.time()
            update_count = self.pan_client.sync_directory_by_path(path)
            elapsed = time.time() - start_time
            
            if update_count == 0:
                self.send_auto_delete_message(
                    update, context, 
                    f"âš ï¸ ç›®å½•åŒæ­¥å®Œæˆï¼Œä½†æœªæ›´æ–°ä»»ä½•ç›®å½•\n"
                    f"â”” è€—æ—¶: {elapsed:.2f}ç§’\n"
                    f"å¯èƒ½åŸå› : ç›®å½•å·²æ˜¯æœ€æ–°çŠ¶æ€æˆ–è·¯å¾„ä¸å­˜åœ¨"
                )
            else:
                self.send_auto_delete_message(
                    update, context, 
                    f"âœ… ç›®å½•åŒæ­¥å®Œæˆï¼\n"
                    f"â”œ æ›´æ–°ç›®å½•: {update_count} ä¸ª\n"
                    f"â”œ æ€»ç¼“å­˜æ•°: {len(self.pan_client.directory_cache)}\n"
                    f"â”” è€—æ—¶: {elapsed:.2f}ç§’"
                )
        except Exception as e:
            logger.error(f"ç›®å½•åŒæ­¥å¤±è´¥: {str(e)}")
            self.send_auto_delete_message(
                update, context, 
                f"âŒ ç›®å½•åŒæ­¥å¤±è´¥: {str(e)}"
            )

def main():
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    BOT_TOKEN = os.getenv("TG_BOT_TOKEN","")
    CLIENT_ID = os.getenv("PAN_CLIENT_ID","")
    CLIENT_SECRET = os.getenv("PAN_CLIENT_SECRET","")
    ADMIN_USER_IDS = [int(id.strip()) for id in os.getenv("TG_ADMIN_USER_IDS", "").split(",") if id.strip()]
    
    # æ£€æŸ¥é…ç½®æ˜¯å¦å®Œæ•´
    if not BOT_TOKEN:
        logger.error("âŒ ç¯å¢ƒå˜é‡ TG_BOT_TOKEN æœªè®¾ç½®")
        return
    
    if not CLIENT_ID:
        logger.error("âŒ ç¯å¢ƒå˜é‡ PAN_CLIENT_ID æœªè®¾ç½®")
        return
    
    if not CLIENT_SECRET:
        logger.error("âŒ ç¯å¢ƒå˜é‡ PAN_CLIENT_SECRET æœªè®¾ç½®")
        return
    
    if not ADMIN_USER_IDS:
        logger.warning("âš ï¸ ç¯å¢ƒå˜é‡ TG_ADMIN_USER_IDS æœªè®¾ç½®æˆ–ä¸ºç©ºï¼Œæœºå™¨äººå°†å¯¹æ‰€æœ‰ç”¨æˆ·å¼€æ”¾")
    
    # è®°å½•é…ç½®ä¿¡æ¯
    #logger.info(f"è½¬å­˜ç›®å½•: {DEFAULT_SAVE_DIR or 'æ ¹ç›®å½•'}")
    #logger.info(f"å¯¼å‡ºåŸºç›®å½•: {EXPORT_BASE_DIR or 'æ ¹ç›®å½•'}")
    #logger.info(f"æœç´¢æœ€å¤§æ·±åº¦: {SEARCH_MAX_DEPTH}å±‚")
    
    logger.info("åˆå§‹åŒ–123äº‘ç›˜å®¢æˆ·ç«¯...")
    pan_client = Pan123Client(CLIENT_ID, CLIENT_SECRET)
    
    # ç¡®ä¿Tokenå·²åŠ è½½æˆ–è·å–
    if not pan_client.token_manager.access_token:
        logger.error("âŒ æ— æ³•è·å–æœ‰æ•ˆçš„Tokenï¼Œè¯·æ£€æŸ¥å‡­è¯")
        return
    
    logger.info("åˆå§‹åŒ–Telegramæœºå™¨äºº...")
    bot_handler = TelegramBotHandler(BOT_TOKEN, pan_client, ADMIN_USER_IDS)
    
    # å¯åŠ¨æœºå™¨äºº
    logger.info("æœºå™¨äººå¯åŠ¨ä¸­...")
    bot_handler.start()

if __name__ == "__main__":
    main()
